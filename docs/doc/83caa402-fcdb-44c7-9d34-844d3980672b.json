{
    "summary": "This code defines classes for T5 and CLIP model-based text and image encoding, initializes an OpenCLIP vision transformer encoder, includes methods to encode text and images, performs convolution and normalization, combines outputs in the `FrozenCLIPT5Encoder` class.",
    "details": [
        {
            "comment": "This code defines two classes: AbstractEncoder and ClassEmbedder. The AbstractEncoder is an abstract class that must be inherited for custom encoders. It has a required encode method. The ClassEmbedder class takes in an embedding dimension, number of classes, key (class), and ucg_rate (undersampling class rate) as parameters. It also has an embedded layer and a n_classes variable. The forward method returns the embeddings based on the given batch and can disable dropout if desired.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/encoders/condition.py\":0-36",
            "content": "import torch\nimport torch.nn as nn\nimport kornia\nimport open_clip\nfrom torch.utils.checkpoint import checkpoint\nfrom transformers import T5Tokenizer, T5EncoderModel, CLIPTokenizer, CLIPTextModel\nfrom lvdm.common import autocast\nfrom utils.utils import count_params\nclass AbstractEncoder(nn.Module):\n    def __init__(self):\n        super().__init__()\n    def encode(self, *args, **kwargs):\n        raise NotImplementedError\nclass IdentityEncoder(AbstractEncoder):\n    def encode(self, x):\n        return x\nclass ClassEmbedder(nn.Module):\n    def __init__(self, embed_dim, n_classes=1000, key='class', ucg_rate=0.1):\n        super().__init__()\n        self.key = key\n        self.embedding = nn.Embedding(n_classes, embed_dim)\n        self.n_classes = n_classes\n        self.ucg_rate = ucg_rate\n    def forward(self, batch, key=None, disable_dropout=False):\n        if key is None:\n            key = self.key\n        # this is for use in crossattn\n        c = batch[key][:, None]\n        if self.ucg_rate > 0. and not disable_dropout:"
        },
        {
            "comment": "This code defines a class for encoding text using T5 Transformer encoder, with an optional unconditional conditioning. The get_unconditional_conditioning method returns a tensor of unconditional class indices for batch size bs, and the FrozenT5Embedder class freezes the embedder model in training mode. The code also includes other methods like disabled_train that overwrite model.train to prevent changing train/eval mode.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/encoders/condition.py\":37-63",
            "content": "            mask = 1. - torch.bernoulli(torch.ones_like(c) * self.ucg_rate)\n            c = mask * c + (1 - mask) * torch.ones_like(c) * (self.n_classes - 1)\n            c = c.long()\n        c = self.embedding(c)\n        return c\n    def get_unconditional_conditioning(self, bs, device=\"cuda\"):\n        uc_class = self.n_classes - 1  # 1000 classes --> 0 ... 999, one extra class for ucg (class 1000)\n        uc = torch.ones((bs,), device=device) * uc_class\n        uc = {self.key: uc}\n        return uc\ndef disabled_train(self, mode=True):\n    \"\"\"Overwrite model.train with this function to make sure train/eval mode\n    does not change anymore.\"\"\"\n    return self\nclass FrozenT5Embedder(AbstractEncoder):\n    \"\"\"Uses the T5 transformer encoder for text\"\"\"\n    def __init__(self, version=\"google/t5-v1_1-large\", device=\"cuda\", max_length=77,\n                 freeze=True):  # others are google/t5-v1_1-xl and google/t5-v1_1-xxl\n        super().__init__()\n        self.tokenizer = T5Tokenizer.from_pretrained(version)\n        self.transformer = T5EncoderModel.from_pretrained(version)"
        },
        {
            "comment": "This code defines a class \"ConditionEncoder\" that initializes an encoder and sets its maximum length. It has methods to freeze the model parameters, forward pass for encoding text using transformer, and encode text. The \"FrozenCLIPEmbedder\" class is an extension of AbstractEncoder using CLIP transformer for text encoding, with specified layers and device settings.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/encoders/condition.py\":64-96",
            "content": "        self.device = device\n        self.max_length = max_length  # TODO: typical value?\n        if freeze:\n            self.freeze()\n    def freeze(self):\n        self.transformer = self.transformer.eval()\n        # self.train = disabled_train\n        for param in self.parameters():\n            param.requires_grad = False\n    def forward(self, text):\n        batch_encoding = self.tokenizer(text, truncation=True, max_length=self.max_length, return_length=True,\n                                        return_overflowing_tokens=False, padding=\"max_length\", return_tensors=\"pt\")\n        tokens = batch_encoding[\"input_ids\"].to(self.device)\n        outputs = self.transformer(input_ids=tokens)\n        z = outputs.last_hidden_state\n        return z\n    def encode(self, text):\n        return self(text)\nclass FrozenCLIPEmbedder(AbstractEncoder):\n    \"\"\"Uses the CLIP transformer encoder for text (from huggingface)\"\"\"\n    LAYERS = [\n        \"last\",\n        \"pooled\",\n        \"hidden\"\n    ]\n    def __init__(self, version=\"openai/clip-vit-large-patch14\", device=\"cuda\", max_length=77,"
        },
        {
            "comment": "This code defines a class that initializes a CLIP model for text encoding. It takes version, device, and optional freeze parameters. The freeze method disables gradient updates and sets the model to evaluation mode. The forward method tokenizes the input text and moves the input_ids tensor to the specified device.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/encoders/condition.py\":97-121",
            "content": "                 freeze=True, layer=\"last\", layer_idx=None):  # clip-vit-base-patch32\n        super().__init__()\n        assert layer in self.LAYERS\n        self.tokenizer = CLIPTokenizer.from_pretrained(version)\n        self.transformer = CLIPTextModel.from_pretrained(version)\n        self.device = device\n        self.max_length = max_length\n        if freeze:\n            self.freeze()\n        self.layer = layer\n        self.layer_idx = layer_idx\n        if layer == \"hidden\":\n            assert layer_idx is not None\n            assert 0 <= abs(layer_idx) <= 12\n    def freeze(self):\n        self.transformer = self.transformer.eval()\n        # self.train = disabled_train\n        for param in self.parameters():\n            param.requires_grad = False\n    def forward(self, text):\n        batch_encoding = self.tokenizer(text, truncation=True, max_length=self.max_length, return_length=True,\n                                        return_overflowing_tokens=False, padding=\"max_length\", return_tensors=\"pt\")\n        tokens = batch_encoding[\"input_ids\"].to(self.device)"
        },
        {
            "comment": "The code contains a class 'ConditionEncoder' with methods to encode text and retrieve encoded outputs based on the specified layer. The 'encode' method takes in text as input and returns the encoded output. The 'ClipImageEmbedder' class initializes an instance of the specified CLIP model, registers mean and std buffers, and sets antialias flag.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/encoders/condition.py\":122-151",
            "content": "        outputs = self.transformer(input_ids=tokens, output_hidden_states=self.layer == \"hidden\")\n        if self.layer == \"last\":\n            z = outputs.last_hidden_state\n        elif self.layer == \"pooled\":\n            z = outputs.pooler_output[:, None, :]\n        else:\n            z = outputs.hidden_states[self.layer_idx]\n        return z\n    def encode(self, text):\n        return self(text)\nclass ClipImageEmbedder(nn.Module):\n    def __init__(\n            self,\n            model,\n            jit=False,\n            device='cuda' if torch.cuda.is_available() else 'cpu',\n            antialias=True,\n            ucg_rate=0.\n    ):\n        super().__init__()\n        from clip import load as load_clip\n        self.model, _ = load_clip(name=model, device=device, jit=jit)\n        self.antialias = antialias\n        self.register_buffer('mean', torch.Tensor([0.48145466, 0.4578275, 0.40821073]), persistent=False)\n        self.register_buffer('std', torch.Tensor([0.26862954, 0.26130258, 0.27577711]), persistent=False)"
        },
        {
            "comment": "This code contains two classes, `Condition` and `FrozenOpenCLIPEmbedder`. The `Condition` class preprocesses input data by normalizing it to [0,1] range and then re-normalizes according to a given clip. It also allows dropout in the forward function if specified. The `FrozenOpenCLIPEmbedder` class is an abstract encoder that uses the OpenCLIP transformer encoder for text input, with customizable layers and options for architecture and device.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/encoders/condition.py\":152-183",
            "content": "        self.ucg_rate = ucg_rate\n    def preprocess(self, x):\n        # normalize to [0,1]\n        x = kornia.geometry.resize(x, (224, 224),\n                                   interpolation='bicubic', align_corners=True,\n                                   antialias=self.antialias)\n        x = (x + 1.) / 2.\n        # re-normalize according to clip\n        x = kornia.enhance.normalize(x, self.mean, self.std)\n        return x\n    def forward(self, x, no_dropout=False):\n        # x is assumed to be in range [-1,1]\n        out = self.model.encode_image(self.preprocess(x))\n        out = out.to(x.dtype)\n        if self.ucg_rate > 0. and not no_dropout:\n            out = torch.bernoulli((1. - self.ucg_rate) * torch.ones(out.shape[0], device=out.device))[:, None] * out\n        return out\nclass FrozenOpenCLIPEmbedder(AbstractEncoder):\n    \"\"\"\n    Uses the OpenCLIP transformer encoder for text\n    \"\"\"\n    LAYERS = [\n        # \"pooled\",\n        \"last\",\n        \"penultimate\"\n    ]\n    def __init__(self, arch=\"ViT-H-14\", version=\"laion2b_s32b_b79k\", device=\"cuda\", max_length=77,"
        },
        {
            "comment": "The code is a part of the condition class in the encoders module. It initializes an instance of the class, freezes model parameters if necessary, and provides a forward pass for encoding text using a transformer. The encode_with_transformer method takes input text and returns encoded representation z.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/encoders/condition.py\":184-214",
            "content": "                 freeze=True, layer=\"last\"):\n        super().__init__()\n        assert layer in self.LAYERS\n        model, _, _ = open_clip.create_model_and_transforms(arch, device=torch.device('cpu'), pretrained=version)\n        del model.visual\n        self.model = model\n        self.device = device\n        self.max_length = max_length\n        if freeze:\n            self.freeze()\n        self.layer = layer\n        if self.layer == \"last\":\n            self.layer_idx = 0\n        elif self.layer == \"penultimate\":\n            self.layer_idx = 1\n        else:\n            raise NotImplementedError()\n    def freeze(self):\n        self.model = self.model.eval()\n        for param in self.parameters():\n            param.requires_grad = False\n    def forward(self, text):\n        tokens = open_clip.tokenize(text) ## all clip models use 77 as context length\n        z = self.encode_with_transformer(tokens.to(self.device))\n        return z\n    def encode_with_transformer(self, text):\n        x = self.model.token_embedding(text)  # [batch_size, n_ctx, d_model]"
        },
        {
            "comment": "This code defines a class that implements an image encoder using the OpenCLIP vision transformer. The encoder takes in a text input and returns encoded output after applying several layers. The `text_transformer_forward` function iterates over the resblock layers, and the `encode` method takes a text as input and returns its encoded representation.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/encoders/condition.py\":215-242",
            "content": "        x = x + self.model.positional_embedding\n        x = x.permute(1, 0, 2)  # NLD -> LND\n        x = self.text_transformer_forward(x, attn_mask=self.model.attn_mask)\n        x = x.permute(1, 0, 2)  # LND -> NLD\n        x = self.model.ln_final(x)\n        return x\n    def text_transformer_forward(self, x: torch.Tensor, attn_mask=None):\n        for i, r in enumerate(self.model.transformer.resblocks):\n            if i == len(self.model.transformer.resblocks) - self.layer_idx:\n                break\n            if self.model.transformer.grad_checkpointing and not torch.jit.is_scripting():\n                x = checkpoint(r, x, attn_mask)\n            else:\n                x = r(x, attn_mask=attn_mask)\n        return x\n    def encode(self, text):\n        return self(text)\nclass FrozenOpenCLIPImageEmbedder(AbstractEncoder):\n    \"\"\"\n    Uses the OpenCLIP vision transformer encoder for images\n    \"\"\"\n    def __init__(self, arch=\"ViT-H-14\", version=\"laion2b_s32b_b79k\", device=\"cuda\", max_length=77,\n                 freeze=True, layer=\"pooled\", antialias=True, ucg_rate=0.):"
        },
        {
            "comment": "The code initializes a model using open_clip library, removes the transformer layer, and assigns it to self.model. It also registers buffer for mean and std values for normalization. The preprocess method resizes input images to 224x224 resolution.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/encoders/condition.py\":243-267",
            "content": "        super().__init__()\n        model, _, _ = open_clip.create_model_and_transforms(arch, device=torch.device('cpu'),\n                                                            pretrained=version, )\n        del model.transformer\n        self.model = model\n        # self.mapper = torch.nn.Linear(1280, 1024)\n        self.device = device\n        self.max_length = max_length\n        if freeze:\n            self.freeze()\n        self.layer = layer\n        if self.layer == \"penultimate\":\n            raise NotImplementedError()\n            self.layer_idx = 1\n        self.antialias = antialias\n        self.register_buffer('mean', torch.Tensor([0.48145466, 0.4578275, 0.40821073]), persistent=False)\n        self.register_buffer('std', torch.Tensor([0.26862954, 0.26130258, 0.27577711]), persistent=False)\n        self.ucg_rate = ucg_rate\n    def preprocess(self, x):\n        # normalize to [0,1]\n        x = kornia.geometry.resize(x, (224, 224),\n                                   interpolation='bicubic', align_corners=True,"
        },
        {
            "comment": "This code defines a FrozenOpenCLIPImageEmbedderV2 class that initializes an OpenCLIP vision transformer encoder for images. It includes methods to encode text and images, freeze the model's parameters, and perform forward pass with optional dropout. The class inherits from AbstractEncoder.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/encoders/condition.py\":268-299",
            "content": "                                   antialias=self.antialias)\n        x = (x + 1.) / 2.\n        # renormalize according to clip\n        x = kornia.enhance.normalize(x, self.mean, self.std)\n        return x\n    def freeze(self):\n        self.model = self.model.eval()\n        for param in self.model.parameters():\n            param.requires_grad = False\n    @autocast\n    def forward(self, image, no_dropout=False):\n        z = self.encode_with_vision_transformer(image)\n        if self.ucg_rate > 0. and not no_dropout:\n            z = torch.bernoulli((1. - self.ucg_rate) * torch.ones(z.shape[0], device=z.device))[:, None] * z\n        return z\n    def encode_with_vision_transformer(self, img):\n        img = self.preprocess(img)\n        x = self.model.visual(img)\n        return x\n    def encode(self, text):\n        return self(text)\nclass FrozenOpenCLIPImageEmbedderV2(AbstractEncoder):\n    \"\"\"\n    Uses the OpenCLIP vision transformer encoder for images\n    \"\"\"\n    def __init__(self, arch=\"ViT-H-14\", version=\"laion2b_s32b_b79k\", device=\"cuda\","
        },
        {
            "comment": "This code initializes an object for preprocessing images using OpenCV's create_model_and_transforms function. It freezes the transformer layer, sets the desired layer to process from, and registers buffer for mean and standard deviation for normalization. It also allows for resizing of the image with antialiasing.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/encoders/condition.py\":300-325",
            "content": "                 freeze=True, layer=\"pooled\", antialias=True):\n        super().__init__()\n        model, _, _ = open_clip.create_model_and_transforms(arch, device=torch.device('cpu'),\n                                                            pretrained=version, )\n        del model.transformer\n        self.model = model\n        self.device = device\n        if freeze:\n            self.freeze()\n        self.layer = layer\n        if self.layer == \"penultimate\":\n            raise NotImplementedError()\n            self.layer_idx = 1\n        self.antialias = antialias\n        self.register_buffer('mean', torch.Tensor([0.48145466, 0.4578275, 0.40821073]), persistent=False)\n        self.register_buffer('std', torch.Tensor([0.26862954, 0.26130258, 0.27577711]), persistent=False)\n    def preprocess(self, x):\n        # normalize to [0,1]\n        x = kornia.geometry.resize(x, (224, 224),\n                                   interpolation='bicubic', align_corners=True,\n                                   antialias=self.antialias)"
        },
        {
            "comment": "The code defines a class with functions for encoding images using a vision transformer, freezing the model parameters, and normalizing input. It first renormalizes the input based on the clip, then encodes it using a vision transformer after preprocessing (e.g., reshaping), and optionally applies dual patchnorm if the input patchnorm of the model is set to True. The class also has methods to freeze the model parameters and forward the encoded image through the model.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/encoders/condition.py\":326-349",
            "content": "        x = (x + 1.) / 2.\n        # renormalize according to clip\n        x = kornia.enhance.normalize(x, self.mean, self.std)\n        return x\n    def freeze(self):\n        self.model = self.model.eval()\n        for param in self.model.parameters():\n            param.requires_grad = False\n    def forward(self, image, no_dropout=False): \n        ## image: b c h w\n        z = self.encode_with_vision_transformer(image)\n        return z\n    def encode_with_vision_transformer(self, x):\n        x = self.preprocess(x)\n        # to patches - whether to use dual patchnorm - https://arxiv.org/abs/2302.01327v1\n        if self.model.visual.input_patchnorm:\n            # einops - rearrange(x, 'b c (h p1) (w p2) -> b (h w) (c p1 p2)')\n            x = x.reshape(x.shape[0], x.shape[1], self.model.visual.grid_size[0], self.model.visual.patch_size[0], self.model.visual.grid_size[1], self.model.visual.patch_size[1])\n            x = x.permute(0, 2, 4, 1, 3, 5)\n            x = x.reshape(x.shape[0], self.model.visual.grid_size[0] * self.model.visual.grid_size[1], -1)"
        },
        {
            "comment": "This code performs convolution and normalization on input, reshapes the tensor, and concatenates class embeddings with positional embeddings. Then it applies patch dropout and layer normalization before passing the data to a Transformer model.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/encoders/condition.py\":350-369",
            "content": "            x = self.model.visual.patchnorm_pre_ln(x)\n            x = self.model.visual.conv1(x)\n        else:\n            x = self.model.visual.conv1(x)  # shape = [*, width, grid, grid]\n            x = x.reshape(x.shape[0], x.shape[1], -1)  # shape = [*, width, grid ** 2]\n            x = x.permute(0, 2, 1)  # shape = [*, grid ** 2, width]\n        # class embeddings and positional embeddings\n        x = torch.cat(\n            [self.model.visual.class_embedding.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device),\n             x], dim=1)  # shape = [*, grid ** 2 + 1, width]\n        x = x + self.model.visual.positional_embedding.to(x.dtype)\n        # a patch_dropout of 0. would mean it is disabled and this function would do nothing but return what was passed in\n        x = self.model.visual.patch_dropout(x)\n        x = self.model.visual.ln_pre(x)\n        x = x.permute(1, 0, 2)  # NLD -> LND\n        x = self.model.visual.transformer(x)\n        x = x.permute(1, 0, 2)  # LND -> NLD"
        },
        {
            "comment": "The code defines a `FrozenCLIPT5Encoder` class that inherits from the abstract base class `AbstractEncoder`. It initializes two encoders: a `FrozenCLIPEmbedder` and a `FrozenT5Embedder`, using specified versions, devices, and maximum lengths. The class overrides the default `encode` method to delegate encoding to its internal encoders, and defines a forward pass that combines encoded outputs from both encoders. The constructor also prints the number of parameters for each embedded model.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/encoders/condition.py\":371-388",
            "content": "        return x\nclass FrozenCLIPT5Encoder(AbstractEncoder):\n    def __init__(self, clip_version=\"openai/clip-vit-large-patch14\", t5_version=\"google/t5-v1_1-xl\", device=\"cuda\",\n                 clip_max_length=77, t5_max_length=77):\n        super().__init__()\n        self.clip_encoder = FrozenCLIPEmbedder(clip_version, device, max_length=clip_max_length)\n        self.t5_encoder = FrozenT5Embedder(t5_version, device, max_length=t5_max_length)\n        print(f\"{self.clip_encoder.__class__.__name__} has {count_params(self.clip_encoder) * 1.e-6:.2f} M parameters, \"\n              f\"{self.t5_encoder.__class__.__name__} comes with {count_params(self.t5_encoder) * 1.e-6:.2f} M params.\")\n    def encode(self, text):\n        return self(text)\n    def forward(self, text):\n        clip_z = self.clip_encoder.encode(text)\n        t5_z = self.t5_encoder.encode(text)\n        return [clip_z, t5_z]"
        }
    ]
}