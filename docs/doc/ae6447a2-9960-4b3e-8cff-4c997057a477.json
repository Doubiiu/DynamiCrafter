{
    "summary": "The code imports libraries, defines a DDPM sampling function, and includes functions for loading model checkpoints, handling prompts, videos, and data batching. The output is the variable 'z'.",
    "details": [
        {
            "comment": "This code imports various libraries and defines a function called \"batch_ddim_sampling\". This function uses DDIMSampler from the lvdm library to perform sampling with denoising diffusion probability model (DDPM). The function takes in several parameters including the model, conditional information, noise shape, number of samples, and optional arguments. The code also constructs unconditional guidance based on the model's unconditional type.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/scripts/evaluation/funcs.py\":0-28",
            "content": "import os, sys, glob\nimport numpy as np\nfrom collections import OrderedDict\nfrom decord import VideoReader, cpu\nimport cv2\nimport torch\nimport torchvision\nsys.path.insert(1, os.path.join(sys.path[0], '..', '..'))\nfrom lvdm.models.samplers.ddim import DDIMSampler\nfrom einops import rearrange\ndef batch_ddim_sampling(model, cond, noise_shape, n_samples=1, ddim_steps=50, ddim_eta=1.0,\\\n                        cfg_scale=1.0, temporal_cfg_scale=None, **kwargs):\n    ddim_sampler = DDIMSampler(model)\n    uncond_type = model.uncond_type\n    batch_size = noise_shape[0]\n    fs = cond[\"fs\"]\n    del cond[\"fs\"]\n    ## construct unconditional guidance\n    if cfg_scale != 1.0:\n        if uncond_type == \"empty_seq\":\n            prompts = batch_size * [\"\"]\n            #prompts = N * T * [\"\"]  ## if is_imgbatch=True\n            uc_emb = model.get_learned_conditioning(prompts)\n        elif uncond_type == \"zero_embed\":\n            c_emb = cond[\"c_crossattn\"][0] if isinstance(cond, dict) else cond\n            uc_emb = torch.zeros_like(c_emb)"
        },
        {
            "comment": "Applies image embedding to the input and concatenates it with conditioning, if provided. If a dictionary is passed as condition, updates the conditioning with the embedded image. Otherwise, sets the conditioning as the embedded image. Samples from model given parameters.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/scripts/evaluation/funcs.py\":30-56",
            "content": "        ## process image embedding token\n        if hasattr(model, 'embedder'):\n            uc_img = torch.zeros(noise_shape[0],3,224,224).to(model.device)\n            ## img: b c h w >> b l c\n            uc_img = model.embedder(uc_img)\n            uc_img = model.image_proj_model(uc_img)\n            uc_emb = torch.cat([uc_emb, uc_img], dim=1)\n        if isinstance(cond, dict):\n            uc = {key:cond[key] for key in cond.keys()}\n            uc.update({'c_crossattn': [uc_emb]})\n        else:\n            uc = uc_emb\n    else:\n        uc = None\n    x_T = None\n    batch_variants = []\n    for _ in range(n_samples):\n        if ddim_sampler is not None:\n            kwargs.update({\"clean_cond\": True})\n            samples, _ = ddim_sampler.sample(S=ddim_steps,\n                                            conditioning=cond,\n                                            batch_size=noise_shape[0],\n                                            shape=noise_shape[1:],\n                                            verbose=False,"
        },
        {
            "comment": "The code defines a function that takes in samples and returns variants of the samples reconstructed from latent to pixel space. It uses the DDIM algorithm for unconditional image generation, and includes options for various parameters such as conditional guidance scale and temporal configuration scale. The code also provides utility functions for getting file lists and directory lists.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/scripts/evaluation/funcs.py\":57-79",
            "content": "                                            unconditional_guidance_scale=cfg_scale,\n                                            unconditional_conditioning=uc,\n                                            eta=ddim_eta,\n                                            temporal_length=noise_shape[2],\n                                            conditional_guidance_scale_temporal=temporal_cfg_scale,\n                                            x_T=x_T,\n                                            fs=fs,\n                                            **kwargs\n                                            )\n        ## reconstruct from latent to pixel space\n        batch_images = model.decode_first_stage(samples)\n        batch_variants.append(batch_images)\n    ## batch, <samples>, c, t, h, w\n    batch_variants = torch.stack(batch_variants, dim=1)\n    return batch_variants\ndef get_filelist(data_dir, ext='*'):\n    file_list = glob.glob(os.path.join(data_dir, '*.%s'%ext))\n    file_list.sort()\n    return file_list\ndef get_dirlist(path):"
        },
        {
            "comment": "The code defines two functions: `load_model_checkpoint` and `load_prompts`. The `load_model_checkpoint` function attempts to load a model's checkpoint from a file, considering the potential use of Deepspeed library. It first tries loading the checkpoint as it is and if an error occurs, it reloads it without the \"module.\" prefix. The `load_prompts` function opens a file containing prompts for reading later use.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/scripts/evaluation/funcs.py\":80-111",
            "content": "    list = []\n    if (os.path.exists(path)):\n        files = os.listdir(path)\n        for file in files:\n            m = os.path.join(path,file)\n            if (os.path.isdir(m)):\n                list.append(m)\n    list.sort()\n    return list\ndef load_model_checkpoint(model, ckpt):\n    def load_checkpoint(model, ckpt, full_strict):\n        state_dict = torch.load(ckpt, map_location=\"cpu\")\n        try:\n            ## deepspeed\n            new_pl_sd = OrderedDict()\n            for key in state_dict['module'].keys():\n                new_pl_sd[key[16:]]=state_dict['module'][key]\n            model.load_state_dict(new_pl_sd, strict=full_strict)\n        except:\n            if \"state_dict\" in list(state_dict.keys()):\n                state_dict = state_dict[\"state_dict\"]\n            model.load_state_dict(state_dict, strict=full_strict)\n        return model\n    load_checkpoint(model, ckpt, full_strict=True)\n    print('>>> model checkpoint loaded.')\n    return model\ndef load_prompts(prompt_file):\n    f = open(prompt_file, 'r')"
        },
        {
            "comment": "This function reads a list of filepaths and processes each video file using VideoReader. It calculates the average frames per second (fps) for each video and determines the maximum valid number of frames based on the specified frame stride. If the requested number of video frames is less than or equal to -1, all frames are collected (fs=1 is a must). The function also checks if the frame stride is a positive integer. It returns the list of prompt strings and the processed batch tensor.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/scripts/evaluation/funcs.py\":112-138",
            "content": "    prompt_list = []\n    for idx, line in enumerate(f.readlines()):\n        l = line.strip()\n        if len(l) != 0:\n            prompt_list.append(l)\n        f.close()\n    return prompt_list\ndef load_video_batch(filepath_list, frame_stride, video_size=(256,256), video_frames=16):\n    '''\n    Notice about some special cases:\n    1. video_frames=-1 means to take all the frames (with fs=1)\n    2. when the total video frames is less than required, padding strategy will be used (repreated last frame)\n    '''\n    fps_list = []\n    batch_tensor = []\n    assert frame_stride > 0, \"valid frame stride should be a positive interge!\"\n    for filepath in filepath_list:\n        padding_num = 0\n        vidreader = VideoReader(filepath, ctx=cpu(0), width=video_size[1], height=video_size[0])\n        fps = vidreader.get_avg_fps()\n        total_frames = len(vidreader)\n        max_valid_frames = (total_frames-1) // frame_stride + 1\n        if video_frames < 0:\n            ## all frames are collected: fs=1 is a must\n            required_frames = total_frames"
        },
        {
            "comment": "This code extracts frames from a video and images from a list, resizes them, and stores them in batches. It handles cases where the number of frames required exceeds the available frames by padding with duplicate last frame(s). The function returns a tensor containing the batched image data.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/scripts/evaluation/funcs.py\":139-162",
            "content": "            frame_stride = 1\n        else:\n            required_frames = video_frames\n        query_frames = min(required_frames, max_valid_frames)\n        frame_indices = [frame_stride*i for i in range(query_frames)]\n        ## [t,h,w,c] -> [c,t,h,w]\n        frames = vidreader.get_batch(frame_indices)\n        frame_tensor = torch.tensor(frames.asnumpy()).permute(3, 0, 1, 2).float()\n        frame_tensor = (frame_tensor / 255. - 0.5) * 2\n        if max_valid_frames < required_frames:\n            padding_num = required_frames - max_valid_frames\n            frame_tensor = torch.cat([frame_tensor, *([frame_tensor[:,-1:,:,:]]*padding_num)], dim=1)\n            print(f'{os.path.split(filepath)[1]} is not long enough: {padding_num} frames padded.')\n        batch_tensor.append(frame_tensor)\n        sample_fps = int(fps/frame_stride)\n        fps_list.append(sample_fps)\n    return torch.stack(batch_tensor, dim=0)\nfrom PIL import Image\ndef load_image_batch(filepath_list, image_size=(256,256)):\n    batch_tensor = []\n    for filepath in filepath_list:"
        },
        {
            "comment": "This code reads a file and determines its format. If it's an mp4 file, it creates a VideoReader object to extract frames as tensors. If it's a png or jpg image, it reads the image using PIL, converts it to RGB, resizes it, and converts it to a tensor. For other formats, it raises an error. It then normalizes each tensor element between 0 and 1 before appending it to batch_tensor.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/scripts/evaluation/funcs.py\":163-180",
            "content": "        _, filename = os.path.split(filepath)\n        _, ext = os.path.splitext(filename)\n        if ext == '.mp4':\n            vidreader = VideoReader(filepath, ctx=cpu(0), width=image_size[1], height=image_size[0])\n            frame = vidreader.get_batch([0])\n            img_tensor = torch.tensor(frame.asnumpy()).squeeze(0).permute(2, 0, 1).float()\n        elif ext == '.png' or ext == '.jpg':\n            img = Image.open(filepath).convert(\"RGB\")\n            rgb_img = np.array(img, np.float32)\n            #bgr_img = cv2.imread(filepath, cv2.IMREAD_COLOR)\n            #bgr_img = cv2.cvtColor(bgr_img, cv2.COLOR_BGR2RGB)\n            rgb_img = cv2.resize(rgb_img, (image_size[1],image_size[0]), interpolation=cv2.INTER_LINEAR)\n            img_tensor = torch.from_numpy(rgb_img).permute(2, 0, 1).float()\n        else:\n            print(f'ERROR: <{ext}> image loading only support format: [mp4], [png], [jpg]')\n            raise NotImplementedError\n        img_tensor = (img_tensor / 255. - 0.5) * 2\n        batch_tensor.append(img_tensor)"
        },
        {
            "comment": "\"This code defines three functions: 'get_latent_z', 'save_videos', and 'rearrange'. It performs video encoding, saving videos, and tensor rearrangement respectively. The get_latent_z function takes a model and videos as input, encodes the videos using the first-stage encode of the model, and returns the encoded output. The save_videos function takes batch_tensors, savedir, filenames, and fps as inputs, processes each video tensor, saves the processed frames as an mp4 file with h264 codec.\"",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/scripts/evaluation/funcs.py\":181-203",
            "content": "    return torch.stack(batch_tensor, dim=0)\ndef save_videos(batch_tensors, savedir, filenames, fps=10):\n    # b,samples,c,t,h,w\n    n_samples = batch_tensors.shape[1]\n    for idx, vid_tensor in enumerate(batch_tensors):\n        video = vid_tensor.detach().cpu()\n        video = torch.clamp(video.float(), -1., 1.)\n        video = video.permute(2, 0, 1, 3, 4) # t,n,c,h,w\n        frame_grids = [torchvision.utils.make_grid(framesheet, nrow=int(n_samples)) for framesheet in video] #[3, 1*h, n*w]\n        grid = torch.stack(frame_grids, dim=0) # stack in temporal dim [t, 3, n*h, w]\n        grid = (grid + 1.0) / 2.0\n        grid = (grid * 255).to(torch.uint8).permute(0, 2, 3, 1)\n        savepath = os.path.join(savedir, f\"{filenames[idx]}.mp4\")\n        torchvision.io.write_video(savepath, grid, fps=fps, video_codec='h264', options={'crf': '10'})\ndef get_latent_z(model, videos):\n    b, c, t, h, w = videos.shape\n    x = rearrange(videos, 'b c t h w -> (b t) c h w')\n    z = model.encode_first_stage(x)\n    z = rearrange(z, '(b t) c h w -> b c t h w', b=b, t=t)"
        },
        {
            "comment": "This code snippet returns the variable 'z'.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/scripts/evaluation/funcs.py\":204-204",
            "content": "    return z"
        }
    ]
}