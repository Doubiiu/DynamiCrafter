{
    "summary": "This code provides a comprehensive X-Transformer implementation with positional embeddings, neural network modules, and multi-head attention configurations. The TransformerWrapper class initializes embedding layers, dropout layers, and defines a transformer module for inference tasks.",
    "details": [
        {
            "comment": "The code appears to be a part of the X-Transformers module, which implements transformer models for time series data. It includes classes such as AbsolutePositionalEmbedding and FixedPositionalEmbedding for adding positional embeddings to input sequences. It also includes namedtuples for storing intermediate results during forward passes of transformer layers. The code imports various functions and libraries from the torch and einops packages, indicating that it involves tensor manipulation and computations specific to transformer models.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/x_transformer.py\":0-40",
            "content": "\"\"\"shout-out to https://github.com/lucidrains/x-transformers/tree/main/x_transformers\"\"\"\nfrom functools import partial\nfrom inspect import isfunction\nfrom collections import namedtuple\nfrom einops import rearrange, repeat\nimport torch\nfrom torch import nn, einsum\nimport torch.nn.functional as F\n# constants\nDEFAULT_DIM_HEAD = 64\nIntermediates = namedtuple('Intermediates', [\n    'pre_softmax_attn',\n    'post_softmax_attn'\n])\nLayerIntermediates = namedtuple('Intermediates', [\n    'hiddens',\n    'attn_intermediates'\n])\nclass AbsolutePositionalEmbedding(nn.Module):\n    def __init__(self, dim, max_seq_len):\n        super().__init__()\n        self.emb = nn.Embedding(max_seq_len, dim)\n        self.init_()\n    def init_(self):\n        nn.init.normal_(self.emb.weight, std=0.02)\n    def forward(self, x):\n        n = torch.arange(x.shape[1], device=x.device)\n        return self.emb(n)[None, :, :]\nclass FixedPositionalEmbedding(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        inv_freq = 1. / (10000 ** (torch.arange(0, dim, 2).float() / dim))"
        },
        {
            "comment": "The code defines a module for transforming input based on sinusoidal functions and registers a buffer 'inv_freq'. The exists, default, always, not_equals, equals, max_neg_value functions are utility functions used for various conditions. pick_and_pop is used to select keys from the dictionary and remove them. group_dict_by_key splits a dictionary based on a condition.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/x_transformer.py\":41-92",
            "content": "        self.register_buffer('inv_freq', inv_freq)\n    def forward(self, x, seq_dim=1, offset=0):\n        t = torch.arange(x.shape[seq_dim], device=x.device).type_as(self.inv_freq) + offset\n        sinusoid_inp = torch.einsum('i , j -> i j', t, self.inv_freq)\n        emb = torch.cat((sinusoid_inp.sin(), sinusoid_inp.cos()), dim=-1)\n        return emb[None, :, :]\n# helpers\ndef exists(val):\n    return val is not None\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if isfunction(d) else d\ndef always(val):\n    def inner(*args, **kwargs):\n        return val\n    return inner\ndef not_equals(val):\n    def inner(x):\n        return x != val\n    return inner\ndef equals(val):\n    def inner(x):\n        return x == val\n    return inner\ndef max_neg_value(tensor):\n    return -torch.finfo(tensor.dtype).max\n# keyword argument helpers\ndef pick_and_pop(keys, d):\n    values = list(map(lambda key: d.pop(key), keys))\n    return dict(zip(keys, values))\ndef group_dict_by_key(cond, d):\n    return_val = [dict(), dict()]"
        },
        {
            "comment": "This code defines a Scale class that scales input using a provided function and a Rezero class that applies a zero-parameter function while also taking into account a \"g\" parameter. The group_by_key_prefix function groups dictionary keys with the same prefix, and the groupby_prefix_and_trim function further trims those keys after removing the prefix. The code snippet also includes partial definitions for cond, string_begins_with, and group_dict_by_key functions, which are likely used elsewhere in the codebase.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/x_transformer.py\":93-132",
            "content": "    for key in d.keys():\n        match = bool(cond(key))\n        ind = int(not match)\n        return_val[ind][key] = d[key]\n    return (*return_val,)\ndef string_begins_with(prefix, str):\n    return str.startswith(prefix)\ndef group_by_key_prefix(prefix, d):\n    return group_dict_by_key(partial(string_begins_with, prefix), d)\ndef groupby_prefix_and_trim(prefix, d):\n    kwargs_with_prefix, kwargs = group_dict_by_key(partial(string_begins_with, prefix), d)\n    kwargs_without_prefix = dict(map(lambda x: (x[0][len(prefix):], x[1]), tuple(kwargs_with_prefix.items())))\n    return kwargs_without_prefix, kwargs\n# classes\nclass Scale(nn.Module):\n    def __init__(self, value, fn):\n        super().__init__()\n        self.value = value\n        self.fn = fn\n    def forward(self, x, **kwargs):\n        x, *rest = self.fn(x, **kwargs)\n        return (x * self.value, *rest)\nclass Rezero(nn.Module):\n    def __init__(self, fn):\n        super().__init__()\n        self.fn = fn\n        self.g = nn.Parameter(torch.zeros(1))\n    def forward(self, x, **kwargs):"
        },
        {
            "comment": "This code defines several neural network modules for normalization and residual connection. The ScaleNorm class scales input by the reciprocal of the dimension, then divides by the L2 norm with optional epsilon to prevent division by zero. The RMSNorm class applies a similar process but also includes a learnable parameter. The Residual class performs element-wise addition between its two inputs. The GRUGating class contains a GRUCell for temporal processing and provides a gated output.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/x_transformer.py\":133-172",
            "content": "        x, *rest = self.fn(x, **kwargs)\n        return (x * self.g, *rest)\nclass ScaleNorm(nn.Module):\n    def __init__(self, dim, eps=1e-5):\n        super().__init__()\n        self.scale = dim ** -0.5\n        self.eps = eps\n        self.g = nn.Parameter(torch.ones(1))\n    def forward(self, x):\n        norm = torch.norm(x, dim=-1, keepdim=True) * self.scale\n        return x / norm.clamp(min=self.eps) * self.g\nclass RMSNorm(nn.Module):\n    def __init__(self, dim, eps=1e-8):\n        super().__init__()\n        self.scale = dim ** -0.5\n        self.eps = eps\n        self.g = nn.Parameter(torch.ones(dim))\n    def forward(self, x):\n        norm = torch.norm(x, dim=-1, keepdim=True) * self.scale\n        return x / norm.clamp(min=self.eps) * self.g\nclass Residual(nn.Module):\n    def forward(self, x, residual):\n        return x + residual\nclass GRUGating(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.gru = nn.GRUCell(dim, dim)\n    def forward(self, x, residual):\n        gated_output = self.gru("
        },
        {
            "comment": "The code defines a GEGLU layer, FeedForward layer, and Attention module used in the Transformer architecture. The GEGLU layer performs gated generalized linear unit operation, while the FeedForward layer consists of a feed-forward neural network with optional GEGLU activation. The Attention module implements multi-head attention mechanism for the Transformer. These layers are used to process and learn meaningful representations from input data in various deep learning tasks.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/x_transformer.py\":173-215",
            "content": "            rearrange(x, 'b n d -> (b n) d'),\n            rearrange(residual, 'b n d -> (b n) d')\n        )\n        return gated_output.reshape_as(x)\n# feedforward\nclass GEGLU(nn.Module):\n    def __init__(self, dim_in, dim_out):\n        super().__init__()\n        self.proj = nn.Linear(dim_in, dim_out * 2)\n    def forward(self, x):\n        x, gate = self.proj(x).chunk(2, dim=-1)\n        return x * F.gelu(gate)\nclass FeedForward(nn.Module):\n    def __init__(self, dim, dim_out=None, mult=4, glu=False, dropout=0.):\n        super().__init__()\n        inner_dim = int(dim * mult)\n        dim_out = default(dim_out, dim)\n        project_in = nn.Sequential(\n            nn.Linear(dim, inner_dim),\n            nn.GELU()\n        ) if not glu else GEGLU(dim, inner_dim)\n        self.net = nn.Sequential(\n            project_in,\n            nn.Dropout(dropout),\n            nn.Linear(inner_dim, dim_out)\n        )\n    def forward(self, x):\n        return self.net(x)\n# attention.\nclass Attention(nn.Module):\n    def __init__(\n            self,"
        },
        {
            "comment": "The code defines a class for a multi-head attention module. It initializes the class with various parameters and performs operations to calculate the attention scores. The use of entmax15 activation is currently not implemented, and if set to True, it raises a NotImplementedError. The scale, number of heads, causality, masking, dropout rate, and talking heads functionality are all configured within this code block.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/x_transformer.py\":216-247",
            "content": "            dim,\n            dim_head=DEFAULT_DIM_HEAD,\n            heads=8,\n            causal=False,\n            mask=None,\n            talking_heads=False,\n            sparse_topk=None,\n            use_entmax15=False,\n            num_mem_kv=0,\n            dropout=0.,\n            on_attn=False\n    ):\n        super().__init__()\n        if use_entmax15:\n            raise NotImplementedError(\"Check out entmax activation instead of softmax activation!\")\n        self.scale = dim_head ** -0.5\n        self.heads = heads\n        self.causal = causal\n        self.mask = mask\n        inner_dim = dim_head * heads\n        self.to_q = nn.Linear(dim, inner_dim, bias=False)\n        self.to_k = nn.Linear(dim, inner_dim, bias=False)\n        self.to_v = nn.Linear(dim, inner_dim, bias=False)\n        self.dropout = nn.Dropout(dropout)\n        # talking heads\n        self.talking_heads = talking_heads\n        if talking_heads:\n            self.pre_softmax_proj = nn.Parameter(torch.randn(heads, heads))\n            self.post_softmax_proj = nn.Parameter(torch.randn(heads, heads))"
        },
        {
            "comment": "This code defines a module for explicit top-k sparse attention with optional memory keys and values, and applies either softmax or entmax15 as the attention function. The forward pass takes in input tensor x and optional context, mask, rel_pos, sinusoidal_emb, prev_attn, and mem tensors. It uses default() to set default values for certain parameters. The code defines b, n, _, h, talking_heads, and device variables from the shape of x and module attributes. It then sets kv_input as either context or x, and q_input as x itself.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/x_transformer.py\":249-280",
            "content": "        # explicit topk sparse attention\n        self.sparse_topk = sparse_topk\n        # entmax\n        #self.attn_fn = entmax15 if use_entmax15 else F.softmax\n        self.attn_fn = F.softmax\n        # add memory key / values\n        self.num_mem_kv = num_mem_kv\n        if num_mem_kv > 0:\n            self.mem_k = nn.Parameter(torch.randn(heads, num_mem_kv, dim_head))\n            self.mem_v = nn.Parameter(torch.randn(heads, num_mem_kv, dim_head))\n        # attention on attention\n        self.attn_on_attn = on_attn\n        self.to_out = nn.Sequential(nn.Linear(inner_dim, dim * 2), nn.GLU()) if on_attn else nn.Linear(inner_dim, dim)\n    def forward(\n            self,\n            x,\n            context=None,\n            mask=None,\n            context_mask=None,\n            rel_pos=None,\n            sinusoidal_emb=None,\n            prev_attn=None,\n            mem=None\n    ):\n        b, n, _, h, talking_heads, device = *x.shape, self.heads, self.talking_heads, x.device\n        kv_input = default(context, x)\n        q_input = x"
        },
        {
            "comment": "This code initializes the input keys and values, concatenates previous memory if it exists, applies sinusoidal embeddings, performs linear transformations to obtain query, key, and value tensors. It then rearranges these tensors, optionally applies input masks based on provided mask or context_mask, and assigns default masks if necessary.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/x_transformer.py\":281-304",
            "content": "        k_input = kv_input\n        v_input = kv_input\n        if exists(mem):\n            k_input = torch.cat((mem, k_input), dim=-2)\n            v_input = torch.cat((mem, v_input), dim=-2)\n        if exists(sinusoidal_emb):\n            # in shortformer, the query would start at a position offset depending on the past cached memory\n            offset = k_input.shape[-2] - q_input.shape[-2]\n            q_input = q_input + sinusoidal_emb(q_input, offset=offset)\n            k_input = k_input + sinusoidal_emb(k_input)\n        q = self.to_q(q_input)\n        k = self.to_k(k_input)\n        v = self.to_v(v_input)\n        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=h), (q, k, v))\n        input_mask = None\n        if any(map(exists, (mask, context_mask))):\n            q_mask = default(mask, lambda: torch.ones((b, n), device=device).bool())\n            k_mask = q_mask if not exists(context) else context_mask\n            k_mask = default(k_mask, lambda: torch.ones((b, k.shape[-2]), device=device).bool())"
        },
        {
            "comment": "This code performs multi-head attention using the Transformer architecture. It applies masking, concatenates key memory with current keys, scales and calculates dot products for all heads. It also optionally applies talking head or relative position bias, and fills masked values with negative maximum value if a mask is present.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/x_transformer.py\":305-334",
            "content": "            q_mask = rearrange(q_mask, 'b i -> b () i ()')\n            k_mask = rearrange(k_mask, 'b j -> b () () j')\n            input_mask = q_mask * k_mask\n        if self.num_mem_kv > 0:\n            mem_k, mem_v = map(lambda t: repeat(t, 'h n d -> b h n d', b=b), (self.mem_k, self.mem_v))\n            k = torch.cat((mem_k, k), dim=-2)\n            v = torch.cat((mem_v, v), dim=-2)\n            if exists(input_mask):\n                input_mask = F.pad(input_mask, (self.num_mem_kv, 0), value=True)\n        dots = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n        mask_value = max_neg_value(dots)\n        if exists(prev_attn):\n            dots = dots + prev_attn\n        pre_softmax_attn = dots\n        if talking_heads:\n            dots = einsum('b h i j, h k -> b k i j', dots, self.pre_softmax_proj).contiguous()\n        if exists(rel_pos):\n            dots = rel_pos(dots)\n        if exists(input_mask):\n            dots.masked_fill_(~input_mask, mask_value)\n            del input_mask\n        if self.causal:"
        },
        {
            "comment": "This code snippet performs attention masking, top-k pruning, and computes the attention output for a transformer module. It masks irrelevant values, selects top-k values, applies attention, softmax normalization, dropout, and rearranges the output to its original form.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/x_transformer.py\":335-362",
            "content": "            i, j = dots.shape[-2:]\n            r = torch.arange(i, device=device)\n            mask = rearrange(r, 'i -> () () i ()') < rearrange(r, 'j -> () () () j')\n            mask = F.pad(mask, (j - i, 0), value=False)\n            dots.masked_fill_(mask, mask_value)\n            del mask\n        if exists(self.sparse_topk) and self.sparse_topk < dots.shape[-1]:\n            top, _ = dots.topk(self.sparse_topk, dim=-1)\n            vk = top[..., -1].unsqueeze(-1).expand_as(dots)\n            mask = dots < vk\n            dots.masked_fill_(mask, mask_value)\n            del mask\n        attn = self.attn_fn(dots, dim=-1)\n        post_softmax_attn = attn\n        attn = self.dropout(attn)\n        if talking_heads:\n            attn = einsum('b h i j, h k -> b k i j', attn, self.post_softmax_proj).contiguous()\n        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        intermediates = Intermediates(\n            pre_softmax_attn=pre_softmax_attn,\n            post_softmax_attn=post_softmax_attn"
        },
        {
            "comment": "The code defines a class `AttentionLayers` inheriting from `nn.Module`, which serves as an attention layer for deep learning models. It takes in parameters like `dim`, `depth`, `heads`, etc., to configure the attention mechanism. It also initializes other sub-modules based on given keyword arguments.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/x_transformer.py\":363-400",
            "content": "        )\n        return self.to_out(out), intermediates\nclass AttentionLayers(nn.Module):\n    def __init__(\n            self,\n            dim,\n            depth,\n            heads=8,\n            causal=False,\n            cross_attend=False,\n            only_cross=False,\n            use_scalenorm=False,\n            use_rmsnorm=False,\n            use_rezero=False,\n            rel_pos_num_buckets=32,\n            rel_pos_max_distance=128,\n            position_infused_attn=False,\n            custom_layers=None,\n            sandwich_coef=None,\n            par_ratio=None,\n            residual_attn=False,\n            cross_residual_attn=False,\n            macaron=False,\n            pre_norm=True,\n            gate_residual=False,\n            **kwargs\n    ):\n        super().__init__()\n        ff_kwargs, kwargs = groupby_prefix_and_trim('ff_', kwargs)\n        attn_kwargs, _ = groupby_prefix_and_trim('attn_', kwargs)\n        dim_head = attn_kwargs.get('dim_head', DEFAULT_DIM_HEAD)\n        self.dim = dim\n        self.depth = depth"
        },
        {
            "comment": "The code initializes module components based on input parameters. It sets layer list, position embedding (optional), rotary pos emb, asserts relative position conditions, assigns relative pos, pre-norm flag, residual attn flags and cross-residual attn flag. It also defines a norm function, an identity function and default block depending on the input parameters.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/x_transformer.py\":401-427",
            "content": "        self.layers = nn.ModuleList([])\n        self.has_pos_emb = position_infused_attn\n        self.pia_pos_emb = FixedPositionalEmbedding(dim) if position_infused_attn else None\n        self.rotary_pos_emb = always(None)\n        assert rel_pos_num_buckets <= rel_pos_max_distance, 'number of relative position buckets must be less than the relative position max distance'\n        self.rel_pos = None\n        self.pre_norm = pre_norm\n        self.residual_attn = residual_attn\n        self.cross_residual_attn = cross_residual_attn\n        norm_class = ScaleNorm if use_scalenorm else nn.LayerNorm\n        norm_class = RMSNorm if use_rmsnorm else norm_class\n        norm_fn = partial(norm_class, dim)\n        norm_fn = nn.Identity if use_rezero else norm_fn\n        branch_fn = Rezero if use_rezero else None\n        if cross_attend and not only_cross:\n            default_block = ('a', 'c', 'f')\n        elif cross_attend and only_cross:\n            default_block = ('c', 'f')\n        else:\n            default_block = ('a', 'f')"
        },
        {
            "comment": "This code block is responsible for setting the 'layer_types' variable based on different conditions. If 'macaron' exists, the default block is modified. If 'custom_layers' or 'par_ratio' exist, different blocks and layer types are determined accordingly. Additionally, if 'sandwich_coef' exists, it must be within specified limits.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/x_transformer.py\":429-446",
            "content": "        if macaron:\n            default_block = ('f',) + default_block\n        if exists(custom_layers):\n            layer_types = custom_layers\n        elif exists(par_ratio):\n            par_depth = depth * len(default_block)\n            assert 1 < par_ratio <= par_depth, 'par ratio out of range'\n            default_block = tuple(filter(not_equals('f'), default_block))\n            par_attn = par_depth // par_ratio\n            depth_cut = par_depth * 2 // 3  # 2 / 3 attention layer cutoff suggested by PAR paper\n            par_width = (depth_cut + depth_cut // par_attn) // par_attn\n            assert len(default_block) <= par_width, 'default block is too large for par_ratio'\n            par_block = default_block + ('f',) * (par_width - len(default_block))\n            par_head = par_block * par_attn\n            layer_types = par_head + ('f',) * (par_depth - len(par_head))\n        elif exists(sandwich_coef):\n            assert sandwich_coef > 0 and sandwich_coef <= depth, 'sandwich coefficient should be less than the depth'"
        },
        {
            "comment": "The code dynamically creates a list of layer types based on the sandwich coefficient, depth, and default block. It then initializes layers for each type ('a' for attention, 'c' for causal attention, 'f' for feedforward) and checks if there are any branches to apply using the branch_fn. If gate residual is enabled, a GRU gating mechanism is used; otherwise, a simple residual connection is used.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/x_transformer.py\":447-471",
            "content": "            layer_types = ('a',) * sandwich_coef + default_block * (depth - sandwich_coef) + ('f',) * sandwich_coef\n        else:\n            layer_types = default_block * depth\n        self.layer_types = layer_types\n        self.num_attn_layers = len(list(filter(equals('a'), layer_types)))\n        for layer_type in self.layer_types:\n            if layer_type == 'a':\n                layer = Attention(dim, heads=heads, causal=causal, **attn_kwargs)\n            elif layer_type == 'c':\n                layer = Attention(dim, heads=heads, **attn_kwargs)\n            elif layer_type == 'f':\n                layer = FeedForward(dim, **ff_kwargs)\n                layer = layer if not macaron else Scale(0.5, layer)\n            else:\n                raise Exception(f'invalid layer type {layer_type}')\n            if isinstance(layer, Attention) and exists(branch_fn):\n                layer = branch_fn(layer)\n            if gate_residual:\n                residual_fn = GRUGating(dim)\n            else:\n                residual_fn = Residual()"
        },
        {
            "comment": "This code defines a transformer module with layers that include normalization, block operations, and residual functions. The forward function iterates through the layer types and performs necessary calculations for each layer type 'a'. It stores intermediary results in hiddens and intermediates lists, and handles context, mask, mems, and return_hiddens parameters.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/x_transformer.py\":473-509",
            "content": "            self.layers.append(nn.ModuleList([\n                norm_fn(),\n                layer,\n                residual_fn\n            ]))\n    def forward(\n            self,\n            x,\n            context=None,\n            mask=None,\n            context_mask=None,\n            mems=None,\n            return_hiddens=False\n    ):\n        hiddens = []\n        intermediates = []\n        prev_attn = None\n        prev_cross_attn = None\n        mems = mems.copy() if exists(mems) else [None] * self.num_attn_layers\n        for ind, (layer_type, (norm, block, residual_fn)) in enumerate(zip(self.layer_types, self.layers)):\n            is_last = ind == (len(self.layers) - 1)\n            if layer_type == 'a':\n                hiddens.append(x)\n                layer_mem = mems.pop(0)\n            residual = x\n            if self.pre_norm:\n                x = norm(x)\n            if layer_type == 'a':\n                out, inter = block(x, mask=mask, sinusoidal_emb=self.pia_pos_emb, rel_pos=self.rel_pos,\n                                   prev_attn=prev_attn, mem=layer_mem)"
        },
        {
            "comment": "This code defines a function that applies a layer type-dependent operation to input 'x' and possibly stores intermediate results in 'intermediates'. The 'block' function is used, with optional parameters such as context, mask, context_mask, and prev_attn. If the layer type is 'a' or 'c', the intermediate result 'inter' is stored. If the layer type is 'a', the previous attention values are updated in 'prev_attn'. For 'c' layer types, the previous cross-attention values are updated in 'prev_cross_attn'. After processing all layers, if 'return_hiddens' is True, it returns both the final output 'x' and the intermediates stored in 'intermediates'. The 'Encoder' class extends the 'AttentionLayers' class with an additional constraint that causality cannot be set.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/x_transformer.py\":510-542",
            "content": "            elif layer_type == 'c':\n                out, inter = block(x, context=context, mask=mask, context_mask=context_mask, prev_attn=prev_cross_attn)\n            elif layer_type == 'f':\n                out = block(x)\n            x = residual_fn(out, residual)\n            if layer_type in ('a', 'c'):\n                intermediates.append(inter)\n            if layer_type == 'a' and self.residual_attn:\n                prev_attn = inter.pre_softmax_attn\n            elif layer_type == 'c' and self.cross_residual_attn:\n                prev_cross_attn = inter.pre_softmax_attn\n            if not self.pre_norm and not is_last:\n                x = norm(x)\n        if return_hiddens:\n            intermediates = LayerIntermediates(\n                hiddens=hiddens,\n                attn_intermediates=intermediates\n            )\n            return x, intermediates\n        return x\nclass Encoder(AttentionLayers):\n    def __init__(self, **kwargs):\n        assert 'causal' not in kwargs, 'cannot set causality on encoder'\n        super().__init__(causal=False, **kwargs)"
        },
        {
            "comment": "This code defines a class called TransformerWrapper that inherits from nn.Module. It takes several arguments including the number of tokens, maximum sequence length, attention layers type, embedding dimension, maximum memory length, embedding dropout rate, and whether to tie embedding weights. The class initializes an embedding layer, a positional embedding (if needed), and an embedding dropout layer. Additionally, it applies a linear projection if the embedding dimension is different from the model's dimension.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/x_transformer.py\":546-575",
            "content": "class TransformerWrapper(nn.Module):\n    def __init__(\n            self,\n            *,\n            num_tokens,\n            max_seq_len,\n            attn_layers,\n            emb_dim=None,\n            max_mem_len=0.,\n            emb_dropout=0.,\n            num_memory_tokens=None,\n            tie_embedding=False,\n            use_pos_emb=True\n    ):\n        super().__init__()\n        assert isinstance(attn_layers, AttentionLayers), 'attention layers must be one of Encoder or Decoder'\n        dim = attn_layers.dim\n        emb_dim = default(emb_dim, dim)\n        self.max_seq_len = max_seq_len\n        self.max_mem_len = max_mem_len\n        self.num_tokens = num_tokens\n        self.token_emb = nn.Embedding(num_tokens, emb_dim)\n        self.pos_emb = AbsolutePositionalEmbedding(emb_dim, max_seq_len) if (\n                    use_pos_emb and not attn_layers.has_pos_emb) else always(0)\n        self.emb_dropout = nn.Dropout(emb_dropout)\n        self.project_emb = nn.Linear(emb_dim, dim) if emb_dim != dim else nn.Identity()"
        },
        {
            "comment": "This code defines a transformer module with attention layers, layer normalization, and optional memory tokens. It initializes the token embedding weights to small random values and provides a forward pass for inference. Memory tokens are additional inputs for the transformer, used for tasks like key/value coding in memory transformers.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/x_transformer.py\":576-606",
            "content": "        self.attn_layers = attn_layers\n        self.norm = nn.LayerNorm(dim)\n        self.init_()\n        self.to_logits = nn.Linear(dim, num_tokens) if not tie_embedding else lambda t: t @ self.token_emb.weight.t()\n        # memory tokens (like [cls]) from Memory Transformers paper\n        num_memory_tokens = default(num_memory_tokens, 0)\n        self.num_memory_tokens = num_memory_tokens\n        if num_memory_tokens > 0:\n            self.memory_tokens = nn.Parameter(torch.randn(num_memory_tokens, dim))\n            # let funnel encoder know number of memory tokens, if specified\n            if hasattr(attn_layers, 'num_memory_tokens'):\n                attn_layers.num_memory_tokens = num_memory_tokens\n    def init_(self):\n        nn.init.normal_(self.token_emb.weight, std=0.02)\n    def forward(\n            self,\n            x,\n            return_embeddings=False,\n            mask=None,\n            return_mems=False,\n            return_attn=False,\n            mems=None,\n            **kwargs\n    ):\n        b, n, device, num_mem = *x.shape, x.device, self.num_memory_tokens"
        },
        {
            "comment": "The code snippet performs embedding operations on input \"x\" by applying token and position embeddings, dropping out some features using dropout layer, projecting the embedded features, and concatenating memory tokens if any. Then, it passes the result to attention layers for further processing, normalizes the output, separates the memory tokens from the rest, and converts the output into logits if required. Additionally, it returns the new memory states if specified, and the attention maps of the intermediate layers if asked.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/x_transformer.py\":607-635",
            "content": "        x = self.token_emb(x)\n        x += self.pos_emb(x)\n        x = self.emb_dropout(x)\n        x = self.project_emb(x)\n        if num_mem > 0:\n            mem = repeat(self.memory_tokens, 'n d -> b n d', b=b)\n            x = torch.cat((mem, x), dim=1)\n            # auto-handle masking after appending memory tokens\n            if exists(mask):\n                mask = F.pad(mask, (num_mem, 0), value=True)\n        x, intermediates = self.attn_layers(x, mask=mask, mems=mems, return_hiddens=True, **kwargs)\n        x = self.norm(x)\n        mem, x = x[:, :num_mem], x[:, num_mem:]\n        out = self.to_logits(x) if not return_embeddings else x\n        if return_mems:\n            hiddens = intermediates.hiddens\n            new_mems = list(map(lambda pair: torch.cat(pair, dim=-2), zip(mems, hiddens))) if exists(mems) else hiddens\n            new_mems = list(map(lambda t: t[..., -self.max_mem_len:, :].detach(), new_mems))\n            return out, new_mems\n        if return_attn:\n            attn_maps = list(map(lambda t: t.post_softmax_attn, intermediates.attn_intermediates))"
        },
        {
            "comment": "This code returns the output and attention maps from a function in the x_transformer module.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/x_transformer.py\":636-638",
            "content": "            return out, attn_maps\n        return out"
        }
    ]
}