{
    "summary": "The code offers utility functions for data processing, mixed-precision, tensor operations, and image/map detection. It also defines the `checkpoint` function to evaluate a function without caching while reducing memory usage with additional computational cost during backward passes.",
    "details": [
        {
            "comment": "\"gather_data\": Gathers data from multiple processes into a single list, optionally returning numpy arrays.\n\"autocast\": Enables automatic mixed-precision for functions using torch.cuda.amp.autocast().\n\"extract_into_tensor\": Extracts values from tensor 'a' at indices specified in tensor 't', reshaping as necessary.\n\"noise_like\": Creates random noise tensor of given shape on specified device, optionally repeatable for consecutive calls.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/common.py\":0-31",
            "content": "import math\nfrom inspect import isfunction\nimport torch\nfrom torch import nn\nimport torch.distributed as dist\ndef gather_data(data, return_np=True):\n    ''' gather data from multiple processes to one list '''\n    data_list = [torch.zeros_like(data) for _ in range(dist.get_world_size())]\n    dist.all_gather(data_list, data)  # gather not supported with NCCL\n    if return_np:\n        data_list = [data.cpu().numpy() for data in data_list]\n    return data_list\ndef autocast(f):\n    def do_autocast(*args, **kwargs):\n        with torch.cuda.amp.autocast(enabled=True,\n                                     dtype=torch.get_autocast_gpu_dtype(),\n                                     cache_enabled=torch.is_autocast_cache_enabled()):\n            return f(*args, **kwargs)\n    return do_autocast\ndef extract_into_tensor(a, t, x_shape):\n    b, *_ = t.shape\n    out = a.gather(-1, t)\n    return out.reshape(b, *((1,) * (len(x_shape) - 1)))\ndef noise_like(shape, device, repeat=False):\n    repeat_noise = lambda: torch.randn((1, *shape[1:]), device=device).repeat(shape[0], *((1,) * (len(shape) - 1)))"
        },
        {
            "comment": "This code contains several utility functions, including noise generation, checking if a value exists, identity mapping, unique elements extraction, calculating mean over non-batch dimensions, image and map detection, max negative value calculation, shape conversion to string, and tensor initialization.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/common.py\":32-76",
            "content": "    noise = lambda: torch.randn(shape, device=device)\n    return repeat_noise() if repeat else noise()\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if isfunction(d) else d\ndef exists(val):\n    return val is not None\ndef identity(*args, **kwargs):\n    return nn.Identity()\ndef uniq(arr):\n    return{el: True for el in arr}.keys()\ndef mean_flat(tensor):\n    \"\"\"\n    Take the mean over all non-batch dimensions.\n    \"\"\"\n    return tensor.mean(dim=list(range(1, len(tensor.shape))))\ndef ismap(x):\n    if not isinstance(x, torch.Tensor):\n        return False\n    return (len(x.shape) == 4) and (x.shape[1] > 3)\ndef isimage(x):\n    if not isinstance(x,torch.Tensor):\n        return False\n    return (len(x.shape) == 4) and (x.shape[1] == 3 or x.shape[1] == 1)\ndef max_neg_value(t):\n    return -torch.finfo(t.dtype).max\ndef shape_to_str(x):\n    shape_str = \"x\".join([str(x) for x in x.shape])\n    return shape_str\ndef init_(tensor):\n    dim = tensor.shape[-1]\n    std = 1 / math.sqrt(dim)\n    tensor.uniform_(-std, std)"
        },
        {
            "comment": "This code defines a function called `checkpoint` that evaluates a given function without caching intermediate activations. It reduces memory usage by incurring additional computational cost during backward passes. The function takes four arguments: the function to evaluate, input arguments for the function, parameters the function depends on but does not explicitly take as arguments, and a flag indicating whether to use gradient checkpointing or not.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/common.py\":77-93",
            "content": "    return tensor\nckpt = torch.utils.checkpoint.checkpoint\ndef checkpoint(func, inputs, params, flag):\n    \"\"\"\n    Evaluate a function without caching intermediate activations, allowing for\n    reduced memory at the expense of extra compute in the backward pass.\n    :param func: the function to evaluate.\n    :param inputs: the argument sequence to pass to `func`.\n    :param params: a sequence of parameters `func` depends on but does not\n                   explicitly take as arguments.\n    :param flag: if False, disable gradient checkpointing.\n    \"\"\"\n    if flag:\n        return ckpt(func, *inputs, use_reentrant=False)\n    else:\n        return func(*inputs)"
        }
    ]
}