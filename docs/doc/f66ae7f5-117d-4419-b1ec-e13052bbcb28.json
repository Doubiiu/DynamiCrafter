{
    "summary": "This code handles data loading, model checkpoints, and DDIMSampler for deep learning models in video synthesis, ensuring compatibility, GPU usage, file existence checks, and prompted example saving. It uses argparse and random seeds for command-line arguments and inference tracking. Looping video generation not supported.",
    "details": [
        {
            "comment": "This code imports necessary libraries and defines functions for loading data, handling model checkpoints, and using DDIMSampler. It also provides a way to load model checkpoints into the specified model with strict loading. The get_filelist function retrieves file lists from a given directory with specified postfixes, sorting them before returning. This code is part of a larger program likely involving image generation or manipulation using a specific model.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/scripts/evaluation/inference.py\":0-30",
            "content": "import argparse, os, sys, glob\nimport datetime, time\nfrom omegaconf import OmegaConf\nfrom tqdm import tqdm\nfrom einops import rearrange, repeat\nfrom collections import OrderedDict\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\nfrom pytorch_lightning import seed_everything\nfrom PIL import Image\nsys.path.insert(1, os.path.join(sys.path[0], '..', '..'))\nfrom lvdm.models.samplers.ddim import DDIMSampler\nfrom lvdm.models.samplers.ddim_multiplecond import DDIMSampler as DDIMSampler_multicond\nfrom utils.utils import instantiate_from_config\ndef get_filelist(data_dir, postfixes):\n    patterns = [os.path.join(data_dir, f\"*.{postfix}\") for postfix in postfixes]\n    file_list = []\n    for pattern in patterns:\n        file_list.extend(glob.glob(pattern))\n    file_list.sort()\n    return file_list\ndef load_model_checkpoint(model, ckpt):\n    state_dict = torch.load(ckpt, map_location=\"cpu\")\n    if \"state_dict\" in list(state_dict.keys()):\n        state_dict = state_dict[\"state_dict\"]\n        model.load_state_dict(state_dict, strict=True)"
        },
        {
            "comment": "Code handles model checkpoint loading, prompt loading, and data preparation. It includes functions for loading the model checkpoint, reading prompts from a file, and applying image transformations to prepare the data.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/scripts/evaluation/inference.py\":31-61",
            "content": "    else:\n        # deepspeed\n        new_pl_sd = OrderedDict()\n        for key in state_dict['module'].keys():\n            new_pl_sd[key[16:]]=state_dict['module'][key]\n        model.load_state_dict(new_pl_sd)\n    print('>>> model checkpoint loaded.')\n    return model\ndef load_prompts(prompt_file):\n    f = open(prompt_file, 'r')\n    prompt_list = []\n    for idx, line in enumerate(f.readlines()):\n        l = line.strip()\n        if len(l) != 0:\n            prompt_list.append(l)\n        f.close()\n    return prompt_list\ndef load_data_prompts(data_dir, video_size=(256,256), video_frames=16, gfi=False):\n    transform = transforms.Compose([\n        transforms.Resize(min(video_size)),\n        transforms.CenterCrop(video_size),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))])\n    ## load prompts\n    prompt_file = get_filelist(data_dir, ['txt'])\n    assert len(prompt_file) > 0, \"Error: found NO prompt file!\"\n    ###### default prompt\n    default_idx = 0\n    default_idx = min(default_idx, len(prompt_file)-1)"
        },
        {
            "comment": "This function loads multiple prompt files and selects the first one alphabetically. It then extracts image data from the specified directory, converts it to a suitable tensor format, and stores the filename and data in separate lists. Finally, it returns these lists along with the prompt file list for further processing or saving.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/scripts/evaluation/inference.py\":62-85",
            "content": "    if len(prompt_file) > 1:\n        print(f\"Warning: multiple prompt files exist. The one {os.path.split(prompt_file[default_idx])[1]} is used.\")\n    ## only use the first one (sorted by name) if multiple exist\n    ## load video\n    file_list = get_filelist(data_dir, ['jpg', 'png', 'jpeg', 'JPEG', 'PNG'])\n    # assert len(file_list) == n_samples, \"Error: data and prompts are NOT paired!\"\n    data_list = []\n    filename_list = []\n    prompt_list = load_prompts(prompt_file[default_idx])\n    n_samples = len(prompt_list)\n    for idx in range(n_samples):\n        image = Image.open(file_list[idx]).convert('RGB')\n        image_tensor = transform(image).unsqueeze(1) # [c,1,h,w]\n        frame_tensor = repeat(image_tensor, 'c t h w -> c (repeat t) h w', repeat=video_frames)\n        data_list.append(frame_tensor)\n        _, filename = os.path.split(file_list[idx])\n        filename_list.append(filename)\n    return filename_list, data_list, prompt_list\ndef save_results(prompt, samples, filename, fakedir, fps=8, loop=False):"
        },
        {
            "comment": "The code saves a video from the input samples and writes it to a specified directory. It processes the video by clamping values, performing necessary permutations, stacking frames, and adjusting brightness before writing it out in .mp4 format using ffmpeg's h264 codec with a quality indicator (CRF).",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/scripts/evaluation/inference.py\":86-111",
            "content": "    filename = filename.split('.')[0]+'.mp4'\n    prompt = prompt[0] if isinstance(prompt, list) else prompt\n    ## save video\n    videos = [samples]\n    savedirs = [fakedir]\n    for idx, video in enumerate(videos):\n        if video is None:\n            continue\n        # b,c,t,h,w\n        video = video.detach().cpu()\n        video = torch.clamp(video.float(), -1., 1.)\n        n = video.shape[0]\n        video = video.permute(2, 0, 1, 3, 4) # t,n,c,h,w\n        if loop:\n            video = video[:-1,...]\n        frame_grids = [torchvision.utils.make_grid(framesheet, nrow=int(n), padding=0) for framesheet in video] #[3, 1*h, n*w]\n        grid = torch.stack(frame_grids, dim=0) # stack in temporal dim [t, 3, h, n*w]\n        grid = (grid + 1.0) / 2.0\n        grid = (grid * 255).to(torch.uint8).permute(0, 2, 3, 1)\n        path = os.path.join(savedirs[idx], filename)\n        torchvision.io.write_video(path, grid, fps=fps, video_codec='h264', options={'crf': '10'}) ## crf indicates the quality\ndef save_results_seperate(prompt, samples, filename, fakedir, fps=10, loop=False):"
        },
        {
            "comment": "This code saves videos by processing them through a deep learning model. The model encodes the video frames, then rearranges and saves each frame as an individual mp4 file.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/scripts/evaluation/inference.py\":112-137",
            "content": "    prompt = prompt[0] if isinstance(prompt, list) else prompt\n    ## save video\n    videos = [samples]\n    savedirs = [fakedir]\n    for idx, video in enumerate(videos):\n        if video is None:\n            continue\n        # b,c,t,h,w\n        video = video.detach().cpu()\n        if loop: # remove the last frame\n            video = video[:,:,:-1,...]\n        video = torch.clamp(video.float(), -1., 1.)\n        n = video.shape[0]\n        for i in range(n):\n            grid = video[i,...]\n            grid = (grid + 1.0) / 2.0\n            grid = (grid * 255).to(torch.uint8).permute(1, 2, 3, 0) #thwc\n            path = os.path.join(savedirs[idx].replace('samples', 'samples_separate'), f'{filename.split(\".\")[0]}_sample{i}.mp4')\n            torchvision.io.write_video(path, grid, fps=fps, video_codec='h264', options={'crf': '10'})\ndef get_latent_z(model, videos):\n    b, c, t, h, w = videos.shape\n    x = rearrange(videos, 'b c t h w -> (b t) c h w')\n    z = model.encode_first_stage(x)\n    z = rearrange(z, '(b t) c h w -> b c t h w', b=b, t=t)"
        },
        {
            "comment": "This function performs image-guided synthesis using a DDIM sampler. It takes a model, prompts, videos, noise shape, and optional parameters as input. The code first creates a DDIM sampler depending on the multiple_cond_cfg flag. Then, it sets batch size and initializes fs tensor. If text_input is False, prompts are set to empty strings for all batch elements. It extracts image embeddings and applies model's image projection. Next, it retrieves conditioning embedding from the model. Finally, if the model uses hybrid conditioning, it calculates latent z and creates img_cat_cond tensor.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/scripts/evaluation/inference.py\":138-161",
            "content": "    return z\ndef image_guided_synthesis(model, prompts, videos, noise_shape, n_samples=1, ddim_steps=50, ddim_eta=1., \\\n                        unconditional_guidance_scale=1.0, cfg_img=None, fs=None, text_input=False, multiple_cond_cfg=False, loop=False, gfi=False, **kwargs):\n    ddim_sampler = DDIMSampler(model) if not multiple_cond_cfg else DDIMSampler_multicond(model)\n    batch_size = noise_shape[0]\n    fs = torch.tensor([fs] * batch_size, dtype=torch.long, device=model.device)\n    if not text_input:\n        prompts = [\"\"]*batch_size\n    img = videos[:,:,0] #bchw\n    img_emb = model.embedder(img) ## blc\n    img_emb = model.image_proj_model(img_emb)\n    cond_emb = model.get_learned_conditioning(prompts)\n    cond = {\"c_crossattn\": [torch.cat([cond_emb,img_emb], dim=1)]}\n    if model.model.conditioning_key == 'hybrid':\n        z = get_latent_z(model, videos) # b c t h w\n        if loop or gfi:\n            img_cat_cond = torch.zeros_like(z)\n            img_cat_cond[:,:,0,:,:] = z[:,:,0,:,:]\n            img_cat_cond[:,:,-1,:,:] = z[:,:,-1,:,:]"
        },
        {
            "comment": "The code is setting up unconditional and conditional inputs for a model. It checks the type of conditioning, creates or retrieves embedding vectors, and concatenates them with image embeddings. If the model requires hybrid conditioning, it adds the image to the unconditional input as well. The code also handles cases where multiple conditions are used and adjusts the inputs accordingly.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/scripts/evaluation/inference.py\":162-184",
            "content": "        else:\n            img_cat_cond = z[:,:,:1,:,:]\n            img_cat_cond = repeat(img_cat_cond, 'b c t h w -> b c (repeat t) h w', repeat=z.shape[2])\n        cond[\"c_concat\"] = [img_cat_cond] # b c 1 h w\n    if unconditional_guidance_scale != 1.0:\n        if model.uncond_type == \"empty_seq\":\n            prompts = batch_size * [\"\"]\n            uc_emb = model.get_learned_conditioning(prompts)\n        elif model.uncond_type == \"zero_embed\":\n            uc_emb = torch.zeros_like(cond_emb)\n        uc_img_emb = model.embedder(torch.zeros_like(img)) ## b l c\n        uc_img_emb = model.image_proj_model(uc_img_emb)\n        uc = {\"c_crossattn\": [torch.cat([uc_emb,uc_img_emb],dim=1)]}\n        if model.model.conditioning_key == 'hybrid':\n            uc[\"c_concat\"] = [img_cat_cond]\n    else:\n        uc = None\n    ## we need one more unconditioning image=yes, text=\"\"\n    if multiple_cond_cfg and cfg_img != 1.0:\n        uc_2 = {\"c_crossattn\": [torch.cat([uc_emb,img_emb],dim=1)]}\n        if model.model.conditioning_key == 'hybrid':"
        },
        {
            "comment": "This code is setting up arguments for a conditional diffusion model. It checks if a specific conditioning image is provided and updates the necessary variables accordingly. It then uses the DDIM sampler to generate samples, with options for unconditional guidance scale and eta. The final output depends on whether the conditioning image is provided or not.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/scripts/evaluation/inference.py\":185-210",
            "content": "            uc_2[\"c_concat\"] = [img_cat_cond]\n        kwargs.update({\"unconditional_conditioning_img_nonetext\": uc_2})\n    else:\n        kwargs.update({\"unconditional_conditioning_img_nonetext\": None})\n    z0 = None\n    cond_mask = None\n    batch_variants = []\n    for _ in range(n_samples):\n        if z0 is not None:\n            cond_z0 = z0.clone()\n            kwargs.update({\"clean_cond\": True})\n        else:\n            cond_z0 = None\n        if ddim_sampler is not None:\n            samples, _ = ddim_sampler.sample(S=ddim_steps,\n                                            conditioning=cond,\n                                            batch_size=batch_size,\n                                            shape=noise_shape[1:],\n                                            verbose=False,\n                                            unconditional_guidance_scale=unconditional_guidance_scale,\n                                            unconditional_conditioning=uc,\n                                            eta=ddim_eta,"
        },
        {
            "comment": "The code snippet is part of an inference function. It loads a model configuration and instantiates the model using the configuration. The model's use_checkpoint is set to False, possibly due to compatibility issues with DeepSpeed. The model is then moved to a specific GPU for computation.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/scripts/evaluation/inference.py\":211-234",
            "content": "                                            cfg_img=cfg_img, \n                                            mask=cond_mask,\n                                            x0=cond_z0,\n                                            fs=fs,\n                                            **kwargs\n                                            )\n        ## reconstruct from latent to pixel space\n        batch_images = model.decode_first_stage(samples)\n        batch_variants.append(batch_images)\n    ## variants, batch, c, t, h, w\n    batch_variants = torch.stack(batch_variants)\n    return batch_variants.permute(1, 0, 2, 3, 4, 5)\ndef run_inference(args, gpu_num, gpu_no):\n    ## model config\n    config = OmegaConf.load(args.config)\n    model_config = config.pop(\"model\", OmegaConf.create())\n    ## set use_checkpoint as False as when using deepspeed, it encounters an error \"deepspeed backend not set\"\n    model_config['params']['unet_config']['params']['use_checkpoint'] = False\n    model = instantiate_from_config(model_config)\n    model = model.cuda(gpu_no)"
        },
        {
            "comment": "The code asserts if the checkpoint file and prompt directory exist, loads the model with the checkpoint path, evaluates the model, checks for image size compatibility, enforces batch size of 1, calculates latent noise shape, creates output directories, and loads data prompts from the specified directory.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/scripts/evaluation/inference.py\":236-258",
            "content": "    assert os.path.exists(args.ckpt_path), \"Error: checkpoint Not Found!\"\n    model = load_model_checkpoint(model, args.ckpt_path)\n    model.eval()\n    ## run over data\n    assert (args.height % 16 == 0) and (args.width % 16 == 0), \"Error: image size [h,w] should be multiples of 16!\"\n    assert args.bs == 1, \"Current implementation only support [batch size = 1]!\"\n    ## latent noise shape\n    h, w = args.height // 8, args.width // 8\n    channels = model.model.diffusion_model.out_channels\n    n_frames = args.video_length\n    print(f'Inference with {n_frames} frames')\n    noise_shape = [args.bs, channels, n_frames, h, w]\n    fakedir = os.path.join(args.savedir, \"samples\")\n    fakedir_separate = os.path.join(args.savedir, \"samples_separate\")\n    # os.makedirs(fakedir, exist_ok=True)\n    os.makedirs(fakedir_separate, exist_ok=True)\n    ## prompt file setting\n    assert os.path.exists(args.prompt_dir), \"Error: prompt file Not Found!\"\n    filename_list, data_list, prompt_list = load_data_prompts(args.prompt_dir, video_size=(args.height, args.width), video_frames=n_frames, gfi=args.gfi)"
        },
        {
            "comment": "This code selects a subset of prompts, data, and filenames for evaluation or inference. It assigns the selected items to specific GPUs based on their indices, and then processes them using automatic mixed precision (AMP). The loop iterates over the selected subsets, stacks videos if they are in a list format, and moves all data to the GPU. This code ensures efficient distribution of tasks across available GPUs for faster processing.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/scripts/evaluation/inference.py\":259-277",
            "content": "    num_samples = len(prompt_list)\n    samples_split = num_samples // gpu_num\n    print('Prompts testing [rank:%d] %d/%d samples loaded.'%(gpu_no, samples_split, num_samples))\n    #indices = random.choices(list(range(0, num_samples)), k=samples_per_device)\n    indices = list(range(samples_split*gpu_no, samples_split*(gpu_no+1)))\n    prompt_list_rank = [prompt_list[i] for i in indices]\n    data_list_rank = [data_list[i] for i in indices]\n    filename_list_rank = [filename_list[i] for i in indices]\n    start = time.time()\n    with torch.no_grad(), torch.cuda.amp.autocast():\n        for idx, indice in tqdm(enumerate(range(0, len(prompt_list_rank), args.bs)), desc='Sample Batch'):\n            prompts = prompt_list_rank[indice:indice+args.bs]\n            videos = data_list_rank[indice:indice+args.bs]\n            filenames = filename_list_rank[indice:indice+args.bs]\n            if isinstance(videos, list):\n                videos = torch.stack(videos, dim=0).to(\"cuda\")\n            else:\n                videos = videos.unsqueeze(0).to(\"cuda\")"
        },
        {
            "comment": "This code performs image generation using a model and prompts, saving each generated example individually. It utilizes a function called `image_guided_synthesis` to create batches of samples, then saves them with their corresponding prompt and filename. The results are saved in the specified directory, and the time taken is printed.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/scripts/evaluation/inference.py\":279-296",
            "content": "            batch_samples = image_guided_synthesis(model, prompts, videos, noise_shape, args.n_samples, args.ddim_steps, args.ddim_eta, \\\n                                args.unconditional_guidance_scale, args.cfg_img, args.frame_stride, args.text_input, args.multiple_cond_cfg, args.loop, args.gfi)\n            ## save each example individually\n            for nn, samples in enumerate(batch_samples):\n                ## samples : [n_samples,c,t,h,w]\n                prompt = prompts[nn]\n                filename = filenames[nn]\n                # save_results(prompt, samples, filename, fakedir, fps=8, loop=args.loop)\n                save_results_seperate(prompt, samples, filename, fakedir, fps=8, loop=args.loop)\n    print(f\"Saved in {args.savedir}. Time used: {(time.time() - start):.2f} seconds\")\ndef get_parser():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--savedir\", type=str, default=None, help=\"results saving path\")\n    parser.add_argument(\"--ckpt_path\", type=str, default=None, help=\"checkpoint path\")"
        },
        {
            "comment": "This code snippet is parsing command-line arguments for an inference script. It defines the config file path, data directory, number of samples per prompt, DDPM/DDim parameters, batch size, and image dimensions, as well as frame stride control options.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/scripts/evaluation/inference.py\":297-305",
            "content": "    parser.add_argument(\"--config\", type=str, help=\"config (yaml) path\")\n    parser.add_argument(\"--prompt_dir\", type=str, default=None, help=\"a data dir containing videos and prompts\")\n    parser.add_argument(\"--n_samples\", type=int, default=1, help=\"num of samples per prompt\",)\n    parser.add_argument(\"--ddim_steps\", type=int, default=50, help=\"steps of ddim if positive, otherwise use DDPM\",)\n    parser.add_argument(\"--ddim_eta\", type=float, default=1.0, help=\"eta for ddim sampling (0.0 yields deterministic sampling)\",)\n    parser.add_argument(\"--bs\", type=int, default=1, help=\"batch size for inference, should be one\")\n    parser.add_argument(\"--height\", type=int, default=512, help=\"image height, in pixel space\")\n    parser.add_argument(\"--width\", type=int, default=512, help=\"image width, in pixel space\")\n    parser.add_argument(\"--frame_stride\", type=int, default=3, choices=[1, 2, 3, 4, 5, 6], help=\"frame stride control for results, smaller value->smaller motion magnitude and more stable, and vice versa\")"
        },
        {
            "comment": "The code above defines command line arguments using the \"argparse\" module in Python. These options control features such as unconditional guidance scale, seed value for randomization, video length, negative prompt usage, input type (text or image), and additional conditioning configurations. The looping video generation and generative frame interpolation are currently not supported.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/scripts/evaluation/inference.py\":306-316",
            "content": "    parser.add_argument(\"--unconditional_guidance_scale\", type=float, default=1.0, help=\"prompt classifier-free guidance\")\n    parser.add_argument(\"--seed\", type=int, default=123, help=\"seed for seed_everything\")\n    parser.add_argument(\"--video_length\", type=int, default=16, help=\"inference video length\")\n    parser.add_argument(\"--negative_prompt\", action='store_true', default=False, help=\"negative prompt\")\n    parser.add_argument(\"--text_input\", action='store_true', default=False, help=\"input text to I2V model or not\")\n    parser.add_argument(\"--multiple_cond_cfg\", action='store_true', default=False, help=\"use multi-condition cfg or not\")\n    parser.add_argument(\"--cfg_img\", type=float, default=None, help=\"guidance scale for image conditioning\")\n    ## currently not support looping video and generative frame interpolation\n    parser.add_argument(\"--loop\", action='store_true', default=False, help=\"generate looping videos or not\")\n    parser.add_argument(\"--gfi\", action='store_true', default=False, help=\"generate generative frame interpolation (gfi) or not\")"
        },
        {
            "comment": "This code sets up a command-line argument parser, then seeds the random number generator based on an input seed and runs inference with specified GPU number and rank. The timestamp is printed for tracking when the inference process started.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/scripts/evaluation/inference.py\":317-328",
            "content": "    return parser\nif __name__ == '__main__':\n    now = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n    print(\"@CoVideoGen cond-Inference: %s\"%now)\n    parser = get_parser()\n    args = parser.parse_args()\n    seed_everything(args.seed)\n    rank, gpu_num = 0, 1\n    run_inference(args, gpu_num, rank)"
        }
    ]
}