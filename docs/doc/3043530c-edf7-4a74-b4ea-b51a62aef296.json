{
    "summary": "The code introduces memory-efficient components for multi-head attention in transformer models, with support for causal/self-attention and sequential block processing. It also includes a SpatialSelfAttention class for CV tasks using GEGLU and FeedForward.",
    "details": [
        {
            "comment": "This code defines a module called \"RelativePosition\" which initializes an embedding table for relative positioning. It takes in the number of units and maximum relative position as parameters. The forward function calculates the embeddings based on input sequence lengths, q and k.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/attention.py\":0-32",
            "content": "import torch\nfrom torch import nn, einsum\nimport torch.nn.functional as F\nfrom einops import rearrange, repeat\nfrom functools import partial\ntry:\n    import xformers\n    import xformers.ops\n    XFORMERS_IS_AVAILBLE = True\nexcept:\n    XFORMERS_IS_AVAILBLE = False\nfrom lvdm.common import (\n    checkpoint,\n    exists,\n    default,\n)\nfrom lvdm.basics import zero_module\nclass RelativePosition(nn.Module):\n    \"\"\" https://github.com/evelinehong/Transformer_Relative_Position_PyTorch/blob/master/relative_position.py \"\"\"\n    def __init__(self, num_units, max_relative_position):\n        super().__init__()\n        self.num_units = num_units\n        self.max_relative_position = max_relative_position\n        self.embeddings_table = nn.Parameter(torch.Tensor(max_relative_position * 2 + 1, num_units))\n        nn.init.xavier_uniform_(self.embeddings_table)\n    def forward(self, length_q, length_k):\n        device = self.embeddings_table.device\n        range_vec_q = torch.arange(length_q, device=device)\n        range_vec_k = torch.arange(length_k, device=device)"
        },
        {
            "comment": "This code defines a class named \"CrossAttention\" which is a subclass of nn.Module and uses attention mechanism for cross-modal attention. It takes query and context embeddings as input, computes the attention weights based on their relative distances, and produces attention-weighted summed embeddings as output. The maximum relative position can be controlled by the max_relative_position variable.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/attention.py\":33-53",
            "content": "        distance_mat = range_vec_k[None, :] - range_vec_q[:, None]\n        distance_mat_clipped = torch.clamp(distance_mat, -self.max_relative_position, self.max_relative_position)\n        final_mat = distance_mat_clipped + self.max_relative_position\n        final_mat = final_mat.long()\n        embeddings = self.embeddings_table[final_mat]\n        return embeddings\nclass CrossAttention(nn.Module):\n    def __init__(self, query_dim, context_dim=None, heads=8, dim_head=64, dropout=0., \n                 relative_position=False, temporal_length=None, video_length=None, image_cross_attention=False, image_cross_attention_scale=1.0, image_cross_attention_scale_learnable=False, text_context_len=77):\n        super().__init__()\n        inner_dim = dim_head * heads\n        context_dim = default(context_dim, query_dim)\n        self.scale = dim_head**-0.5\n        self.heads = heads\n        self.dim_head = dim_head\n        self.to_q = nn.Linear(query_dim, inner_dim, bias=False)\n        self.to_k = nn.Linear(context_dim, inner_dim, bias=False)"
        },
        {
            "comment": "This code initializes a module for attention mechanisms. It includes a linear layer, dropout, and optional relative position layers depending on the input type (spatial or temporal). The code also sets parameters like video length, image cross-attention settings, and text context length. If XFORMERS library is available and temporal length is None, it assigns an efficient forward function for spatial attention.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/attention.py\":54-72",
            "content": "        self.to_v = nn.Linear(context_dim, inner_dim, bias=False)\n        self.to_out = nn.Sequential(nn.Linear(inner_dim, query_dim), nn.Dropout(dropout))\n        self.relative_position = relative_position\n        if self.relative_position:\n            assert(temporal_length is not None)\n            self.relative_position_k = RelativePosition(num_units=dim_head, max_relative_position=temporal_length)\n            self.relative_position_v = RelativePosition(num_units=dim_head, max_relative_position=temporal_length)\n        else:\n            ## only used for spatial attention, while NOT for temporal attention\n            if XFORMERS_IS_AVAILBLE and temporal_length is None:\n                self.forward = self.efficient_forward\n        self.video_length = video_length\n        self.image_cross_attention = image_cross_attention\n        self.image_cross_attention_scale = image_cross_attention_scale\n        self.text_context_len = text_context_len\n        self.image_cross_attention_scale_learnable = image_cross_attention_scale_learnable"
        },
        {
            "comment": "This code defines a module for attention mechanism, where it checks if image cross-attention is enabled. If enabled and spatial self-attention is not used, it performs cross-attention between text and image contexts. It initializes the necessary linear layers and registers an optional learnable parameter 'alpha'. It also handles default inputs for context and mask parameters.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/attention.py\":73-97",
            "content": "        if self.image_cross_attention:\n            self.to_k_ip = nn.Linear(context_dim, inner_dim, bias=False)\n            self.to_v_ip = nn.Linear(context_dim, inner_dim, bias=False)\n            if image_cross_attention_scale_learnable:\n                self.register_parameter('alpha', nn.Parameter(torch.tensor(0.)) )\n    def forward(self, x, context=None, mask=None):\n        spatial_self_attn = (context is None)\n        k_ip, v_ip, out_ip = None, None, None\n        h = self.heads\n        q = self.to_q(x)\n        context = default(context, x)\n        if self.image_cross_attention and not spatial_self_attn:\n            context, context_image = context[:,:self.text_context_len,:], context[:,self.text_context_len:,:]\n            k = self.to_k(context)\n            v = self.to_v(context)\n            k_ip = self.to_k_ip(context_image)\n            v_ip = self.to_v_ip(context_image)\n        else:\n            if not spatial_self_attn:\n                context = context[:,:self.text_context_len,:]\n            k = self.to_k(context)"
        },
        {
            "comment": "This code block performs multi-head self-attention with relative position information and optional causal attention masking. It first rearranges the query, key, and value matrices to match the required dimensions. Then it calculates the scaled dot product between the rearranged key and query matrices. If relative position is enabled, it adds another scaled dot product calculated from the relative position-aware key matrix. The attention scores are then computed by normalizing the combined similarity scores using softmax function. Finally, the value matrix is weighted by the computed attention scores to produce the output.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/attention.py\":98-122",
            "content": "            v = self.to_v(context)\n        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h=h), (q, k, v))\n        sim = torch.einsum('b i d, b j d -> b i j', q, k) * self.scale\n        if self.relative_position:\n            len_q, len_k, len_v = q.shape[1], k.shape[1], v.shape[1]\n            k2 = self.relative_position_k(len_q, len_k)\n            sim2 = einsum('b t d, t s d -> b t s', q, k2) * self.scale # TODO check \n            sim += sim2\n        del k\n        if exists(mask):\n            ## feasible for causal attention mask only\n            max_neg_value = -torch.finfo(sim.dtype).max\n            mask = repeat(mask, 'b i j -> (b h) i j', h=h)\n            sim.masked_fill_(~(mask>0.5), max_neg_value)\n        # attention, what we cannot get enough of\n        sim = sim.softmax(dim=-1)\n        out = torch.einsum('b i j, b j d -> b i d', sim, v)\n        if self.relative_position:\n            v2 = self.relative_position_v(len_q, len_v)\n            out2 = einsum('b t s, t s d -> b t d', sim, v2) # TODO check"
        },
        {
            "comment": "This code performs multi-head attention, with the option to include image cross-attention. It combines query and key-value pairs into a single output tensor, then rearranges the dimensions for further processing. If image cross-attention is enabled, it scales and adds the image cross-attention results accordingly. Finally, it passes the result through a transformation function before returning it.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/attention.py\":123-149",
            "content": "            out += out2\n        out = rearrange(out, '(b h) n d -> b n (h d)', h=h)\n        ## for image cross-attention\n        if k_ip is not None:\n            k_ip, v_ip = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h=h), (k_ip, v_ip))\n            sim_ip =  torch.einsum('b i d, b j d -> b i j', q, k_ip) * self.scale\n            del k_ip\n            sim_ip = sim_ip.softmax(dim=-1)\n            out_ip = torch.einsum('b i j, b j d -> b i d', sim_ip, v_ip)\n            out_ip = rearrange(out_ip, '(b h) n d -> b n (h d)', h=h)\n        if out_ip is not None:\n            if self.image_cross_attention_scale_learnable:\n                out = out + self.image_cross_attention_scale * out_ip * (torch.tanh(self.alpha)+1)\n            else:\n                out = out + self.image_cross_attention_scale * out_ip\n        return self.to_out(out)\n    def efficient_forward(self, x, context=None, mask=None):\n        spatial_self_attn = (context is None)\n        k_ip, v_ip, out_ip = None, None, None\n        q = self.to_q(x)"
        },
        {
            "comment": "This code splits the context into text and image parts if the `image_cross_attention` flag is set, then transforms and reshapes the query (q), key (k), and value (v) tensors for memory-efficient attention computation using XFormers library's `memory_efficient_attention()`.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/attention.py\":150-174",
            "content": "        context = default(context, x)\n        if self.image_cross_attention and not spatial_self_attn:\n            context, context_image = context[:,:self.text_context_len,:], context[:,self.text_context_len:,:]\n            k = self.to_k(context)\n            v = self.to_v(context)\n            k_ip = self.to_k_ip(context_image)\n            v_ip = self.to_v_ip(context_image)\n        else:\n            if not spatial_self_attn:\n                context = context[:,:self.text_context_len,:]\n            k = self.to_k(context)\n            v = self.to_v(context)\n        b, _, _ = q.shape\n        q, k, v = map(\n            lambda t: t.unsqueeze(3)\n            .reshape(b, t.shape[1], self.heads, self.dim_head)\n            .permute(0, 2, 1, 3)\n            .reshape(b * self.heads, t.shape[1], self.dim_head)\n            .contiguous(),\n            (q, k, v),\n        )\n        # actually compute the attention, what we cannot get enough of\n        out = xformers.ops.memory_efficient_attention(q, k, v, attn_bias=None, op=None)"
        },
        {
            "comment": "This code is performing multi-head attention for an image-based model. It handles cross-attention when `k_ip` is not None, and reshapes tensors for efficient memory usage. If a mask exists, it raises a NotImplementedError. Finally, it reshapes the output tensor for the final result.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/attention.py\":176-202",
            "content": "        ## for image cross-attention\n        if k_ip is not None:\n            k_ip, v_ip = map(\n                lambda t: t.unsqueeze(3)\n                .reshape(b, t.shape[1], self.heads, self.dim_head)\n                .permute(0, 2, 1, 3)\n                .reshape(b * self.heads, t.shape[1], self.dim_head)\n                .contiguous(),\n                (k_ip, v_ip),\n            )\n            out_ip = xformers.ops.memory_efficient_attention(q, k_ip, v_ip, attn_bias=None, op=None)\n            out_ip = (\n                out_ip.unsqueeze(0)\n                .reshape(b, self.heads, out.shape[1], self.dim_head)\n                .permute(0, 2, 1, 3)\n                .reshape(b, out.shape[1], self.heads * self.dim_head)\n            )\n        if exists(mask):\n            raise NotImplementedError\n        out = (\n            out.unsqueeze(0)\n            .reshape(b, self.heads, out.shape[1], self.dim_head)\n            .permute(0, 2, 1, 3)\n            .reshape(b, out.shape[1], self.heads * self.dim_head)\n        )\n        if out_ip is not None:"
        },
        {
            "comment": "The code defines a BasicTransformerBlock class with parameters dim, n_heads, d_head, dropout, context_dim, gated_ff, checkpoint, disable_self_attn, attention_cls, video_length, image_cross_attention, image_cross_attention_scale, and image_cross_attention_scale_learnable. It initializes an instance of a CrossAttention class (or specified attention_cls) as attn1 for self-attention or cross-attention, and a FeedForward layer as ff for the feed forward network.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/attention.py\":203-220",
            "content": "            if self.image_cross_attention_scale_learnable:\n                out = out + self.image_cross_attention_scale * out_ip * (torch.tanh(self.alpha)+1)\n            else:\n                out = out + self.image_cross_attention_scale * out_ip\n        return self.to_out(out)\nclass BasicTransformerBlock(nn.Module):\n    def __init__(self, dim, n_heads, d_head, dropout=0., context_dim=None, gated_ff=True, checkpoint=True,\n                disable_self_attn=False, attention_cls=None, video_length=None, image_cross_attention=False, image_cross_attention_scale=1.0, image_cross_attention_scale_learnable=False, text_context_len=77):\n        super().__init__()\n        attn_cls = CrossAttention if attention_cls is None else attention_cls\n        self.disable_self_attn = disable_self_attn\n        self.attn1 = attn_cls(query_dim=dim, heads=n_heads, dim_head=d_head, dropout=dropout,\n            context_dim=context_dim if self.disable_self_attn else None)\n        self.ff = FeedForward(dim, dropout=dropout, glu=gated_ff)"
        },
        {
            "comment": "This code creates an attention module and defines its forward pass, allowing for masking and context inputs. It also incorporates checkpointing to prevent issues with non-tensor arguments like None or scalars. The input tuple is constructed to ensure that all relevant inputs are passed correctly during the forward pass.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/attention.py\":221-236",
            "content": "        self.attn2 = attn_cls(query_dim=dim, context_dim=context_dim, heads=n_heads, dim_head=d_head, dropout=dropout, video_length=video_length, image_cross_attention=image_cross_attention, image_cross_attention_scale=image_cross_attention_scale, image_cross_attention_scale_learnable=image_cross_attention_scale_learnable,text_context_len=text_context_len)\n        self.image_cross_attention = image_cross_attention\n        self.norm1 = nn.LayerNorm(dim)\n        self.norm2 = nn.LayerNorm(dim)\n        self.norm3 = nn.LayerNorm(dim)\n        self.checkpoint = checkpoint\n    def forward(self, x, context=None, mask=None, **kwargs):\n        ## implementation tricks: because checkpointing doesn't support non-tensor (e.g. None or scalar) arguments\n        input_tuple = (x,)      ## should not be (x), otherwise *input_tuple will decouple x into multiple arguments\n        if context is not None:\n            input_tuple = (x, context)\n        if mask is not None:\n            forward_mask = partial(self._forward, mask=mask)"
        },
        {
            "comment": "This code defines a Transformer block for image-like data with spatial axis. It first projects the input (embedding), then reshapes to b, t, d and applies standard transformer action, followed by reshaping back into an image. The class 'SpatialTransformer' initializes the module with various parameters including number of heads, dropout rate, context_dim, use_checkpoint, disable_self_attn, use_linear, video_length and image_cross_attention.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/attention.py\":237-260",
            "content": "            return checkpoint(forward_mask, (x,), self.parameters(), self.checkpoint)\n        return checkpoint(self._forward, input_tuple, self.parameters(), self.checkpoint)\n    def _forward(self, x, context=None, mask=None):\n        x = self.attn1(self.norm1(x), context=context if self.disable_self_attn else None, mask=mask) + x\n        x = self.attn2(self.norm2(x), context=context, mask=mask) + x\n        x = self.ff(self.norm3(x)) + x\n        return x\nclass SpatialTransformer(nn.Module):\n    \"\"\"\n    Transformer block for image-like data in spatial axis.\n    First, project the input (aka embedding)\n    and reshape to b, t, d.\n    Then apply standard transformer action.\n    Finally, reshape to image\n    NEW: use_linear for more efficiency instead of the 1x1 convs\n    \"\"\"\n    def __init__(self, in_channels, n_heads, d_head, depth=1, dropout=0., context_dim=None,\n                 use_checkpoint=True, disable_self_attn=False, use_linear=False, video_length=None,\n                 image_cross_attention=False, image_cross_attention_scale_learnable=False):"
        },
        {
            "comment": "This code initializes an attention module with specified parameters. It creates a basic transformer block for a given number of depths, using either convolutional or linear projection, and applies normalization. The rest of the parameters are used to configure the specific transformer block implementation.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/attention.py\":261-284",
            "content": "        super().__init__()\n        self.in_channels = in_channels\n        inner_dim = n_heads * d_head\n        self.norm = torch.nn.GroupNorm(num_groups=32, num_channels=in_channels, eps=1e-6, affine=True)\n        if not use_linear:\n            self.proj_in = nn.Conv2d(in_channels, inner_dim, kernel_size=1, stride=1, padding=0)\n        else:\n            self.proj_in = nn.Linear(in_channels, inner_dim)\n        attention_cls = None\n        self.transformer_blocks = nn.ModuleList([\n            BasicTransformerBlock(\n                inner_dim,\n                n_heads,\n                d_head,\n                dropout=dropout,\n                context_dim=context_dim,\n                disable_self_attn=disable_self_attn,\n                checkpoint=use_checkpoint,\n                attention_cls=attention_cls,\n                video_length=video_length,\n                image_cross_attention=image_cross_attention,\n                image_cross_attention_scale_learnable=image_cross_attention_scale_learnable,\n                ) for d in range(depth)"
        },
        {
            "comment": "The code defines a module for an attention-based transformer block that operates on image-like data. It includes parameters for the number of input and output channels, inner dimensions, and kernel size. The forward function applies normalization, projection, transformer blocks, and rearrangement operations to process the input tensor, adding it back at the end. The TemporalTransformer class extends this module specifically for temporal data.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/attention.py\":285-315",
            "content": "        ])\n        if not use_linear:\n            self.proj_out = zero_module(nn.Conv2d(inner_dim, in_channels, kernel_size=1, stride=1, padding=0))\n        else:\n            self.proj_out = zero_module(nn.Linear(inner_dim, in_channels))\n        self.use_linear = use_linear\n    def forward(self, x, context=None, **kwargs):\n        b, c, h, w = x.shape\n        x_in = x\n        x = self.norm(x)\n        if not self.use_linear:\n            x = self.proj_in(x)\n        x = rearrange(x, 'b c h w -> b (h w) c').contiguous()\n        if self.use_linear:\n            x = self.proj_in(x)\n        for i, block in enumerate(self.transformer_blocks):\n            x = block(x, context=context, **kwargs)\n        if self.use_linear:\n            x = self.proj_out(x)\n        x = rearrange(x, 'b (h w) c -> b c h w', h=h, w=w).contiguous()\n        if not self.use_linear:\n            x = self.proj_out(x)\n        return x + x_in\nclass TemporalTransformer(nn.Module):\n    \"\"\"\n    Transformer block for image-like data in temporal axis.\n    First, reshape to b, t, d."
        },
        {
            "comment": "This code initializes a Transformer module with specified parameters. It checks if only self-attention is used, if relative position encoding is enabled, and whether causal attention should be applied. It also sets up convolutional or linear projection layers accordingly, as well as normalization layer.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/attention.py\":316-335",
            "content": "    Then apply standard transformer action.\n    Finally, reshape to image\n    \"\"\"\n    def __init__(self, in_channels, n_heads, d_head, depth=1, dropout=0., context_dim=None,\n                 use_checkpoint=True, use_linear=False, only_self_att=True, causal_attention=False, causal_block_size=1,\n                 relative_position=False, temporal_length=None):\n        super().__init__()\n        self.only_self_att = only_self_att\n        self.relative_position = relative_position\n        self.causal_attention = causal_attention\n        self.causal_block_size = causal_block_size\n        self.in_channels = in_channels\n        inner_dim = n_heads * d_head\n        self.norm = torch.nn.GroupNorm(num_groups=32, num_channels=in_channels, eps=1e-6, affine=True)\n        self.proj_in = nn.Conv1d(in_channels, inner_dim, kernel_size=1, stride=1, padding=0)\n        if not use_linear:\n            self.proj_in = nn.Conv1d(in_channels, inner_dim, kernel_size=1, stride=1, padding=0)\n        else:\n            self.proj_in = nn.Linear(in_channels, inner_dim)"
        },
        {
            "comment": "The code is configuring a transformer model with various options and arguments. It creates an attention layer depending on the relative position flag and temporal length. If causal attention is enabled, it creates a mask to enforce causality. If only self-attention is desired, it sets the context dimension as None. Finally, it builds a list of transformer blocks based on input dimensions and options.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/attention.py\":337-360",
            "content": "        if relative_position:\n            assert(temporal_length is not None)\n            attention_cls = partial(CrossAttention, relative_position=True, temporal_length=temporal_length)\n        else:\n            attention_cls = partial(CrossAttention, temporal_length=temporal_length)\n        if self.causal_attention:\n            assert(temporal_length is not None)\n            self.mask = torch.tril(torch.ones([1, temporal_length, temporal_length]))\n        if self.only_self_att:\n            context_dim = None\n        self.transformer_blocks = nn.ModuleList([\n            BasicTransformerBlock(\n                inner_dim,\n                n_heads,\n                d_head,\n                dropout=dropout,\n                context_dim=context_dim,\n                attention_cls=attention_cls,\n                checkpoint=use_checkpoint) for d in range(depth)\n        ])\n        if not use_linear:\n            self.proj_out = zero_module(nn.Conv1d(inner_dim, in_channels, kernel_size=1, stride=1, padding=0))\n        else:"
        },
        {
            "comment": "This code defines a class for an attention module in a neural network. It includes linear projection and normalization operations, and supports causal attention and self-attention. The forward method processes input x through these operations, potentially applying linear projections depending on use_linear flag, and may apply a mask based on the causal_attention and only_self_att flags. If context is provided, cross-attention will be used; otherwise, self-attention is defaulted. The transformer_blocks are applied sequentially to process the input x.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/attention.py\":361-389",
            "content": "            self.proj_out = zero_module(nn.Linear(inner_dim, in_channels))\n        self.use_linear = use_linear\n    def forward(self, x, context=None):\n        b, c, t, h, w = x.shape\n        x_in = x\n        x = self.norm(x)\n        x = rearrange(x, 'b c t h w -> (b h w) c t').contiguous()\n        if not self.use_linear:\n            x = self.proj_in(x)\n        x = rearrange(x, 'bhw c t -> bhw t c').contiguous()\n        if self.use_linear:\n            x = self.proj_in(x)\n        temp_mask = None\n        if self.causal_attention:\n            # slice the from mask map\n            temp_mask = self.mask[:,:t,:t].to(x.device)\n        if temp_mask is not None:\n            mask = temp_mask.to(x.device)\n            mask = repeat(mask, 'l i j -> (l bhw) i j', bhw=b*h*w)\n        else:\n            mask = None\n        if self.only_self_att:\n            ## note: if no context is given, cross-attention defaults to self-attention\n            for i, block in enumerate(self.transformer_blocks):\n                x = block(x, mask=mask)"
        },
        {
            "comment": "This code performs self-attention and possibly cross-attention using a Transformer block. It rearranges input dimensions for the attention operation, applies the transformer block to each batch individually due to memory constraints, and optionally applies a linear layer at the end.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/attention.py\":390-408",
            "content": "            x = rearrange(x, '(b hw) t c -> b hw t c', b=b).contiguous()\n        else:\n            x = rearrange(x, '(b hw) t c -> b hw t c', b=b).contiguous()\n            context = rearrange(context, '(b t) l con -> b t l con', t=t).contiguous()\n            for i, block in enumerate(self.transformer_blocks):\n                # calculate each batch one by one (since number in shape could not greater then 65,535 for some package)\n                for j in range(b):\n                    context_j = repeat(\n                        context[j],\n                        't l con -> (t r) l con', r=(h * w) // t, t=t).contiguous()\n                    ## note: causal mask will not applied in cross-attention case\n                    x[j] = block(x[j], context=context_j)\n        if self.use_linear:\n            x = self.proj_out(x)\n            x = rearrange(x, 'b (h w) t c -> b c t h w', h=h, w=w).contiguous()\n        if not self.use_linear:\n            x = rearrange(x, 'b hw t c -> (b hw) c t').contiguous()\n            x = self.proj_out(x)"
        },
        {
            "comment": "Code snippet defines several neural network modules for attention mechanisms, including rearrange function and classes like GEGLU, FeedForward, and LinearAttention. These modules are used in computer vision tasks to improve model performance by selectively focusing on important features in the input data.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/attention.py\":409-447",
            "content": "            x = rearrange(x, '(b h w) c t -> b c t h w', b=b, h=h, w=w).contiguous()\n        return x + x_in\nclass GEGLU(nn.Module):\n    def __init__(self, dim_in, dim_out):\n        super().__init__()\n        self.proj = nn.Linear(dim_in, dim_out * 2)\n    def forward(self, x):\n        x, gate = self.proj(x).chunk(2, dim=-1)\n        return x * F.gelu(gate)\nclass FeedForward(nn.Module):\n    def __init__(self, dim, dim_out=None, mult=4, glu=False, dropout=0.):\n        super().__init__()\n        inner_dim = int(dim * mult)\n        dim_out = default(dim_out, dim)\n        project_in = nn.Sequential(\n            nn.Linear(dim, inner_dim),\n            nn.GELU()\n        ) if not glu else GEGLU(dim, inner_dim)\n        self.net = nn.Sequential(\n            project_in,\n            nn.Dropout(dropout),\n            nn.Linear(inner_dim, dim_out)\n        )\n    def forward(self, x):\n        return self.net(x)\nclass LinearAttention(nn.Module):\n    def __init__(self, dim, heads=4, dim_head=32):\n        super().__init__()\n        self.heads = heads"
        },
        {
            "comment": "This code defines a class for SpatialSelfAttention. It initializes a GroupNorm layer and three convolutional layers in the constructor. The forward method performs spatial self-attention, rearranging the input, calculating attention scores, and producing output with the defined convolutional layers applied.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/attention.py\":448-472",
            "content": "        hidden_dim = dim_head * heads\n        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias = False)\n        self.to_out = nn.Conv2d(hidden_dim, dim, 1)\n    def forward(self, x):\n        b, c, h, w = x.shape\n        qkv = self.to_qkv(x)\n        q, k, v = rearrange(qkv, 'b (qkv heads c) h w -> qkv b heads c (h w)', heads = self.heads, qkv=3)\n        k = k.softmax(dim=-1)  \n        context = torch.einsum('bhdn,bhen->bhde', k, v)\n        out = torch.einsum('bhde,bhdn->bhen', context, q)\n        out = rearrange(out, 'b heads c (h w) -> b (heads c) h w', heads=self.heads, h=h, w=w)\n        return self.to_out(out)\nclass SpatialSelfAttention(nn.Module):\n    def __init__(self, in_channels):\n        super().__init__()\n        self.in_channels = in_channels\n        self.norm = torch.nn.GroupNorm(num_groups=32, num_channels=in_channels, eps=1e-6, affine=True)\n        self.q = torch.nn.Conv2d(in_channels,\n                                 in_channels,\n                                 kernel_size=1,\n                                 stride=1,"
        },
        {
            "comment": "This code defines a class for attention module using Conv2d layers in PyTorch. It initializes three convolution layers (q, k, and v) with the same input and output channels but different kernel sizes. The forward function applies normalization, passes the input through the q, k, and v layers, then computes attention using rearranged tensors for queries, keys, and values.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/attention.py\":473-500",
            "content": "                                 padding=0)\n        self.k = torch.nn.Conv2d(in_channels,\n                                 in_channels,\n                                 kernel_size=1,\n                                 stride=1,\n                                 padding=0)\n        self.v = torch.nn.Conv2d(in_channels,\n                                 in_channels,\n                                 kernel_size=1,\n                                 stride=1,\n                                 padding=0)\n        self.proj_out = torch.nn.Conv2d(in_channels,\n                                        in_channels,\n                                        kernel_size=1,\n                                        stride=1,\n                                        padding=0)\n    def forward(self, x):\n        h_ = x\n        h_ = self.norm(h_)\n        q = self.q(h_)\n        k = self.k(h_)\n        v = self.v(h_)\n        # compute attention\n        b,c,h,w = q.shape\n        q = rearrange(q, 'b c h w -> b (h w) c')\n        k = rearrange(k, 'b c h w -> b c (h w)')"
        },
        {
            "comment": "This code snippet is performing multi-head attention in a transformer model. It calculates the weights (`w_`) for each head, applies softmax to normalize them, and then attends to the values by multiplying with the rearranged value matrix (`v`). The resulting hidden states (`h_`) are passed through a final projection layer before being added back to the input (`x`) and returned.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/attention.py\":501-513",
            "content": "        w_ = torch.einsum('bij,bjk->bik', q, k)\n        w_ = w_ * (int(c)**(-0.5))\n        w_ = torch.nn.functional.softmax(w_, dim=2)\n        # attend to values\n        v = rearrange(v, 'b c h w -> b c (h w)')\n        w_ = rearrange(w_, 'b i j -> b j i')\n        h_ = torch.einsum('bij,bjk->bik', v, w_)\n        h_ = rearrange(h_, 'b c (h w) -> b c h w', h=h)\n        h_ = self.proj_out(h_)\n        return x+h_"
        }
    ]
}