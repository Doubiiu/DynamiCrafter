{
    "summary": "The code defines AbstractDistribution class with sample, mode methods and derived classes DiracDistribution, DiagonalGaussianDistribution for implementing specific distributions. It calculates KL divergence between Gaussian distributions, normalizes samples, computes negative log-likelihood, and provides mode function. Used in image generation and machine learning tasks. The function calculates KL divergence between two Gaussian distributions using mean1, logvar1, mean2, and logvar2 as inputs, ensuring at least one argument is a tensor for efficient computation.",
    "details": [
        {
            "comment": "The code defines an AbstractDistribution class with sample and mode methods, along with two derived classes (DiracDistribution and DiagonalGaussianDistribution) for implementing specific distributions. The DiracDistribution represents a delta function at a specified value, while DiagonalGaussianDistribution represents a diagonal Gaussian distribution with learnable mean and log variance parameters. The latter also has options to make the distribution deterministic or stochastic by setting the std and var accordingly.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/distributions.py\":0-38",
            "content": "import torch\nimport numpy as np\nclass AbstractDistribution:\n    def sample(self):\n        raise NotImplementedError()\n    def mode(self):\n        raise NotImplementedError()\nclass DiracDistribution(AbstractDistribution):\n    def __init__(self, value):\n        self.value = value\n    def sample(self):\n        return self.value\n    def mode(self):\n        return self.value\nclass DiagonalGaussianDistribution(object):\n    def __init__(self, parameters, deterministic=False):\n        self.parameters = parameters\n        self.mean, self.logvar = torch.chunk(parameters, 2, dim=1)\n        self.logvar = torch.clamp(self.logvar, -30.0, 20.0)\n        self.deterministic = deterministic\n        self.std = torch.exp(0.5 * self.logvar)\n        self.var = torch.exp(self.logvar)\n        if self.deterministic:\n            self.var = self.std = torch.zeros_like(self.mean).to(device=self.parameters.device)\n    def sample(self, noise=None):\n        if noise is None:\n            noise = torch.randn(self.mean.shape)\n        x = self.mean + self.std * noise.to(device=self.parameters.device)"
        },
        {
            "comment": "The code calculates the Kullback-Leibler (KL) divergence between two Gaussian distributions, normalizes the input sample based on its mean and variance, and computes the negative log-likelihood of the input sample. It also provides a mode function for retrieving the mean value. The code is used in image generation and other machine learning tasks that involve probability distributions.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/distributions.py\":39-69",
            "content": "        return x\n    def kl(self, other=None):\n        if self.deterministic:\n            return torch.Tensor([0.])\n        else:\n            if other is None:\n                return 0.5 * torch.sum(torch.pow(self.mean, 2)\n                                       + self.var - 1.0 - self.logvar,\n                                       dim=[1, 2, 3])\n            else:\n                return 0.5 * torch.sum(\n                    torch.pow(self.mean - other.mean, 2) / other.var\n                    + self.var / other.var - 1.0 - self.logvar + other.logvar,\n                    dim=[1, 2, 3])\n    def nll(self, sample, dims=[1,2,3]):\n        if self.deterministic:\n            return torch.Tensor([0.])\n        logtwopi = np.log(2.0 * np.pi)\n        return 0.5 * torch.sum(\n            logtwopi + self.logvar + torch.pow(sample - self.mean, 2) / self.var,\n            dim=dims)\n    def mode(self):\n        return self.mean\ndef normal_kl(mean1, logvar1, mean2, logvar2):\n    \"\"\"\n    source: https://github.com/openai/guided-diffusion/blob/27c20a8fab9cb472df5d6bdd6c8d11c8f430b924/guided_diffusion/losses.py#L12"
        },
        {
            "comment": "This function calculates the KL divergence between two Gaussian distributions. It takes four arguments: mean1, logvar1, mean2, and logvar2. The code ensures that at least one argument is a tensor and forces variances to be tensors for efficient computation. The final result is returned after applying various mathematical operations on the inputs.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/distributions.py\":70-94",
            "content": "    Compute the KL divergence between two gaussians.\n    Shapes are automatically broadcasted, so batches can be compared to\n    scalars, among other use cases.\n    \"\"\"\n    tensor = None\n    for obj in (mean1, logvar1, mean2, logvar2):\n        if isinstance(obj, torch.Tensor):\n            tensor = obj\n            break\n    assert tensor is not None, \"at least one argument must be a Tensor\"\n    # Force variances to be Tensors. Broadcasting helps convert scalars to\n    # Tensors, but it does not work for torch.exp().\n    logvar1, logvar2 = [\n        x if isinstance(x, torch.Tensor) else torch.tensor(x).to(tensor)\n        for x in (logvar1, logvar2)\n    ]\n    return 0.5 * (\n        -1.0\n        + logvar2\n        - logvar1\n        + torch.exp(logvar1 - logvar2)\n        + ((mean1 - mean2) ** 2) * torch.exp(-logvar2)\n    )"
        }
    ]
}