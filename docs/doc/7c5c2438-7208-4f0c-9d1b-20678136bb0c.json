{
    "summary": "This code snippet handles convolutional layers in deep learning models, supports various modules including 1D-3D convolutions, linear layers, pooling, and nonlinearities. It includes a normalization layer and an instance of a hybrid conditioner that concatenates two configuration-based conditioners.",
    "details": [
        {
            "comment": "This code snippet appears to be a module for handling convolutional layers in a deep learning model. It includes functions for changing the training mode, zeroing out and scaling parameters of modules, and creating convolutional layers with user-specified dimensions.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/basics.py\":0-35",
            "content": "# adopted from\n# https://github.com/openai/improved-diffusion/blob/main/improved_diffusion/gaussian_diffusion.py\n# and\n# https://github.com/lucidrains/denoising-diffusion-pytorch/blob/7706bdfc6f527f58d33f84b7b522e61e6e3164b3/denoising_diffusion_pytorch/denoising_diffusion_pytorch.py\n# and\n# https://github.com/openai/guided-diffusion/blob/0ba878e517b276c45d1195eb29f6f5f72659a05b/guided_diffusion/nn.py\n#\n# thanks!\nimport torch.nn as nn\nfrom utils.utils import instantiate_from_config\ndef disabled_train(self, mode=True):\n    \"\"\"Overwrite model.train with this function to make sure train/eval mode\n    does not change anymore.\"\"\"\n    return self\ndef zero_module(module):\n    \"\"\"\n    Zero out the parameters of a module and return it.\n    \"\"\"\n    for p in module.parameters():\n        p.detach().zero_()\n    return module\ndef scale_module(module, scale):\n    \"\"\"\n    Scale the parameters of a module and return it.\n    \"\"\"\n    for p in module.parameters():\n        p.detach().mul_(scale)\n    return module\ndef conv_nd(dims, *args, **kwargs):"
        },
        {
            "comment": "This code provides utility functions for creating various deep learning modules, including convolutions (1D, 2D, 3D), linear layers, average pooling (1D, 2D, 3D), and nonlinearities (SiLU or Leaky ReLU). The `GroupNormSpecific` class extends PyTorch's GroupNorm with a specific forward function to ensure correct output dtype.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/basics.py\":36-80",
            "content": "    \"\"\"\n    Create a 1D, 2D, or 3D convolution module.\n    \"\"\"\n    if dims == 1:\n        return nn.Conv1d(*args, **kwargs)\n    elif dims == 2:\n        return nn.Conv2d(*args, **kwargs)\n    elif dims == 3:\n        return nn.Conv3d(*args, **kwargs)\n    raise ValueError(f\"unsupported dimensions: {dims}\")\ndef linear(*args, **kwargs):\n    \"\"\"\n    Create a linear module.\n    \"\"\"\n    return nn.Linear(*args, **kwargs)\ndef avg_pool_nd(dims, *args, **kwargs):\n    \"\"\"\n    Create a 1D, 2D, or 3D average pooling module.\n    \"\"\"\n    if dims == 1:\n        return nn.AvgPool1d(*args, **kwargs)\n    elif dims == 2:\n        return nn.AvgPool2d(*args, **kwargs)\n    elif dims == 3:\n        return nn.AvgPool3d(*args, **kwargs)\n    raise ValueError(f\"unsupported dimensions: {dims}\")\ndef nonlinearity(type='silu'):\n    if type == 'silu':\n        return nn.SiLU()\n    elif type == 'leaky_relu':\n        return nn.LeakyReLU()\nclass GroupNormSpecific(nn.GroupNorm):\n    def forward(self, x):\n        return super().forward(x.float()).type(x.dtype)\ndef normalization(channels, num_groups=32):"
        },
        {
            "comment": "The code defines a normalization layer and creates an instance of a hybrid conditioner that concatenates two configuration-based conditioners.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/basics.py\":81-99",
            "content": "    \"\"\"\n    Make a standard normalization layer.\n    :param channels: number of input channels.\n    :return: an nn.Module for normalization.\n    \"\"\"\n    return GroupNormSpecific(num_groups, channels)\nclass HybridConditioner(nn.Module):\n    def __init__(self, c_concat_config, c_crossattn_config):\n        super().__init__()\n        self.concat_conditioner = instantiate_from_config(c_concat_config)\n        self.crossattn_conditioner = instantiate_from_config(c_crossattn_config)\n    def forward(self, c_concat, c_crossattn):\n        c_concat = self.concat_conditioner(c_concat)\n        c_crossattn = self.crossattn_conditioner(c_crossattn)\n        return {'c_concat': [c_concat], 'c_crossattn': [c_crossattn]}"
        }
    ]
}