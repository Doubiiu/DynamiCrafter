{
    "summary": "The code defines a PyTorch class \"LitEma\" for Exponential Moving Average, initializes EMA parameters and registers buffers, applies EMA calculation during training and manages shadow parameters. It also provides temporary storage and restoration for torch.nn.Parameter parameters useful for model validation without affecting optimization process.",
    "details": [
        {
            "comment": "The code defines a PyTorch class \"LitEma\" that implements Exponential Moving Average (EMA) for a given model. It initializes the EMA parameters and registers buffers to store the moving averages of the model's parameters. The forward method updates the decay rate based on the number of parameter updates and applies the EMA calculation during model training.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/ema.py\":0-29",
            "content": "import torch\nfrom torch import nn\nclass LitEma(nn.Module):\n    def __init__(self, model, decay=0.9999, use_num_upates=True):\n        super().__init__()\n        if decay < 0.0 or decay > 1.0:\n            raise ValueError('Decay must be between 0 and 1')\n        self.m_name2s_name = {}\n        self.register_buffer('decay', torch.tensor(decay, dtype=torch.float32))\n        self.register_buffer('num_updates', torch.tensor(0,dtype=torch.int) if use_num_upates\n                             else torch.tensor(-1,dtype=torch.int))\n        for name, p in model.named_parameters():\n            if p.requires_grad:\n                #remove as '.'-character is not allowed in buffers\n                s_name = name.replace('.','')\n                self.m_name2s_name.update({name:s_name})\n                self.register_buffer(s_name,p.clone().detach().data)\n        self.collected_params = []\n    def forward(self,model):\n        decay = self.decay\n        if self.num_updates >= 0:\n            self.num_updates += 1\n            decay = min(self.decay,(1 + self.num_updates) / (10 + self.num_updates))"
        },
        {
            "comment": "This code defines a class that appears to manage shadow parameters (backup for original model parameters). It compares the original and backup parameters, updating the backup based on a decay value. The `copy_to` function transfers the saved parameters back into the model, while `store` saves the current parameters to be used later.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/ema.py\":31-56",
            "content": "        one_minus_decay = 1.0 - decay\n        with torch.no_grad():\n            m_param = dict(model.named_parameters())\n            shadow_params = dict(self.named_buffers())\n            for key in m_param:\n                if m_param[key].requires_grad:\n                    sname = self.m_name2s_name[key]\n                    shadow_params[sname] = shadow_params[sname].type_as(m_param[key])\n                    shadow_params[sname].sub_(one_minus_decay * (shadow_params[sname] - m_param[key]))\n                else:\n                    assert not key in self.m_name2s_name\n    def copy_to(self, model):\n        m_param = dict(model.named_parameters())\n        shadow_params = dict(self.named_buffers())\n        for key in m_param:\n            if m_param[key].requires_grad:\n                m_param[key].data.copy_(shadow_params[self.m_name2s_name[key]].data)\n            else:\n                assert not key in self.m_name2s_name\n    def store(self, parameters):\n        \"\"\"\n        Save the current parameters for restoring later."
        },
        {
            "comment": "The code defines a class with methods to store and restore torch.nn.Parameter parameters temporarily, useful for validating the model without affecting the original optimization process.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/ema.py\":57-75",
            "content": "        Args:\n          parameters: Iterable of `torch.nn.Parameter`; the parameters to be\n            temporarily stored.\n        \"\"\"\n        self.collected_params = [param.clone() for param in parameters]\n    def restore(self, parameters):\n        \"\"\"\n        Restore the parameters stored with the `store` method.\n        Useful to validate the model with EMA parameters without affecting the\n        original optimization process. Store the parameters before the\n        `copy_to` method. After validation (or model saving), use this to\n        restore the former parameters.\n        Args:\n          parameters: Iterable of `torch.nn.Parameter`; the parameters to be\n            updated with the stored parameters.\n        \"\"\"\n        for c_param, param in zip(self.collected_params, parameters):\n            param.data.copy_(c_param.data)"
        }
    ]
}