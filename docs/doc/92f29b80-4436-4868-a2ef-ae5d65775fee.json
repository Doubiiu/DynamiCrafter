{
    "summary": "The code creates a PyTorch Autoencoder module with attention blocks, normalization layers, convolutional layers, Swish nonlinearity, and timestep embedding for image processing. It also disables gradient computation for pretrained model parameters and includes 'encode_with_pretrained' and 'forward' functions.",
    "details": [
        {
            "comment": "This code defines a module for attention blocks, which includes normalization layers, convolutional layers, and Swish nonlinearity. It also includes functions to define the number of groups for group normalization and create instances from configuration files. The modules aim to process input with various channels and heads in attention mechanisms.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/networks/ae_modules.py\":0-35",
            "content": "# pytorch_diffusion + derived encoder decoder\nimport math\nimport torch\nimport numpy as np\nimport torch.nn as nn\nfrom einops import rearrange\nfrom utils.utils import instantiate_from_config\nfrom lvdm.modules.attention import LinearAttention\ndef nonlinearity(x):\n    # swish\n    return x*torch.sigmoid(x)\ndef Normalize(in_channels, num_groups=32):\n    return torch.nn.GroupNorm(num_groups=num_groups, num_channels=in_channels, eps=1e-6, affine=True)\nclass LinAttnBlock(LinearAttention):\n    \"\"\"to match AttnBlock usage\"\"\"\n    def __init__(self, in_channels):\n        super().__init__(dim=in_channels, heads=1, dim_head=in_channels)\nclass AttnBlock(nn.Module):\n    def __init__(self, in_channels):\n        super().__init__()\n        self.in_channels = in_channels\n        self.norm = Normalize(in_channels)\n        self.q = torch.nn.Conv2d(in_channels,\n                                 in_channels,\n                                 kernel_size=1,\n                                 stride=1,\n                                 padding=0)"
        },
        {
            "comment": "This code defines a Conv2d layer for key, value and output projection in an attention mechanism. The forward function computes the attention by reshaping and permuting the input tensors, then performs element-wise multiplication and softmax normalization to get the attention weights.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/networks/ae_modules.py\":36-63",
            "content": "        self.k = torch.nn.Conv2d(in_channels,\n                                 in_channels,\n                                 kernel_size=1,\n                                 stride=1,\n                                 padding=0)\n        self.v = torch.nn.Conv2d(in_channels,\n                                 in_channels,\n                                 kernel_size=1,\n                                 stride=1,\n                                 padding=0)\n        self.proj_out = torch.nn.Conv2d(in_channels,\n                                        in_channels,\n                                        kernel_size=1,\n                                        stride=1,\n                                        padding=0)\n    def forward(self, x):\n        h_ = x\n        h_ = self.norm(h_)\n        q = self.q(h_)\n        k = self.k(h_)\n        v = self.v(h_)\n        # compute attention\n        b,c,h,w = q.shape\n        q = q.reshape(b,c,h*w) # bcl\n        q = q.permute(0,2,1)   # bcl -> blc l=hw\n        k = k.reshape(b,c,h*w) # bcl"
        },
        {
            "comment": "The code defines a function for making attention blocks with different types (vanilla, linear, or none) and a downsampling module. It also includes functions to calculate attention scores and apply them to values, resulting in an output.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/networks/ae_modules.py\":65-93",
            "content": "        w_ = torch.bmm(q,k)    # b,hw,hw    w[b,i,j]=sum_c q[b,i,c]k[b,c,j]\n        w_ = w_ * (int(c)**(-0.5))\n        w_ = torch.nn.functional.softmax(w_, dim=2)\n        # attend to values\n        v = v.reshape(b,c,h*w)\n        w_ = w_.permute(0,2,1)   # b,hw,hw (first hw of k, second of q)\n        h_ = torch.bmm(v,w_)     # b, c,hw (hw of q) h_[b,c,j] = sum_i v[b,c,i] w_[b,i,j]\n        h_ = h_.reshape(b,c,h,w)\n        h_ = self.proj_out(h_)\n        return x+h_\ndef make_attn(in_channels, attn_type=\"vanilla\"):\n    assert attn_type in [\"vanilla\", \"linear\", \"none\"], f'attn_type {attn_type} unknown'\n    #print(f\"making attention of type '{attn_type}' with {in_channels} in_channels\")\n    if attn_type == \"vanilla\":\n        return AttnBlock(in_channels)\n    elif attn_type == \"none\":\n        return nn.Identity(in_channels)\n    else:\n        return LinAttnBlock(in_channels)\nclass Downsample(nn.Module):\n    def __init__(self, in_channels, with_conv):\n        super().__init__()\n        self.with_conv = with_conv\n        self.in_channels = in_channels"
        },
        {
            "comment": "The code defines a class \"AE_Module\" with a Conv2d layer and an optional upsampling operation based on the \"with_conv\" flag. If \"with_conv\" is True, it applies padding to the input, performs convolution, and returns the result. Otherwise, it uses average pooling for downsampling. The code also defines a separate class \"Upsample\" that inherits from nn.Module and takes in_channels and with_conv as parameters.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/networks/ae_modules.py\":94-119",
            "content": "        if self.with_conv:\n            # no asymmetric padding in torch conv, must do it ourselves\n            self.conv = torch.nn.Conv2d(in_channels,\n                                        in_channels,\n                                        kernel_size=3,\n                                        stride=2,\n                                        padding=0)\n    def forward(self, x):\n        if self.with_conv:\n            pad = (0,1,0,1)\n            x = torch.nn.functional.pad(x, pad, mode=\"constant\", value=0)\n            x = self.conv(x)\n        else:\n            x = torch.nn.functional.avg_pool2d(x, kernel_size=2, stride=2)\n        return x\nclass Upsample(nn.Module):\n    def __init__(self, in_channels, with_conv):\n        super().__init__()\n        self.with_conv = with_conv\n        self.in_channels = in_channels\n        if self.with_conv:\n            self.conv = torch.nn.Conv2d(in_channels,\n                                        in_channels,\n                                        kernel_size=3,\n                                        stride=1,"
        },
        {
            "comment": "The function defines a convolutional neural network module that performs interpolation and optional convolution on its input. It also includes a separate function that generates timestep embeddings based on the sinusoidal function, which matches implementations in Denoising Diffusion Probabilistic Models from Fairseq and slightly differs from the description in \"Attention Is All You Need\". The generated embeddings are used for some downstream task.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/networks/ae_modules.py\":120-146",
            "content": "                                        padding=1)\n    def forward(self, x):\n        x = torch.nn.functional.interpolate(x, scale_factor=2.0, mode=\"nearest\")\n        if self.with_conv:\n            x = self.conv(x)\n        return x\ndef get_timestep_embedding(timesteps, embedding_dim):\n    \"\"\"\n    This matches the implementation in Denoising Diffusion Probabilistic Models:\n    From Fairseq.\n    Build sinusoidal embeddings.\n    This matches the implementation in tensor2tensor, but differs slightly\n    from the description in Section 3.5 of \"Attention Is All You Need\".\n    \"\"\"\n    assert len(timesteps.shape) == 1\n    half_dim = embedding_dim // 2\n    emb = math.log(10000) / (half_dim - 1)\n    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n    emb = emb.to(device=timesteps.device)\n    emb = timesteps.float()[:, None] * emb[None, :]\n    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n    if embedding_dim % 2 == 1:  # zero pad\n        emb = torch.nn.functional.pad(emb, (0,1,0,0))\n    return emb"
        },
        {
            "comment": "This code defines a ResnetBlock class that inherits from nn.Module in PyTorch. It takes input channels, optional output channels, convolutional shortcut flag, dropout rate and temb_channels as parameters. The block consists of normalization layers, convolution layers and a dropout layer to process the input data.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/networks/ae_modules.py\":150-171",
            "content": "class ResnetBlock(nn.Module):\n    def __init__(self, *, in_channels, out_channels=None, conv_shortcut=False,\n                 dropout, temb_channels=512):\n        super().__init__()\n        self.in_channels = in_channels\n        out_channels = in_channels if out_channels is None else out_channels\n        self.out_channels = out_channels\n        self.use_conv_shortcut = conv_shortcut\n        self.norm1 = Normalize(in_channels)\n        self.conv1 = torch.nn.Conv2d(in_channels,\n                                     out_channels,\n                                     kernel_size=3,\n                                     stride=1,\n                                     padding=1)\n        if temb_channels > 0:\n            self.temb_proj = torch.nn.Linear(temb_channels,\n                                             out_channels)\n        self.norm2 = Normalize(out_channels)\n        self.dropout = torch.nn.Dropout(dropout)\n        self.conv2 = torch.nn.Conv2d(out_channels,\n                                     out_channels,"
        },
        {
            "comment": "The code initializes a Conv2d layer with specific parameters and applies normalization, nonlinearity, and convolution operations on the input 'x'. If in_channels and out_channels are different, it also adds a shortcut connection using either Conv2d or another Conv2d depending on the use_conv_shortcut flag.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/networks/ae_modules.py\":172-193",
            "content": "                                     kernel_size=3,\n                                     stride=1,\n                                     padding=1)\n        if self.in_channels != self.out_channels:\n            if self.use_conv_shortcut:\n                self.conv_shortcut = torch.nn.Conv2d(in_channels,\n                                                     out_channels,\n                                                     kernel_size=3,\n                                                     stride=1,\n                                                     padding=1)\n            else:\n                self.nin_shortcut = torch.nn.Conv2d(in_channels,\n                                                    out_channels,\n                                                    kernel_size=1,\n                                                    stride=1,\n                                                    padding=0)\n    def forward(self, x, temb):\n        h = x\n        h = self.norm1(h)\n        h = nonlinearity(h)\n        h = self.conv1(h)"
        },
        {
            "comment": "The code defines a class for an attention-based architecture module. It includes operations like convolution, normalization, nonlinearity activation, and dropout. The module can have configurable options such as in_channels, out_channels, number of resolution levels, and whether to use timestamps or linear attention. The code also initializes member variables for the number of resolutions, in/out channels, and the type of attention mechanism being used.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/networks/ae_modules.py\":195-224",
            "content": "        if temb is not None:\n            h = h + self.temb_proj(nonlinearity(temb))[:,:,None,None]\n        h = self.norm2(h)\n        h = nonlinearity(h)\n        h = self.dropout(h)\n        h = self.conv2(h)\n        if self.in_channels != self.out_channels:\n            if self.use_conv_shortcut:\n                x = self.conv_shortcut(x)\n            else:\n                x = self.nin_shortcut(x)\n        return x+h\nclass Model(nn.Module):\n    def __init__(self, *, ch, out_ch, ch_mult=(1,2,4,8), num_res_blocks,\n                 attn_resolutions, dropout=0.0, resamp_with_conv=True, in_channels,\n                 resolution, use_timestep=True, use_linear_attn=False, attn_type=\"vanilla\"):\n        super().__init__()\n        if use_linear_attn: attn_type = \"linear\"\n        self.ch = ch\n        self.temb_ch = self.ch*4\n        self.num_resolutions = len(ch_mult)\n        self.num_res_blocks = num_res_blocks\n        self.resolution = resolution\n        self.in_channels = in_channels\n        self.use_timestep = use_timestep"
        },
        {
            "comment": "This code defines a module for an Autoencoder, consisting of timestep embedding and downsampling layers. The timestep embedding uses two dense layers for transformation. Downsampling is performed using conv2d with specified kernel size, stride, and padding. It has multiple resolutions with different channel multipliers and num_res_blocks. ResnetBlock is also defined as a separate module.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/networks/ae_modules.py\":225-251",
            "content": "        if self.use_timestep:\n            # timestep embedding\n            self.temb = nn.Module()\n            self.temb.dense = nn.ModuleList([\n                torch.nn.Linear(self.ch,\n                                self.temb_ch),\n                torch.nn.Linear(self.temb_ch,\n                                self.temb_ch),\n            ])\n        # downsampling\n        self.conv_in = torch.nn.Conv2d(in_channels,\n                                       self.ch,\n                                       kernel_size=3,\n                                       stride=1,\n                                       padding=1)\n        curr_res = resolution\n        in_ch_mult = (1,)+tuple(ch_mult)\n        self.down = nn.ModuleList()\n        for i_level in range(self.num_resolutions):\n            block = nn.ModuleList()\n            attn = nn.ModuleList()\n            block_in = ch*in_ch_mult[i_level]\n            block_out = ch*ch_mult[i_level]\n            for i_block in range(self.num_res_blocks):\n                block.append(ResnetBlock(in_channels=block_in,"
        },
        {
            "comment": "This code initializes a downsampling network with multiple blocks, each containing convolutional layers and attention modules. It then creates a middle block with ResnetBlocks and attention modules. The code also handles downsampling for non-final resolutions by appending downsample modules to the network.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/networks/ae_modules.py\":252-273",
            "content": "                                         out_channels=block_out,\n                                         temb_channels=self.temb_ch,\n                                         dropout=dropout))\n                block_in = block_out\n                if curr_res in attn_resolutions:\n                    attn.append(make_attn(block_in, attn_type=attn_type))\n            down = nn.Module()\n            down.block = block\n            down.attn = attn\n            if i_level != self.num_resolutions-1:\n                down.downsample = Downsample(block_in, resamp_with_conv)\n                curr_res = curr_res // 2\n            self.down.append(down)\n        # middle\n        self.mid = nn.Module()\n        self.mid.block_1 = ResnetBlock(in_channels=block_in,\n                                       out_channels=block_in,\n                                       temb_channels=self.temb_ch,\n                                       dropout=dropout)\n        self.mid.attn_1 = make_attn(block_in, attn_type=attn_type)\n        self.mid.block_2 = ResnetBlock(in_channels=block_in,"
        },
        {
            "comment": "This code defines a network module with residual blocks and attention mechanisms. It has configurable parameters such as block input/output channels, dropout rate, number of resolutions, and types of attention used. The upsampling part creates a list of residual blocks to increase the resolution level for each stage.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/networks/ae_modules.py\":274-294",
            "content": "                                       out_channels=block_in,\n                                       temb_channels=self.temb_ch,\n                                       dropout=dropout)\n        # upsampling\n        self.up = nn.ModuleList()\n        for i_level in reversed(range(self.num_resolutions)):\n            block = nn.ModuleList()\n            attn = nn.ModuleList()\n            block_out = ch*ch_mult[i_level]\n            skip_in = ch*ch_mult[i_level]\n            for i_block in range(self.num_res_blocks+1):\n                if i_block == self.num_res_blocks:\n                    skip_in = ch*in_ch_mult[i_level]\n                block.append(ResnetBlock(in_channels=block_in+skip_in,\n                                         out_channels=block_out,\n                                         temb_channels=self.temb_ch,\n                                         dropout=dropout))\n                block_in = block_out\n                if curr_res in attn_resolutions:\n                    attn.append(make_attn(block_in, attn_type=attn_type))"
        },
        {
            "comment": "The code defines a neural network module for upsampling input with skip connections, normalization and convolution. The module takes input of shape (batch_size, block_in, height, width) and returns output of shape (batch_size, out_ch, height, width). It also includes an optional context input and a timestep embedding if specified.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/networks/ae_modules.py\":295-320",
            "content": "            up = nn.Module()\n            up.block = block\n            up.attn = attn\n            if i_level != 0:\n                up.upsample = Upsample(block_in, resamp_with_conv)\n                curr_res = curr_res * 2\n            self.up.insert(0, up) # prepend to get consistent order\n        # end\n        self.norm_out = Normalize(block_in)\n        self.conv_out = torch.nn.Conv2d(block_in,\n                                        out_ch,\n                                        kernel_size=3,\n                                        stride=1,\n                                        padding=1)\n    def forward(self, x, t=None, context=None):\n        #assert x.shape[2] == x.shape[3] == self.resolution\n        if context is not None:\n            # assume aligned context, cat along channel axis\n            x = torch.cat((x, context), dim=1)\n        if self.use_timestep:\n            # timestep embedding\n            assert t is not None\n            temb = get_timestep_embedding(t, self.ch)\n            temb = self.temb.dense[0](temb)"
        },
        {
            "comment": "This code is implementing a neural network module, performing downsampling, middle operations, and upsampling for each level of the network. It also includes non-linear transformations and attention mechanisms. The final output is obtained after processing through all levels and blocks.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/networks/ae_modules.py\":321-348",
            "content": "            temb = nonlinearity(temb)\n            temb = self.temb.dense[1](temb)\n        else:\n            temb = None\n        # downsampling\n        hs = [self.conv_in(x)]\n        for i_level in range(self.num_resolutions):\n            for i_block in range(self.num_res_blocks):\n                h = self.down[i_level].block[i_block](hs[-1], temb)\n                if len(self.down[i_level].attn) > 0:\n                    h = self.down[i_level].attn[i_block](h)\n                hs.append(h)\n            if i_level != self.num_resolutions-1:\n                hs.append(self.down[i_level].downsample(hs[-1]))\n        # middle\n        h = hs[-1]\n        h = self.mid.block_1(h, temb)\n        h = self.mid.attn_1(h)\n        h = self.mid.block_2(h, temb)\n        # upsampling\n        for i_level in reversed(range(self.num_resolutions)):\n            for i_block in range(self.num_res_blocks+1):\n                h = self.up[i_level].block[i_block](\n                    torch.cat([h, hs.pop()], dim=1), temb)\n                if len(self.up[i_level].attn) > 0:"
        },
        {
            "comment": "This code defines a class for an Encoder module in a neural network. It initializes the attributes and has methods for processing data through downsampling, residual blocks, and applying nonlinearities. The get_last_layer method returns the weight of the last convolutional layer. The Encoder class accepts parameters for channel size, output channel size, channel multipliers, number of residual blocks, attention resolutions, dropout rate, etc. It also includes options for different attention types and linear attentions.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/networks/ae_modules.py\":349-379",
            "content": "                    h = self.up[i_level].attn[i_block](h)\n            if i_level != 0:\n                h = self.up[i_level].upsample(h)\n        # end\n        h = self.norm_out(h)\n        h = nonlinearity(h)\n        h = self.conv_out(h)\n        return h\n    def get_last_layer(self):\n        return self.conv_out.weight\nclass Encoder(nn.Module):\n    def __init__(self, *, ch, out_ch, ch_mult=(1,2,4,8), num_res_blocks,\n                 attn_resolutions, dropout=0.0, resamp_with_conv=True, in_channels,\n                 resolution, z_channels, double_z=True, use_linear_attn=False, attn_type=\"vanilla\",\n                 **ignore_kwargs):\n        super().__init__()\n        if use_linear_attn: attn_type = \"linear\"\n        self.ch = ch\n        self.temb_ch = 0\n        self.num_resolutions = len(ch_mult)\n        self.num_res_blocks = num_res_blocks\n        self.resolution = resolution\n        self.in_channels = in_channels\n        # downsampling\n        self.conv_in = torch.nn.Conv2d(in_channels,\n                                       self.ch,"
        },
        {
            "comment": "This code initializes a downsampling module for an AE model. It creates a series of ResnetBlocks and optional attention modules, with varying input/output channels depending on the resolution level. The block_in and curr_res variables track the current input size and downsampled resolution respectively.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/networks/ae_modules.py\":380-402",
            "content": "                                       kernel_size=3,\n                                       stride=1,\n                                       padding=1)\n        curr_res = resolution\n        in_ch_mult = (1,)+tuple(ch_mult)\n        self.in_ch_mult = in_ch_mult\n        self.down = nn.ModuleList()\n        for i_level in range(self.num_resolutions):\n            block = nn.ModuleList()\n            attn = nn.ModuleList()\n            block_in = ch*in_ch_mult[i_level]\n            block_out = ch*ch_mult[i_level]\n            for i_block in range(self.num_res_blocks):\n                block.append(ResnetBlock(in_channels=block_in,\n                                         out_channels=block_out,\n                                         temb_channels=self.temb_ch,\n                                         dropout=dropout))\n                block_in = block_out\n                if curr_res in attn_resolutions:\n                    attn.append(make_attn(block_in, attn_type=attn_type))\n            down = nn.Module()\n            down.block = block"
        },
        {
            "comment": "The code defines a network module with downsampling, middle blocks (ResnetBlock and attention), and an output layer. It creates the downsampling layers based on the current resolution and appends them to self.down list. The middle part includes two ResnetBlocks and an attention layer. Finally, it creates a normalization layer and a convolution output layer for the final output.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/networks/ae_modules.py\":403-424",
            "content": "            down.attn = attn\n            if i_level != self.num_resolutions-1:\n                down.downsample = Downsample(block_in, resamp_with_conv)\n                curr_res = curr_res // 2\n            self.down.append(down)\n        # middle\n        self.mid = nn.Module()\n        self.mid.block_1 = ResnetBlock(in_channels=block_in,\n                                       out_channels=block_in,\n                                       temb_channels=self.temb_ch,\n                                       dropout=dropout)\n        self.mid.attn_1 = make_attn(block_in, attn_type=attn_type)\n        self.mid.block_2 = ResnetBlock(in_channels=block_in,\n                                       out_channels=block_in,\n                                       temb_channels=self.temb_ch,\n                                       dropout=dropout)\n        # end\n        self.norm_out = Normalize(block_in)\n        self.conv_out = torch.nn.Conv2d(block_in,\n                                        2*z_channels if double_z else z_channels,"
        },
        {
            "comment": "This code defines a network encoder, with convolutional layers and downsampling blocks. It takes input, performs timestep embedding, and applies successive downsampling and attention operations to produce features for further processing.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/networks/ae_modules.py\":425-449",
            "content": "                                        kernel_size=3,\n                                        stride=1,\n                                        padding=1)\n    def forward(self, x):\n        # timestep embedding\n        temb = None\n        # print(f'encoder-input={x.shape}')\n        # downsampling\n        hs = [self.conv_in(x)]\n        # print(f'encoder-conv in feat={hs[0].shape}')\n        for i_level in range(self.num_resolutions):\n            for i_block in range(self.num_res_blocks):\n                h = self.down[i_level].block[i_block](hs[-1], temb)\n                # print(f'encoder-down feat={h.shape}')\n                if len(self.down[i_level].attn) > 0:\n                    h = self.down[i_level].attn[i_block](h)\n                hs.append(h)\n            if i_level != self.num_resolutions-1:\n                # print(f'encoder-downsample (input)={hs[-1].shape}')\n                hs.append(self.down[i_level].downsample(hs[-1]))\n                # print(f'encoder-downsample (output)={hs[-1].shape}')\n        # middle"
        },
        {
            "comment": "Encoder module processes input, returns final feature map; Decoder module initializes with specified parameters, prepares for resnet-like blocks and attention mechanisms.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/networks/ae_modules.py\":450-478",
            "content": "        h = hs[-1]\n        h = self.mid.block_1(h, temb)\n        # print(f'encoder-mid1 feat={h.shape}')\n        h = self.mid.attn_1(h)\n        h = self.mid.block_2(h, temb)\n        # print(f'encoder-mid2 feat={h.shape}')\n        # end\n        h = self.norm_out(h)\n        h = nonlinearity(h)\n        h = self.conv_out(h)\n        # print(f'end feat={h.shape}')\n        return h\nclass Decoder(nn.Module):\n    def __init__(self, *, ch, out_ch, ch_mult=(1,2,4,8), num_res_blocks,\n                 attn_resolutions, dropout=0.0, resamp_with_conv=True, in_channels,\n                 resolution, z_channels, give_pre_end=False, tanh_out=False, use_linear_attn=False,\n                 attn_type=\"vanilla\", **ignorekwargs):\n        super().__init__()\n        if use_linear_attn: attn_type = \"linear\"\n        self.ch = ch\n        self.temb_ch = 0\n        self.num_resolutions = len(ch_mult)\n        self.num_res_blocks = num_res_blocks\n        self.resolution = resolution\n        self.in_channels = in_channels\n        self.give_pre_end = give_pre_end"
        },
        {
            "comment": "This code is initializing a model for Autoencoder with given parameters. It defines the Conv2d layer, ResnetBlock, and make_attn function to transform the input (z) into block_in shape and applies middle blocks. The code also prints the dimensions of the z tensor.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/networks/ae_modules.py\":479-502",
            "content": "        self.tanh_out = tanh_out\n        # compute in_ch_mult, block_in and curr_res at lowest res\n        in_ch_mult = (1,)+tuple(ch_mult)\n        block_in = ch*ch_mult[self.num_resolutions-1]\n        curr_res = resolution // 2**(self.num_resolutions-1)\n        self.z_shape = (1,z_channels,curr_res,curr_res)\n        print(\"AE working on z of shape {} = {} dimensions.\".format(\n            self.z_shape, np.prod(self.z_shape)))\n        # z to block_in\n        self.conv_in = torch.nn.Conv2d(z_channels,\n                                       block_in,\n                                       kernel_size=3,\n                                       stride=1,\n                                       padding=1)\n        # middle\n        self.mid = nn.Module()\n        self.mid.block_1 = ResnetBlock(in_channels=block_in,\n                                       out_channels=block_in,\n                                       temb_channels=self.temb_ch,\n                                       dropout=dropout)\n        self.mid.attn_1 = make_attn(block_in, attn_type=attn_type)"
        },
        {
            "comment": "This code creates a ResnetBlock and multiple ResnetBlocks for upsampling in a network, with attention mechanism optionally added. It uses specified input and output channels, embedding dimensions, dropout rate, and number of resolutions. The blocks are organized within ModuleLists for flexibility and efficient processing.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/networks/ae_modules.py\":503-524",
            "content": "        self.mid.block_2 = ResnetBlock(in_channels=block_in,\n                                       out_channels=block_in,\n                                       temb_channels=self.temb_ch,\n                                       dropout=dropout)\n        # upsampling\n        self.up = nn.ModuleList()\n        for i_level in reversed(range(self.num_resolutions)):\n            block = nn.ModuleList()\n            attn = nn.ModuleList()\n            block_out = ch*ch_mult[i_level]\n            for i_block in range(self.num_res_blocks+1):\n                block.append(ResnetBlock(in_channels=block_in,\n                                         out_channels=block_out,\n                                         temb_channels=self.temb_ch,\n                                         dropout=dropout))\n                block_in = block_out\n                if curr_res in attn_resolutions:\n                    attn.append(make_attn(block_in, attn_type=attn_type))\n            up = nn.Module()\n            up.block = block\n            up.attn = attn"
        },
        {
            "comment": "The code snippet initializes a network module with an upsampling block for each resolution level and a normalization layer. It also includes a convolutional layer for the output. The forward method performs convolution, processes middle layers using attention mechanism, and iterates through upsampling blocks in reverse order to produce the final output.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/networks/ae_modules.py\":525-557",
            "content": "            if i_level != 0:\n                up.upsample = Upsample(block_in, resamp_with_conv)\n                curr_res = curr_res * 2\n            self.up.insert(0, up) # prepend to get consistent order\n        # end\n        self.norm_out = Normalize(block_in)\n        self.conv_out = torch.nn.Conv2d(block_in,\n                                        out_ch,\n                                        kernel_size=3,\n                                        stride=1,\n                                        padding=1)\n    def forward(self, z):\n        #assert z.shape[1:] == self.z_shape[1:]\n        self.last_z_shape = z.shape\n        # print(f'decoder-input={z.shape}')\n        # timestep embedding\n        temb = None\n        # z to block_in\n        h = self.conv_in(z)\n        # print(f'decoder-conv in feat={h.shape}')\n        # middle\n        h = self.mid.block_1(h, temb)\n        h = self.mid.attn_1(h)\n        h = self.mid.block_2(h, temb)\n        # print(f'decoder-mid feat={h.shape}')\n        # upsampling\n        for i_level in reversed(range(self.num_resolutions)):"
        },
        {
            "comment": "This code defines a Decoder class that processes features extracted from an encoder. It includes convolutional layers, residual blocks, and optional upsampling. The resulting features are passed through normalization, non-linear activation, and potentially a tanh operation before being returned as output.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/networks/ae_modules.py\":558-585",
            "content": "            for i_block in range(self.num_res_blocks+1):\n                h = self.up[i_level].block[i_block](h, temb)\n                if len(self.up[i_level].attn) > 0:\n                    h = self.up[i_level].attn[i_block](h)\n                # print(f'decoder-up feat={h.shape}')\n            if i_level != 0:\n                h = self.up[i_level].upsample(h)\n                # print(f'decoder-upsample feat={h.shape}')\n        # end\n        if self.give_pre_end:\n            return h\n        h = self.norm_out(h)\n        h = nonlinearity(h)\n        h = self.conv_out(h)\n        # print(f'decoder-conv_out feat={h.shape}')\n        if self.tanh_out:\n            h = torch.tanh(h)\n        return h\nclass SimpleDecoder(nn.Module):\n    def __init__(self, in_channels, out_channels, *args, **kwargs):\n        super().__init__()\n        self.model = nn.ModuleList([nn.Conv2d(in_channels, in_channels, 1),\n                                     ResnetBlock(in_channels=in_channels,\n                                                 out_channels=2 * in_channels,"
        },
        {
            "comment": "This code defines a ResnetBlock based model for image upsampling and normalization. It uses convolutional layers, residual blocks, normalization, and upsampling operations. The model takes input channels (in_channels) and outputs channels (out_channels). It consists of several Conv2d and ResnetBlock layers followed by a Normalize layer and a final Conv2d layer for the output.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/networks/ae_modules.py\":586-601",
            "content": "                                                 temb_channels=0, dropout=0.0),\n                                     ResnetBlock(in_channels=2 * in_channels,\n                                                out_channels=4 * in_channels,\n                                                temb_channels=0, dropout=0.0),\n                                     ResnetBlock(in_channels=4 * in_channels,\n                                                out_channels=2 * in_channels,\n                                                temb_channels=0, dropout=0.0),\n                                     nn.Conv2d(2*in_channels, in_channels, 1),\n                                     Upsample(in_channels, with_conv=True)])\n        # end\n        self.norm_out = Normalize(in_channels)\n        self.conv_out = torch.nn.Conv2d(in_channels,\n                                        out_channels,\n                                        kernel_size=3,\n                                        stride=1,\n                                        padding=1)"
        },
        {
            "comment": "This code defines a class `UpsampleDecoder` that initializes an upsampling decoder module for a neural network. The module consists of multiple resnet blocks and upsample blocks, which are initialized based on the input and output channels, channel multipliers, number of resolution levels, and the desired resolution. These blocks perform various operations such as convolution, normalization, and nonlinearity.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/networks/ae_modules.py\":603-632",
            "content": "    def forward(self, x):\n        for i, layer in enumerate(self.model):\n            if i in [1,2,3]:\n                x = layer(x, None)\n            else:\n                x = layer(x)\n        h = self.norm_out(x)\n        h = nonlinearity(h)\n        x = self.conv_out(h)\n        return x\nclass UpsampleDecoder(nn.Module):\n    def __init__(self, in_channels, out_channels, ch, num_res_blocks, resolution,\n                 ch_mult=(2,2), dropout=0.0):\n        super().__init__()\n        # upsampling\n        self.temb_ch = 0\n        self.num_resolutions = len(ch_mult)\n        self.num_res_blocks = num_res_blocks\n        block_in = in_channels\n        curr_res = resolution // 2 ** (self.num_resolutions - 1)\n        self.res_blocks = nn.ModuleList()\n        self.upsample_blocks = nn.ModuleList()\n        for i_level in range(self.num_resolutions):\n            res_block = []\n            block_out = ch * ch_mult[i_level]\n            for i_block in range(self.num_res_blocks + 1):\n                res_block.append(ResnetBlock(in_channels=block_in,"
        },
        {
            "comment": "This code initializes a residual network for image processing. It iterates through multiple levels and blocks, appending res_blocks and upsample_blocks as necessary. The final layers are a normalization layer and a convolutional layer for the output channel. The forward function performs upsampling followed by applying the initialized blocks in the network sequentially.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/networks/ae_modules.py\":633-656",
            "content": "                                         out_channels=block_out,\n                                         temb_channels=self.temb_ch,\n                                         dropout=dropout))\n                block_in = block_out\n            self.res_blocks.append(nn.ModuleList(res_block))\n            if i_level != self.num_resolutions - 1:\n                self.upsample_blocks.append(Upsample(block_in, True))\n                curr_res = curr_res * 2\n        # end\n        self.norm_out = Normalize(block_in)\n        self.conv_out = torch.nn.Conv2d(block_in,\n                                        out_channels,\n                                        kernel_size=3,\n                                        stride=1,\n                                        padding=1)\n    def forward(self, x):\n        # upsampling\n        h = x\n        for k, i_level in enumerate(range(self.num_resolutions)):\n            for i_block in range(self.num_res_blocks + 1):\n                h = self.res_blocks[i_level][i_block](h, None)\n            if i_level != self.num_resolutions - 1:"
        },
        {
            "comment": "The code defines a LatentRescaler class with residual blocks and an attention block, which takes input channels, mid-channels, output channels, and depth as parameters. It performs upsampling, normalization, nonlinearity, convolution, and returns the output. The AttnBlock is used to apply attention on the mid-channel features.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/networks/ae_modules.py\":657-679",
            "content": "                h = self.upsample_blocks[k](h)\n        h = self.norm_out(h)\n        h = nonlinearity(h)\n        h = self.conv_out(h)\n        return h\nclass LatentRescaler(nn.Module):\n    def __init__(self, factor, in_channels, mid_channels, out_channels, depth=2):\n        super().__init__()\n        # residual block, interpolate, residual block\n        self.factor = factor\n        self.conv_in = nn.Conv2d(in_channels,\n                                 mid_channels,\n                                 kernel_size=3,\n                                 stride=1,\n                                 padding=1)\n        self.res_block1 = nn.ModuleList([ResnetBlock(in_channels=mid_channels,\n                                                     out_channels=mid_channels,\n                                                     temb_channels=0,\n                                                     dropout=0.0) for _ in range(depth)])\n        self.attn = AttnBlock(mid_channels)\n        self.res_block2 = nn.ModuleList([ResnetBlock(in_channels=mid_channels,"
        },
        {
            "comment": "This code defines a neural network module for a MergedRescaleEncoder. It has two resblocks, convolutions, and an attention mechanism. The resblocks are defined within the class and applied to the input tensor 'x'. The output is then passed through a convolution layer before being returned. Resamp_with_conv parameter determines whether to use a convolutional layer for resampling or not. This module can be used for image processing tasks such as image classification, segmentation, etc.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/networks/ae_modules.py\":680-704",
            "content": "                                                     out_channels=mid_channels,\n                                                     temb_channels=0,\n                                                     dropout=0.0) for _ in range(depth)])\n        self.conv_out = nn.Conv2d(mid_channels,\n                                  out_channels,\n                                  kernel_size=1,\n                                  )\n    def forward(self, x):\n        x = self.conv_in(x)\n        for block in self.res_block1:\n            x = block(x, None)\n        x = torch.nn.functional.interpolate(x, size=(int(round(x.shape[2]*self.factor)), int(round(x.shape[3]*self.factor))))\n        x = self.attn(x)\n        for block in self.res_block2:\n            x = block(x, None)\n        x = self.conv_out(x)\n        return x\nclass MergedRescaleEncoder(nn.Module):\n    def __init__(self, in_channels, ch, resolution, out_ch, num_res_blocks,\n                 attn_resolutions, dropout=0.0, resamp_with_conv=True,\n                 ch_mult=(1,2,4,8), rescale_factor=1.0, rescale_module_depth=1):"
        },
        {
            "comment": "This code defines two classes: \"AERescalerDecoder\" and \"MergedRescaleDecoder\". The AERescalerDecoder class initializes an encoder and a rescaler module. The encoder takes input, performs some transformations, and outputs intermediate features. The rescaler then scales these features based on certain factors before returning the result. The MergedRescaleDecoder class is similar to AERescalerDecoder but with additional arguments for resolution, num_res_blocks, attn_resolutions, and ch_mult. Both classes have a forward function that performs the encoding and rescaling operations.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/networks/ae_modules.py\":705-723",
            "content": "        super().__init__()\n        intermediate_chn = ch * ch_mult[-1]\n        self.encoder = Encoder(in_channels=in_channels, num_res_blocks=num_res_blocks, ch=ch, ch_mult=ch_mult,\n                               z_channels=intermediate_chn, double_z=False, resolution=resolution,\n                               attn_resolutions=attn_resolutions, dropout=dropout, resamp_with_conv=resamp_with_conv,\n                               out_ch=None)\n        self.rescaler = LatentRescaler(factor=rescale_factor, in_channels=intermediate_chn,\n                                       mid_channels=intermediate_chn, out_channels=out_ch, depth=rescale_module_depth)\n    def forward(self, x):\n        x = self.encoder(x)\n        x = self.rescaler(x)\n        return x\nclass MergedRescaleDecoder(nn.Module):\n    def __init__(self, z_channels, out_ch, resolution, num_res_blocks, attn_resolutions, ch, ch_mult=(1,2,4,8),\n                 dropout=0.0, resamp_with_conv=True, rescale_factor=1.0, rescale_module_depth=1):\n        super().__init__()"
        },
        {
            "comment": "The code defines a class called `AE_Module` with an initializer method and a forward pass method. It also includes a nested class `Upsampler`. The initializer sets up instances of the `Decoder` and `LatentRescaler` classes, which are used in the forward pass. The `Upsampler` class is defined within this code block and takes in input and output sizes, channel counts, and a multiplier for channel expansion. The constructor calculates the number of upsampling blocks needed based on the size difference between input and output, and the factor by which to increase each block's input size. The constructor also prints out the details of the upsampler being built.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/networks/ae_modules.py\":724-743",
            "content": "        tmp_chn = z_channels*ch_mult[-1]\n        self.decoder = Decoder(out_ch=out_ch, z_channels=tmp_chn, attn_resolutions=attn_resolutions, dropout=dropout,\n                               resamp_with_conv=resamp_with_conv, in_channels=None, num_res_blocks=num_res_blocks,\n                               ch_mult=ch_mult, resolution=resolution, ch=ch)\n        self.rescaler = LatentRescaler(factor=rescale_factor, in_channels=z_channels, mid_channels=tmp_chn,\n                                       out_channels=tmp_chn, depth=rescale_module_depth)\n    def forward(self, x):\n        x = self.rescaler(x)\n        x = self.decoder(x)\n        return x\nclass Upsampler(nn.Module):\n    def __init__(self, in_size, out_size, in_channels, out_channels, ch_mult=2):\n        super().__init__()\n        assert out_size >= in_size\n        num_blocks = int(np.log2(out_size//in_size))+1\n        factor_up = 1.+ (out_size % in_size)\n        print(f\"Building {self.__class__.__name__} with in_size: {in_size} --> out_size {out_size} and factor {factor_up}\")"
        },
        {
            "comment": "This code defines a class that includes a rescaler and decoder for image processing. The rescaler adjusts the input based on a factor, and the decoder applies a specific configuration to decode the adjusted data. It also includes a Resize class that can use either learned or fixed downsampling but currently raises an error for learned downsampling.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/networks/ae_modules.py\":744-765",
            "content": "        self.rescaler = LatentRescaler(factor=factor_up, in_channels=in_channels, mid_channels=2*in_channels,\n                                       out_channels=in_channels)\n        self.decoder = Decoder(out_ch=out_channels, resolution=out_size, z_channels=in_channels, num_res_blocks=2,\n                               attn_resolutions=[], in_channels=None, ch=in_channels,\n                               ch_mult=[ch_mult for _ in range(num_blocks)])\n    def forward(self, x):\n        x = self.rescaler(x)\n        x = self.decoder(x)\n        return x\nclass Resize(nn.Module):\n    def __init__(self, in_channels=None, learned=False, mode=\"bilinear\"):\n        super().__init__()\n        self.with_conv = learned\n        self.mode = mode\n        if self.with_conv:\n            print(f\"Note: {self.__class__.__name} uses learned downsampling and will ignore the fixed {mode} mode\")\n            raise NotImplementedError()\n            assert in_channels is not None\n            # no asymmetric padding in torch conv, must do it ourselves"
        },
        {
            "comment": "The code defines a Conv2d layer and a forward function that performs interpolation if the scale_factor is not 1.0. It also initializes a FirstStagePostProcessor class with various parameters such as channel multipliers, input channels, pretrained model or config, reshape flag, number of output channels, and dropout rate.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/networks/ae_modules.py\":766-791",
            "content": "            self.conv = torch.nn.Conv2d(in_channels,\n                                        in_channels,\n                                        kernel_size=4,\n                                        stride=2,\n                                        padding=1)\n    def forward(self, x, scale_factor=1.0):\n        if scale_factor==1.0:\n            return x\n        else:\n            x = torch.nn.functional.interpolate(x, mode=self.mode, align_corners=False, scale_factor=scale_factor)\n        return x\nclass FirstStagePostProcessor(nn.Module):\n    def __init__(self, ch_mult:list, in_channels,\n                 pretrained_model:nn.Module=None,\n                 reshape=False,\n                 n_channels=None,\n                 dropout=0.,\n                 pretrained_config=None):\n        super().__init__()\n        if pretrained_config is None:\n            assert pretrained_model is not None, 'Either \"pretrained_model\" or \"pretrained_config\" must not be None'\n            self.pretrained_model = pretrained_model\n        else:"
        },
        {
            "comment": "The code initializes a residual network (ResnetBlock) for an autoencoder. It takes input channels, output channels, and dropout rate as parameters. The `instantiate_pretrained` method is called to create the pre-trained model based on the provided configuration. The code also defines downsampler modules.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/networks/ae_modules.py\":792-819",
            "content": "            assert pretrained_config is not None, 'Either \"pretrained_model\" or \"pretrained_config\" must not be None'\n            self.instantiate_pretrained(pretrained_config)\n        self.do_reshape = reshape\n        if n_channels is None:\n            n_channels = self.pretrained_model.encoder.ch\n        self.proj_norm = Normalize(in_channels,num_groups=in_channels//2)\n        self.proj = nn.Conv2d(in_channels,n_channels,kernel_size=3,\n                            stride=1,padding=1)\n        blocks = []\n        downs = []\n        ch_in = n_channels\n        for m in ch_mult:\n            blocks.append(ResnetBlock(in_channels=ch_in,out_channels=m*n_channels,dropout=dropout))\n            ch_in = m * n_channels\n            downs.append(Downsample(ch_in, with_conv=False))\n        self.model = nn.ModuleList(blocks)\n        self.downsampler = nn.ModuleList(downs)\n    def instantiate_pretrained(self, config):\n        model = instantiate_from_config(config)\n        self.pretrained_model = model.eval()\n        # self.pretrained_model.train = False"
        },
        {
            "comment": "This code belongs to a network module and disables gradient computation for all parameters in the pretrained model. It has two functions: 'encode_with_pretrained' which encodes an input using the pretrained model and returns either the encoded result or its mode, and 'forward' which performs forward propagation through the network. The 'forward' function applies normalization, nonlinearity, and downsampling operations before potentially reshaping the output.",
            "location": "\"/media/root/Prima/works/DynamiCrafter/docs/src/lvdm/modules/networks/ae_modules.py\":820-843",
            "content": "        for param in self.pretrained_model.parameters():\n            param.requires_grad = False\n    @torch.no_grad()\n    def encode_with_pretrained(self,x):\n        c = self.pretrained_model.encode(x)\n        if isinstance(c, DiagonalGaussianDistribution):\n            c = c.mode()\n        return  c\n    def forward(self,x):\n        z_fs = self.encode_with_pretrained(x)\n        z = self.proj_norm(z_fs)\n        z = self.proj(z)\n        z = nonlinearity(z)\n        for submodel, downmodel in zip(self.model,self.downsampler):\n            z = submodel(z,temb=None)\n            z = downmodel(z)\n        if self.do_reshape:\n            z = rearrange(z,'b c h w -> b (h w) c')\n        return z"
        }
    ]
}