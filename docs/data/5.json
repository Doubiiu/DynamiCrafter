{
    "500": {
        "file_id": 21,
        "content": "                                            unconditional_guidance_scale=cfg_scale,\n                                            unconditional_conditioning=uc,\n                                            eta=ddim_eta,\n                                            temporal_length=noise_shape[2],\n                                            conditional_guidance_scale_temporal=temporal_cfg_scale,\n                                            x_T=x_T,\n                                            fs=fs,\n                                            **kwargs\n                                            )\n        ## reconstruct from latent to pixel space\n        batch_images = model.decode_first_stage(samples)\n        batch_variants.append(batch_images)\n    ## batch, <samples>, c, t, h, w\n    batch_variants = torch.stack(batch_variants, dim=1)\n    return batch_variants\ndef get_filelist(data_dir, ext='*'):\n    file_list = glob.glob(os.path.join(data_dir, '*.%s'%ext))\n    file_list.sort()\n    return file_list\ndef get_dirlist(path):",
        "type": "code",
        "location": "/scripts/evaluation/funcs.py:58-80"
    },
    "501": {
        "file_id": 21,
        "content": "The code defines a function that takes in samples and returns variants of the samples reconstructed from latent to pixel space. It uses the DDIM algorithm for unconditional image generation, and includes options for various parameters such as conditional guidance scale and temporal configuration scale. The code also provides utility functions for getting file lists and directory lists.",
        "type": "comment"
    },
    "502": {
        "file_id": 21,
        "content": "    list = []\n    if (os.path.exists(path)):\n        files = os.listdir(path)\n        for file in files:\n            m = os.path.join(path,file)\n            if (os.path.isdir(m)):\n                list.append(m)\n    list.sort()\n    return list\ndef load_model_checkpoint(model, ckpt):\n    def load_checkpoint(model, ckpt, full_strict):\n        state_dict = torch.load(ckpt, map_location=\"cpu\")\n        try:\n            ## deepspeed\n            new_pl_sd = OrderedDict()\n            for key in state_dict['module'].keys():\n                new_pl_sd[key[16:]]=state_dict['module'][key]\n            model.load_state_dict(new_pl_sd, strict=full_strict)\n        except:\n            if \"state_dict\" in list(state_dict.keys()):\n                state_dict = state_dict[\"state_dict\"]\n            model.load_state_dict(state_dict, strict=full_strict)\n        return model\n    load_checkpoint(model, ckpt, full_strict=True)\n    print('>>> model checkpoint loaded.')\n    return model\ndef load_prompts(prompt_file):\n    f = open(prompt_file, 'r')",
        "type": "code",
        "location": "/scripts/evaluation/funcs.py:81-112"
    },
    "503": {
        "file_id": 21,
        "content": "The code defines two functions: `load_model_checkpoint` and `load_prompts`. The `load_model_checkpoint` function attempts to load a model's checkpoint from a file, considering the potential use of Deepspeed library. It first tries loading the checkpoint as it is and if an error occurs, it reloads it without the \"module.\" prefix. The `load_prompts` function opens a file containing prompts for reading later use.",
        "type": "comment"
    },
    "504": {
        "file_id": 21,
        "content": "    prompt_list = []\n    for idx, line in enumerate(f.readlines()):\n        l = line.strip()\n        if len(l) != 0:\n            prompt_list.append(l)\n        f.close()\n    return prompt_list\ndef load_video_batch(filepath_list, frame_stride, video_size=(256,256), video_frames=16):\n    '''\n    Notice about some special cases:\n    1. video_frames=-1 means to take all the frames (with fs=1)\n    2. when the total video frames is less than required, padding strategy will be used (repreated last frame)\n    '''\n    fps_list = []\n    batch_tensor = []\n    assert frame_stride > 0, \"valid frame stride should be a positive interge!\"\n    for filepath in filepath_list:\n        padding_num = 0\n        vidreader = VideoReader(filepath, ctx=cpu(0), width=video_size[1], height=video_size[0])\n        fps = vidreader.get_avg_fps()\n        total_frames = len(vidreader)\n        max_valid_frames = (total_frames-1) // frame_stride + 1\n        if video_frames < 0:\n            ## all frames are collected: fs=1 is a must\n            required_frames = total_frames",
        "type": "code",
        "location": "/scripts/evaluation/funcs.py:113-139"
    },
    "505": {
        "file_id": 21,
        "content": "This function reads a list of filepaths and processes each video file using VideoReader. It calculates the average frames per second (fps) for each video and determines the maximum valid number of frames based on the specified frame stride. If the requested number of video frames is less than or equal to -1, all frames are collected (fs=1 is a must). The function also checks if the frame stride is a positive integer. It returns the list of prompt strings and the processed batch tensor.",
        "type": "comment"
    },
    "506": {
        "file_id": 21,
        "content": "            frame_stride = 1\n        else:\n            required_frames = video_frames\n        query_frames = min(required_frames, max_valid_frames)\n        frame_indices = [frame_stride*i for i in range(query_frames)]\n        ## [t,h,w,c] -> [c,t,h,w]\n        frames = vidreader.get_batch(frame_indices)\n        frame_tensor = torch.tensor(frames.asnumpy()).permute(3, 0, 1, 2).float()\n        frame_tensor = (frame_tensor / 255. - 0.5) * 2\n        if max_valid_frames < required_frames:\n            padding_num = required_frames - max_valid_frames\n            frame_tensor = torch.cat([frame_tensor, *([frame_tensor[:,-1:,:,:]]*padding_num)], dim=1)\n            print(f'{os.path.split(filepath)[1]} is not long enough: {padding_num} frames padded.')\n        batch_tensor.append(frame_tensor)\n        sample_fps = int(fps/frame_stride)\n        fps_list.append(sample_fps)\n    return torch.stack(batch_tensor, dim=0)\nfrom PIL import Image\ndef load_image_batch(filepath_list, image_size=(256,256)):\n    batch_tensor = []\n    for filepath in filepath_list:",
        "type": "code",
        "location": "/scripts/evaluation/funcs.py:140-163"
    },
    "507": {
        "file_id": 21,
        "content": "This code extracts frames from a video and images from a list, resizes them, and stores them in batches. It handles cases where the number of frames required exceeds the available frames by padding with duplicate last frame(s). The function returns a tensor containing the batched image data.",
        "type": "comment"
    },
    "508": {
        "file_id": 21,
        "content": "        _, filename = os.path.split(filepath)\n        _, ext = os.path.splitext(filename)\n        if ext == '.mp4':\n            vidreader = VideoReader(filepath, ctx=cpu(0), width=image_size[1], height=image_size[0])\n            frame = vidreader.get_batch([0])\n            img_tensor = torch.tensor(frame.asnumpy()).squeeze(0).permute(2, 0, 1).float()\n        elif ext == '.png' or ext == '.jpg':\n            img = Image.open(filepath).convert(\"RGB\")\n            rgb_img = np.array(img, np.float32)\n            #bgr_img = cv2.imread(filepath, cv2.IMREAD_COLOR)\n            #bgr_img = cv2.cvtColor(bgr_img, cv2.COLOR_BGR2RGB)\n            rgb_img = cv2.resize(rgb_img, (image_size[1],image_size[0]), interpolation=cv2.INTER_LINEAR)\n            img_tensor = torch.from_numpy(rgb_img).permute(2, 0, 1).float()\n        else:\n            print(f'ERROR: <{ext}> image loading only support format: [mp4], [png], [jpg]')\n            raise NotImplementedError\n        img_tensor = (img_tensor / 255. - 0.5) * 2\n        batch_tensor.append(img_tensor)",
        "type": "code",
        "location": "/scripts/evaluation/funcs.py:164-181"
    },
    "509": {
        "file_id": 21,
        "content": "This code reads a file and determines its format. If it's an mp4 file, it creates a VideoReader object to extract frames as tensors. If it's a png or jpg image, it reads the image using PIL, converts it to RGB, resizes it, and converts it to a tensor. For other formats, it raises an error. It then normalizes each tensor element between 0 and 1 before appending it to batch_tensor.",
        "type": "comment"
    },
    "510": {
        "file_id": 21,
        "content": "    return torch.stack(batch_tensor, dim=0)\ndef save_videos(batch_tensors, savedir, filenames, fps=10):\n    # b,samples,c,t,h,w\n    n_samples = batch_tensors.shape[1]\n    for idx, vid_tensor in enumerate(batch_tensors):\n        video = vid_tensor.detach().cpu()\n        video = torch.clamp(video.float(), -1., 1.)\n        video = video.permute(2, 0, 1, 3, 4) # t,n,c,h,w\n        frame_grids = [torchvision.utils.make_grid(framesheet, nrow=int(n_samples)) for framesheet in video] #[3, 1*h, n*w]\n        grid = torch.stack(frame_grids, dim=0) # stack in temporal dim [t, 3, n*h, w]\n        grid = (grid + 1.0) / 2.0\n        grid = (grid * 255).to(torch.uint8).permute(0, 2, 3, 1)\n        savepath = os.path.join(savedir, f\"{filenames[idx]}.mp4\")\n        torchvision.io.write_video(savepath, grid, fps=fps, video_codec='h264', options={'crf': '10'})\ndef get_latent_z(model, videos):\n    b, c, t, h, w = videos.shape\n    x = rearrange(videos, 'b c t h w -> (b t) c h w')\n    z = model.encode_first_stage(x)\n    z = rearrange(z, '(b t) c h w -> b c t h w', b=b, t=t)",
        "type": "code",
        "location": "/scripts/evaluation/funcs.py:182-204"
    },
    "511": {
        "file_id": 21,
        "content": "This code defines three functions: 'get_latent_z', 'save_videos', and 'rearrange'. It performs video encoding, saving videos, and tensor rearrangement respectively. The get_latent_z function takes a model and videos as input, encodes the videos using the first-stage encode of the model, and returns the encoded output. The save_videos function takes batch_tensors, savedir, filenames, and fps as inputs, processes each video tensor, saves the processed frames as an mp4 file with h264 codec.",
        "type": "comment"
    },
    "512": {
        "file_id": 21,
        "content": "    return z",
        "type": "code",
        "location": "/scripts/evaluation/funcs.py:205-205"
    },
    "513": {
        "file_id": 21,
        "content": "This code snippet returns the variable 'z'.",
        "type": "comment"
    },
    "514": {
        "file_id": 22,
        "content": "/scripts/evaluation/inference.py",
        "type": "filepath"
    },
    "515": {
        "file_id": 22,
        "content": "This code handles data loading, model checkpoints, and DDIMSampler for deep learning models in video synthesis, ensuring compatibility, GPU usage, file existence checks, and prompted example saving. It uses argparse and random seeds for command-line arguments and inference tracking. Looping video generation not supported.",
        "type": "summary"
    },
    "516": {
        "file_id": 22,
        "content": "import argparse, os, sys, glob\nimport datetime, time\nfrom omegaconf import OmegaConf\nfrom tqdm import tqdm\nfrom einops import rearrange, repeat\nfrom collections import OrderedDict\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\nfrom pytorch_lightning import seed_everything\nfrom PIL import Image\nsys.path.insert(1, os.path.join(sys.path[0], '..', '..'))\nfrom lvdm.models.samplers.ddim import DDIMSampler\nfrom lvdm.models.samplers.ddim_multiplecond import DDIMSampler as DDIMSampler_multicond\nfrom utils.utils import instantiate_from_config\ndef get_filelist(data_dir, postfixes):\n    patterns = [os.path.join(data_dir, f\"*.{postfix}\") for postfix in postfixes]\n    file_list = []\n    for pattern in patterns:\n        file_list.extend(glob.glob(pattern))\n    file_list.sort()\n    return file_list\ndef load_model_checkpoint(model, ckpt):\n    state_dict = torch.load(ckpt, map_location=\"cpu\")\n    if \"state_dict\" in list(state_dict.keys()):\n        state_dict = state_dict[\"state_dict\"]\n        model.load_state_dict(state_dict, strict=True)",
        "type": "code",
        "location": "/scripts/evaluation/inference.py:1-31"
    },
    "517": {
        "file_id": 22,
        "content": "This code imports necessary libraries and defines functions for loading data, handling model checkpoints, and using DDIMSampler. It also provides a way to load model checkpoints into the specified model with strict loading. The get_filelist function retrieves file lists from a given directory with specified postfixes, sorting them before returning. This code is part of a larger program likely involving image generation or manipulation using a specific model.",
        "type": "comment"
    },
    "518": {
        "file_id": 22,
        "content": "    else:\n        # deepspeed\n        new_pl_sd = OrderedDict()\n        for key in state_dict['module'].keys():\n            new_pl_sd[key[16:]]=state_dict['module'][key]\n        model.load_state_dict(new_pl_sd)\n    print('>>> model checkpoint loaded.')\n    return model\ndef load_prompts(prompt_file):\n    f = open(prompt_file, 'r')\n    prompt_list = []\n    for idx, line in enumerate(f.readlines()):\n        l = line.strip()\n        if len(l) != 0:\n            prompt_list.append(l)\n        f.close()\n    return prompt_list\ndef load_data_prompts(data_dir, video_size=(256,256), video_frames=16, gfi=False):\n    transform = transforms.Compose([\n        transforms.Resize(min(video_size)),\n        transforms.CenterCrop(video_size),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))])\n    ## load prompts\n    prompt_file = get_filelist(data_dir, ['txt'])\n    assert len(prompt_file) > 0, \"Error: found NO prompt file!\"\n    ###### default prompt\n    default_idx = 0\n    default_idx = min(default_idx, len(prompt_file)-1)",
        "type": "code",
        "location": "/scripts/evaluation/inference.py:32-62"
    },
    "519": {
        "file_id": 22,
        "content": "Code handles model checkpoint loading, prompt loading, and data preparation. It includes functions for loading the model checkpoint, reading prompts from a file, and applying image transformations to prepare the data.",
        "type": "comment"
    },
    "520": {
        "file_id": 22,
        "content": "    if len(prompt_file) > 1:\n        print(f\"Warning: multiple prompt files exist. The one {os.path.split(prompt_file[default_idx])[1]} is used.\")\n    ## only use the first one (sorted by name) if multiple exist\n    ## load video\n    file_list = get_filelist(data_dir, ['jpg', 'png', 'jpeg', 'JPEG', 'PNG'])\n    # assert len(file_list) == n_samples, \"Error: data and prompts are NOT paired!\"\n    data_list = []\n    filename_list = []\n    prompt_list = load_prompts(prompt_file[default_idx])\n    n_samples = len(prompt_list)\n    for idx in range(n_samples):\n        image = Image.open(file_list[idx]).convert('RGB')\n        image_tensor = transform(image).unsqueeze(1) # [c,1,h,w]\n        frame_tensor = repeat(image_tensor, 'c t h w -> c (repeat t) h w', repeat=video_frames)\n        data_list.append(frame_tensor)\n        _, filename = os.path.split(file_list[idx])\n        filename_list.append(filename)\n    return filename_list, data_list, prompt_list\ndef save_results(prompt, samples, filename, fakedir, fps=8, loop=False):",
        "type": "code",
        "location": "/scripts/evaluation/inference.py:63-86"
    },
    "521": {
        "file_id": 22,
        "content": "This function loads multiple prompt files and selects the first one alphabetically. It then extracts image data from the specified directory, converts it to a suitable tensor format, and stores the filename and data in separate lists. Finally, it returns these lists along with the prompt file list for further processing or saving.",
        "type": "comment"
    },
    "522": {
        "file_id": 22,
        "content": "    filename = filename.split('.')[0]+'.mp4'\n    prompt = prompt[0] if isinstance(prompt, list) else prompt\n    ## save video\n    videos = [samples]\n    savedirs = [fakedir]\n    for idx, video in enumerate(videos):\n        if video is None:\n            continue\n        # b,c,t,h,w\n        video = video.detach().cpu()\n        video = torch.clamp(video.float(), -1., 1.)\n        n = video.shape[0]\n        video = video.permute(2, 0, 1, 3, 4) # t,n,c,h,w\n        if loop:\n            video = video[:-1,...]\n        frame_grids = [torchvision.utils.make_grid(framesheet, nrow=int(n), padding=0) for framesheet in video] #[3, 1*h, n*w]\n        grid = torch.stack(frame_grids, dim=0) # stack in temporal dim [t, 3, h, n*w]\n        grid = (grid + 1.0) / 2.0\n        grid = (grid * 255).to(torch.uint8).permute(0, 2, 3, 1)\n        path = os.path.join(savedirs[idx], filename)\n        torchvision.io.write_video(path, grid, fps=fps, video_codec='h264', options={'crf': '10'}) ## crf indicates the quality\ndef save_results_seperate(prompt, samples, filename, fakedir, fps=10, loop=False):",
        "type": "code",
        "location": "/scripts/evaluation/inference.py:87-112"
    },
    "523": {
        "file_id": 22,
        "content": "The code saves a video from the input samples and writes it to a specified directory. It processes the video by clamping values, performing necessary permutations, stacking frames, and adjusting brightness before writing it out in .mp4 format using ffmpeg's h264 codec with a quality indicator (CRF).",
        "type": "comment"
    },
    "524": {
        "file_id": 22,
        "content": "    prompt = prompt[0] if isinstance(prompt, list) else prompt\n    ## save video\n    videos = [samples]\n    savedirs = [fakedir]\n    for idx, video in enumerate(videos):\n        if video is None:\n            continue\n        # b,c,t,h,w\n        video = video.detach().cpu()\n        if loop: # remove the last frame\n            video = video[:,:,:-1,...]\n        video = torch.clamp(video.float(), -1., 1.)\n        n = video.shape[0]\n        for i in range(n):\n            grid = video[i,...]\n            grid = (grid + 1.0) / 2.0\n            grid = (grid * 255).to(torch.uint8).permute(1, 2, 3, 0) #thwc\n            path = os.path.join(savedirs[idx].replace('samples', 'samples_separate'), f'{filename.split(\".\")[0]}_sample{i}.mp4')\n            torchvision.io.write_video(path, grid, fps=fps, video_codec='h264', options={'crf': '10'})\ndef get_latent_z(model, videos):\n    b, c, t, h, w = videos.shape\n    x = rearrange(videos, 'b c t h w -> (b t) c h w')\n    z = model.encode_first_stage(x)\n    z = rearrange(z, '(b t) c h w -> b c t h w', b=b, t=t)",
        "type": "code",
        "location": "/scripts/evaluation/inference.py:113-138"
    },
    "525": {
        "file_id": 22,
        "content": "This code saves videos by processing them through a deep learning model. The model encodes the video frames, then rearranges and saves each frame as an individual mp4 file.",
        "type": "comment"
    },
    "526": {
        "file_id": 22,
        "content": "    return z\ndef image_guided_synthesis(model, prompts, videos, noise_shape, n_samples=1, ddim_steps=50, ddim_eta=1., \\\n                        unconditional_guidance_scale=1.0, cfg_img=None, fs=None, text_input=False, multiple_cond_cfg=False, loop=False, gfi=False, **kwargs):\n    ddim_sampler = DDIMSampler(model) if not multiple_cond_cfg else DDIMSampler_multicond(model)\n    batch_size = noise_shape[0]\n    fs = torch.tensor([fs] * batch_size, dtype=torch.long, device=model.device)\n    if not text_input:\n        prompts = [\"\"]*batch_size\n    img = videos[:,:,0] #bchw\n    img_emb = model.embedder(img) ## blc\n    img_emb = model.image_proj_model(img_emb)\n    cond_emb = model.get_learned_conditioning(prompts)\n    cond = {\"c_crossattn\": [torch.cat([cond_emb,img_emb], dim=1)]}\n    if model.model.conditioning_key == 'hybrid':\n        z = get_latent_z(model, videos) # b c t h w\n        if loop or gfi:\n            img_cat_cond = torch.zeros_like(z)\n            img_cat_cond[:,:,0,:,:] = z[:,:,0,:,:]\n            img_cat_cond[:,:,-1,:,:] = z[:,:,-1,:,:]",
        "type": "code",
        "location": "/scripts/evaluation/inference.py:139-162"
    },
    "527": {
        "file_id": 22,
        "content": "This function performs image-guided synthesis using a DDIM sampler. It takes a model, prompts, videos, noise shape, and optional parameters as input. The code first creates a DDIM sampler depending on the multiple_cond_cfg flag. Then, it sets batch size and initializes fs tensor. If text_input is False, prompts are set to empty strings for all batch elements. It extracts image embeddings and applies model's image projection. Next, it retrieves conditioning embedding from the model. Finally, if the model uses hybrid conditioning, it calculates latent z and creates img_cat_cond tensor.",
        "type": "comment"
    },
    "528": {
        "file_id": 22,
        "content": "        else:\n            img_cat_cond = z[:,:,:1,:,:]\n            img_cat_cond = repeat(img_cat_cond, 'b c t h w -> b c (repeat t) h w', repeat=z.shape[2])\n        cond[\"c_concat\"] = [img_cat_cond] # b c 1 h w\n    if unconditional_guidance_scale != 1.0:\n        if model.uncond_type == \"empty_seq\":\n            prompts = batch_size * [\"\"]\n            uc_emb = model.get_learned_conditioning(prompts)\n        elif model.uncond_type == \"zero_embed\":\n            uc_emb = torch.zeros_like(cond_emb)\n        uc_img_emb = model.embedder(torch.zeros_like(img)) ## b l c\n        uc_img_emb = model.image_proj_model(uc_img_emb)\n        uc = {\"c_crossattn\": [torch.cat([uc_emb,uc_img_emb],dim=1)]}\n        if model.model.conditioning_key == 'hybrid':\n            uc[\"c_concat\"] = [img_cat_cond]\n    else:\n        uc = None\n    ## we need one more unconditioning image=yes, text=\"\"\n    if multiple_cond_cfg and cfg_img != 1.0:\n        uc_2 = {\"c_crossattn\": [torch.cat([uc_emb,img_emb],dim=1)]}\n        if model.model.conditioning_key == 'hybrid':",
        "type": "code",
        "location": "/scripts/evaluation/inference.py:163-185"
    },
    "529": {
        "file_id": 22,
        "content": "The code is setting up unconditional and conditional inputs for a model. It checks the type of conditioning, creates or retrieves embedding vectors, and concatenates them with image embeddings. If the model requires hybrid conditioning, it adds the image to the unconditional input as well. The code also handles cases where multiple conditions are used and adjusts the inputs accordingly.",
        "type": "comment"
    },
    "530": {
        "file_id": 22,
        "content": "            uc_2[\"c_concat\"] = [img_cat_cond]\n        kwargs.update({\"unconditional_conditioning_img_nonetext\": uc_2})\n    else:\n        kwargs.update({\"unconditional_conditioning_img_nonetext\": None})\n    z0 = None\n    cond_mask = None\n    batch_variants = []\n    for _ in range(n_samples):\n        if z0 is not None:\n            cond_z0 = z0.clone()\n            kwargs.update({\"clean_cond\": True})\n        else:\n            cond_z0 = None\n        if ddim_sampler is not None:\n            samples, _ = ddim_sampler.sample(S=ddim_steps,\n                                            conditioning=cond,\n                                            batch_size=batch_size,\n                                            shape=noise_shape[1:],\n                                            verbose=False,\n                                            unconditional_guidance_scale=unconditional_guidance_scale,\n                                            unconditional_conditioning=uc,\n                                            eta=ddim_eta,",
        "type": "code",
        "location": "/scripts/evaluation/inference.py:186-211"
    },
    "531": {
        "file_id": 22,
        "content": "This code is setting up arguments for a conditional diffusion model. It checks if a specific conditioning image is provided and updates the necessary variables accordingly. It then uses the DDIM sampler to generate samples, with options for unconditional guidance scale and eta. The final output depends on whether the conditioning image is provided or not.",
        "type": "comment"
    },
    "532": {
        "file_id": 22,
        "content": "                                            cfg_img=cfg_img, \n                                            mask=cond_mask,\n                                            x0=cond_z0,\n                                            fs=fs,\n                                            **kwargs\n                                            )\n        ## reconstruct from latent to pixel space\n        batch_images = model.decode_first_stage(samples)\n        batch_variants.append(batch_images)\n    ## variants, batch, c, t, h, w\n    batch_variants = torch.stack(batch_variants)\n    return batch_variants.permute(1, 0, 2, 3, 4, 5)\ndef run_inference(args, gpu_num, gpu_no):\n    ## model config\n    config = OmegaConf.load(args.config)\n    model_config = config.pop(\"model\", OmegaConf.create())\n    ## set use_checkpoint as False as when using deepspeed, it encounters an error \"deepspeed backend not set\"\n    model_config['params']['unet_config']['params']['use_checkpoint'] = False\n    model = instantiate_from_config(model_config)\n    model = model.cuda(gpu_no)",
        "type": "code",
        "location": "/scripts/evaluation/inference.py:212-235"
    },
    "533": {
        "file_id": 22,
        "content": "The code snippet is part of an inference function. It loads a model configuration and instantiates the model using the configuration. The model's use_checkpoint is set to False, possibly due to compatibility issues with DeepSpeed. The model is then moved to a specific GPU for computation.",
        "type": "comment"
    },
    "534": {
        "file_id": 22,
        "content": "    assert os.path.exists(args.ckpt_path), \"Error: checkpoint Not Found!\"\n    model = load_model_checkpoint(model, args.ckpt_path)\n    model.eval()\n    ## run over data\n    assert (args.height % 16 == 0) and (args.width % 16 == 0), \"Error: image size [h,w] should be multiples of 16!\"\n    assert args.bs == 1, \"Current implementation only support [batch size = 1]!\"\n    ## latent noise shape\n    h, w = args.height // 8, args.width // 8\n    channels = model.model.diffusion_model.out_channels\n    n_frames = args.video_length\n    print(f'Inference with {n_frames} frames')\n    noise_shape = [args.bs, channels, n_frames, h, w]\n    fakedir = os.path.join(args.savedir, \"samples\")\n    fakedir_separate = os.path.join(args.savedir, \"samples_separate\")\n    # os.makedirs(fakedir, exist_ok=True)\n    os.makedirs(fakedir_separate, exist_ok=True)\n    ## prompt file setting\n    assert os.path.exists(args.prompt_dir), \"Error: prompt file Not Found!\"\n    filename_list, data_list, prompt_list = load_data_prompts(args.prompt_dir, video_size=(args.height, args.width), video_frames=n_frames, gfi=args.gfi)",
        "type": "code",
        "location": "/scripts/evaluation/inference.py:237-259"
    },
    "535": {
        "file_id": 22,
        "content": "The code asserts if the checkpoint file and prompt directory exist, loads the model with the checkpoint path, evaluates the model, checks for image size compatibility, enforces batch size of 1, calculates latent noise shape, creates output directories, and loads data prompts from the specified directory.",
        "type": "comment"
    },
    "536": {
        "file_id": 22,
        "content": "    num_samples = len(prompt_list)\n    samples_split = num_samples // gpu_num\n    print('Prompts testing [rank:%d] %d/%d samples loaded.'%(gpu_no, samples_split, num_samples))\n    #indices = random.choices(list(range(0, num_samples)), k=samples_per_device)\n    indices = list(range(samples_split*gpu_no, samples_split*(gpu_no+1)))\n    prompt_list_rank = [prompt_list[i] for i in indices]\n    data_list_rank = [data_list[i] for i in indices]\n    filename_list_rank = [filename_list[i] for i in indices]\n    start = time.time()\n    with torch.no_grad(), torch.cuda.amp.autocast():\n        for idx, indice in tqdm(enumerate(range(0, len(prompt_list_rank), args.bs)), desc='Sample Batch'):\n            prompts = prompt_list_rank[indice:indice+args.bs]\n            videos = data_list_rank[indice:indice+args.bs]\n            filenames = filename_list_rank[indice:indice+args.bs]\n            if isinstance(videos, list):\n                videos = torch.stack(videos, dim=0).to(\"cuda\")\n            else:\n                videos = videos.unsqueeze(0).to(\"cuda\")",
        "type": "code",
        "location": "/scripts/evaluation/inference.py:260-278"
    },
    "537": {
        "file_id": 22,
        "content": "This code selects a subset of prompts, data, and filenames for evaluation or inference. It assigns the selected items to specific GPUs based on their indices, and then processes them using automatic mixed precision (AMP). The loop iterates over the selected subsets, stacks videos if they are in a list format, and moves all data to the GPU. This code ensures efficient distribution of tasks across available GPUs for faster processing.",
        "type": "comment"
    },
    "538": {
        "file_id": 22,
        "content": "            batch_samples = image_guided_synthesis(model, prompts, videos, noise_shape, args.n_samples, args.ddim_steps, args.ddim_eta, \\\n                                args.unconditional_guidance_scale, args.cfg_img, args.frame_stride, args.text_input, args.multiple_cond_cfg, args.loop, args.gfi)\n            ## save each example individually\n            for nn, samples in enumerate(batch_samples):\n                ## samples : [n_samples,c,t,h,w]\n                prompt = prompts[nn]\n                filename = filenames[nn]\n                # save_results(prompt, samples, filename, fakedir, fps=8, loop=args.loop)\n                save_results_seperate(prompt, samples, filename, fakedir, fps=8, loop=args.loop)\n    print(f\"Saved in {args.savedir}. Time used: {(time.time() - start):.2f} seconds\")\ndef get_parser():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--savedir\", type=str, default=None, help=\"results saving path\")\n    parser.add_argument(\"--ckpt_path\", type=str, default=None, help=\"checkpoint path\")",
        "type": "code",
        "location": "/scripts/evaluation/inference.py:280-297"
    },
    "539": {
        "file_id": 22,
        "content": "This code performs image generation using a model and prompts, saving each generated example individually. It utilizes a function called `image_guided_synthesis` to create batches of samples, then saves them with their corresponding prompt and filename. The results are saved in the specified directory, and the time taken is printed.",
        "type": "comment"
    },
    "540": {
        "file_id": 22,
        "content": "    parser.add_argument(\"--config\", type=str, help=\"config (yaml) path\")\n    parser.add_argument(\"--prompt_dir\", type=str, default=None, help=\"a data dir containing videos and prompts\")\n    parser.add_argument(\"--n_samples\", type=int, default=1, help=\"num of samples per prompt\",)\n    parser.add_argument(\"--ddim_steps\", type=int, default=50, help=\"steps of ddim if positive, otherwise use DDPM\",)\n    parser.add_argument(\"--ddim_eta\", type=float, default=1.0, help=\"eta for ddim sampling (0.0 yields deterministic sampling)\",)\n    parser.add_argument(\"--bs\", type=int, default=1, help=\"batch size for inference, should be one\")\n    parser.add_argument(\"--height\", type=int, default=512, help=\"image height, in pixel space\")\n    parser.add_argument(\"--width\", type=int, default=512, help=\"image width, in pixel space\")\n    parser.add_argument(\"--frame_stride\", type=int, default=3, choices=[1, 2, 3, 4, 5, 6], help=\"frame stride control for results, smaller value->smaller motion magnitude and more stable, and vice versa\")",
        "type": "code",
        "location": "/scripts/evaluation/inference.py:298-306"
    },
    "541": {
        "file_id": 22,
        "content": "This code snippet is parsing command-line arguments for an inference script. It defines the config file path, data directory, number of samples per prompt, DDPM/DDim parameters, batch size, and image dimensions, as well as frame stride control options.",
        "type": "comment"
    },
    "542": {
        "file_id": 22,
        "content": "    parser.add_argument(\"--unconditional_guidance_scale\", type=float, default=1.0, help=\"prompt classifier-free guidance\")\n    parser.add_argument(\"--seed\", type=int, default=123, help=\"seed for seed_everything\")\n    parser.add_argument(\"--video_length\", type=int, default=16, help=\"inference video length\")\n    parser.add_argument(\"--negative_prompt\", action='store_true', default=False, help=\"negative prompt\")\n    parser.add_argument(\"--text_input\", action='store_true', default=False, help=\"input text to I2V model or not\")\n    parser.add_argument(\"--multiple_cond_cfg\", action='store_true', default=False, help=\"use multi-condition cfg or not\")\n    parser.add_argument(\"--cfg_img\", type=float, default=None, help=\"guidance scale for image conditioning\")\n    ## currently not support looping video and generative frame interpolation\n    parser.add_argument(\"--loop\", action='store_true', default=False, help=\"generate looping videos or not\")\n    parser.add_argument(\"--gfi\", action='store_true', default=False, help=\"generate generative frame interpolation (gfi) or not\")",
        "type": "code",
        "location": "/scripts/evaluation/inference.py:307-317"
    },
    "543": {
        "file_id": 22,
        "content": "The code above defines command line arguments using the \"argparse\" module in Python. These options control features such as unconditional guidance scale, seed value for randomization, video length, negative prompt usage, input type (text or image), and additional conditioning configurations. The looping video generation and generative frame interpolation are currently not supported.",
        "type": "comment"
    },
    "544": {
        "file_id": 22,
        "content": "    return parser\nif __name__ == '__main__':\n    now = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n    print(\"@CoVideoGen cond-Inference: %s\"%now)\n    parser = get_parser()\n    args = parser.parse_args()\n    seed_everything(args.seed)\n    rank, gpu_num = 0, 1\n    run_inference(args, gpu_num, rank)",
        "type": "code",
        "location": "/scripts/evaluation/inference.py:318-329"
    },
    "545": {
        "file_id": 22,
        "content": "This code sets up a command-line argument parser, then seeds the random number generator based on an input seed and runs inference with specified GPU number and rank. The timestamp is printed for tracking when the inference process started.",
        "type": "comment"
    },
    "546": {
        "file_id": 23,
        "content": "/scripts/gradio/i2v_test.py",
        "type": "filepath"
    },
    "547": {
        "file_id": 23,
        "content": "The code contains an Image2Video class for video synthesis using DDIM sampling, text and image conditioning in a diffusion model. It includes functions for character replacement, length limitation, model downloads, and video saving. The script generates a video from an image prompt and prints the generated video path upon completion.",
        "type": "summary"
    },
    "548": {
        "file_id": 23,
        "content": "import os\nimport time\nfrom omegaconf import OmegaConf\nimport torch\nfrom scripts.evaluation.funcs import load_model_checkpoint, save_videos, batch_ddim_sampling, get_latent_z\nfrom utils.utils import instantiate_from_config\nfrom huggingface_hub import hf_hub_download\nfrom einops import repeat\nimport torchvision.transforms as transforms\nfrom pytorch_lightning import seed_everything\nclass Image2Video():\n    def __init__(self,result_dir='./tmp/',gpu_num=1) -> None:\n        self.download_model()\n        self.result_dir = result_dir\n        if not os.path.exists(self.result_dir):\n            os.mkdir(self.result_dir)\n        ckpt_path='checkpoints/dynamicrafter_256_v1/model.ckpt'\n        config_file='configs/inference_256_v1.0.yaml'\n        config = OmegaConf.load(config_file)\n        model_config = config.pop(\"model\", OmegaConf.create())\n        model_config['params']['unet_config']['params']['use_checkpoint']=False   \n        model_list = []\n        for gpu_id in range(gpu_num):\n            model = instantiate_from_config(model_config)",
        "type": "code",
        "location": "/scripts/gradio/i2v_test.py:1-26"
    },
    "549": {
        "file_id": 23,
        "content": "This code is an implementation of the Image2Video class. It downloads the model, sets the result directory, and loads the model configuration from a YAML file. It also instantiates the model using the provided configuration and sets the GPU number for parallel processing.",
        "type": "comment"
    },
    "550": {
        "file_id": 23,
        "content": "            # model = model.cuda(gpu_id)\n            assert os.path.exists(ckpt_path), \"Error: checkpoint Not Found!\"\n            model = load_model_checkpoint(model, ckpt_path)\n            model.eval()\n            model_list.append(model)\n        self.model_list = model_list\n        self.save_fps = 8\n    def get_image(self, image, prompt, steps=50, cfg_scale=7.5, eta=1.0, fs=3, seed=123):\n        seed_everything(seed)\n        transform = transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(256),\n            ])\n        torch.cuda.empty_cache()\n        print('start:', prompt, time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())))\n        start = time.time()\n        gpu_id=0\n        if steps > 60:\n            steps = 60 \n        model = self.model_list[gpu_id]\n        model = model.cuda()\n        batch_size=1\n        channels = model.model.diffusion_model.out_channels\n        frames = model.temporal_length\n        h, w = 256 // 8, 256 // 8\n        noise_shape = [batch_size, channels, frames, h, w]",
        "type": "code",
        "location": "/scripts/gradio/i2v_test.py:27-53"
    },
    "551": {
        "file_id": 23,
        "content": "Function `get_image` takes an image and a prompt, applies transformations to resize and crop the input image, empties GPU cache, and prints the start time. It then selects the appropriate model from `self.model_list` based on GPU ID, sends it to GPU, sets batch size and channels, defines frames, and sets noise shape for inversion process.",
        "type": "comment"
    },
    "552": {
        "file_id": 23,
        "content": "        # text cond\n        text_emb = model.get_learned_conditioning([prompt])\n        # img cond\n        img_tensor = torch.from_numpy(image).permute(2, 0, 1).float().to(model.device)\n        img_tensor = (img_tensor / 255. - 0.5) * 2\n        image_tensor_resized = transform(img_tensor) #3,256,256\n        videos = image_tensor_resized.unsqueeze(0) # bchw\n        z = get_latent_z(model, videos.unsqueeze(2)) #bc,1,hw\n        img_tensor_repeat = repeat(z, 'b c t h w -> b c (repeat t) h w', repeat=frames)\n        cond_images = model.embedder(img_tensor.unsqueeze(0)) ## blc\n        img_emb = model.image_proj_model(cond_images)\n        imtext_cond = torch.cat([text_emb, img_emb], dim=1)\n        fs = torch.tensor([fs], dtype=torch.long, device=model.device)\n        cond = {\"c_crossattn\": [imtext_cond], \"fs\": fs, \"c_concat\": [img_tensor_repeat]}\n        ## inference\n        batch_samples = batch_ddim_sampling(model, cond, noise_shape, n_samples=1, ddim_steps=steps, ddim_eta=eta, cfg_scale=cfg_scale)\n        ## b,samples,c,t,h,w",
        "type": "code",
        "location": "/scripts/gradio/i2v_test.py:55-79"
    },
    "553": {
        "file_id": 23,
        "content": "This code generates images using DDIM sampling with text and image conditioning in a diffusion model. It prepares the text, image, and their embeddings, then performs batch inference through the DDIM sampling process.",
        "type": "comment"
    },
    "554": {
        "file_id": 23,
        "content": "        prompt_str = prompt.replace(\"/\", \"_slash_\") if \"/\" in prompt else prompt\n        prompt_str = prompt_str.replace(\" \", \"_\") if \" \" in prompt else prompt_str\n        prompt_str=prompt_str[:40]\n        if len(prompt_str) == 0:\n            prompt_str = 'empty_prompt'\n        save_videos(batch_samples, self.result_dir, filenames=[prompt_str], fps=self.save_fps)\n        print(f\"Saved in {prompt_str}. Time used: {(time.time() - start):.2f} seconds\")\n        model = model.cpu()\n        return os.path.join(self.result_dir, f\"{prompt_str}.mp4\")\n    def download_model(self):\n        REPO_ID = 'Doubiiu/DynamiCrafter'\n        filename_list = ['model.ckpt']\n        if not os.path.exists('./checkpoints/dynamicrafter_256_v1/'):\n            os.makedirs('./dynamicrafter_256_v1/')\n        for filename in filename_list:\n            local_file = os.path.join('./checkpoints/dynamicrafter_256_v1/', filename)\n            if not os.path.exists(local_file):\n                hf_hub_download(repo_id=REPO_ID, filename=filename, local_dir='./checkpoints/dynamicrafter_256_v1/', local_dir_use_symlinks=False)",
        "type": "code",
        "location": "/scripts/gradio/i2v_test.py:80-99"
    },
    "555": {
        "file_id": 23,
        "content": "Code snippet contains two functions: 'i2v_test.py':79-98 is a method that replaces specific characters in the prompt string, limits its length to 40 characters, saves the video using saved models, and prints the saving time. 'download_model' function downloads a model checkpoint from Hugging Face Hub into the specified directory if it doesn't already exist.",
        "type": "comment"
    },
    "556": {
        "file_id": 23,
        "content": "if __name__ == '__main__':\n    i2v = Image2Video()\n    video_path = i2v.get_image('prompts/art.png','man fishing in a boat at sunset')\n    print('done', video_path)",
        "type": "code",
        "location": "/scripts/gradio/i2v_test.py:101-104"
    },
    "557": {
        "file_id": 23,
        "content": "This code checks if the script is being run directly, then creates an Image2Video object and uses it to generate a video from an image prompt. The path of the generated video is printed as 'done' followed by the video path.",
        "type": "comment"
    },
    "558": {
        "file_id": 24,
        "content": "/scripts/run.sh",
        "type": "filepath"
    },
    "559": {
        "file_id": 24,
        "content": "This script runs an inference process using Dynamic Rafter's model with specific configurations and saves the results. It sets the seed, ckpt_path, config path, savedir, n_samples, bs, height, width, unconditional_guidance_scale, ddim_steps, ddim_eta, prompt_dir, text_input, video_length, and frame_stride parameters. The script also has an optional argument to use multiple conditional CFGs.",
        "type": "summary"
    },
    "560": {
        "file_id": 24,
        "content": "name=\"dynamicrafter_256\"\nckpt='checkpoints/dynamicrafter_256_v1/model.ckpt'\nconfig='configs/inference_256_v1.0.yaml'\nprompt_dir=\"prompts/\"\nres_dir=\"results\"\nCUDA_VISIBLE_DEVICES=0 python3 scripts/evaluation/inference.py \\\n--seed 123 \\\n--ckpt_path $ckpt \\\n--config $config \\\n--savedir $res_dir/$name \\\n--n_samples 1 \\\n--bs 1 --height 256 --width 256 \\\n--unconditional_guidance_scale 7.5 \\\n--ddim_steps 50 \\\n--ddim_eta 1.0 \\\n--prompt_dir $prompt_dir \\\n--text_input \\\n--video_length 16 \\\n--frame_stride 3\n## multi-cond CFG: the <unconditional_guidance_scale> is s_txt, <cfg_img> is s_img\n#--multiple_cond_cfg --cfg_img 7.5",
        "type": "code",
        "location": "/scripts/run.sh:1-25"
    },
    "561": {
        "file_id": 24,
        "content": "This script runs an inference process using Dynamic Rafter's model with specific configurations and saves the results. It sets the seed, ckpt_path, config path, savedir, n_samples, bs, height, width, unconditional_guidance_scale, ddim_steps, ddim_eta, prompt_dir, text_input, video_length, and frame_stride parameters. The script also has an optional argument to use multiple conditional CFGs.",
        "type": "comment"
    },
    "562": {
        "file_id": 25,
        "content": "/scripts/run_mp.sh",
        "type": "filepath"
    },
    "563": {
        "file_id": 25,
        "content": "The script performs inference on a DynamiCrafter model using single or multi-GPUs, executing with specific parameters and utilizing the torch.distributed.launch module for multi-GPU execution. It also sets options like prompt directory, text input mode, video length, and frame stride via command line.",
        "type": "summary"
    },
    "564": {
        "file_id": 25,
        "content": "name=\"dynamicrafter_256\"\nckpt='checkpoints/dynamicrafter_256_v1/model.ckpt'\nconfig='configs/inference_256_v1.0.yaml'\nprompt_dir=\"prompts/\"\nres_dir=\"results\"\n## inference using a single GPU\n# CUDA_VISIBLE_DEVICES=0 python3 scripts/evaluation/inference.py \\\n# --seed 123 \\\n# --ckpt_path $ckpt \\\n# --config $config \\\n# --savedir $res_dir/$name \\\n# --n_samples 1 \\\n# --bs 1 --height 256 --width 256 \\\n# --unconditional_guidance_scale 7.5 \\\n# --ddim_steps 50 \\\n# --ddim_eta 1.0 \\\n# --prompt_dir $prompt_dir \\\n# --text_input \\\n# --video_length 16 \\\n# --frame_stride 3\n## inference using single node with multi-GPUs:\nCUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 python3 -m torch.distributed.launch \\\n--nproc_per_node=8 --nnodes=1 --master_addr=127.0.0.1 --master_port=23456 --node_rank=0 \\\nscripts/evaluation/ddp_wrapper.py \\\n--module 'inference' \\\n--seed 123 \\\n--ckpt_path $ckpt \\\n--config $config \\\n--savedir $res_dir/$name \\\n--n_samples 1 \\\n--bs 1 --height 256 --width 256 \\\n--unconditional_guidance_scale 7.5 \\\n--ddim_steps 50 \\\n--ddim_eta 1.0 \\",
        "type": "code",
        "location": "/scripts/run_mp.sh:1-38"
    },
    "565": {
        "file_id": 25,
        "content": "This script performs inference using a single GPU and multi-GPUs on a DynamiCrafter model. It uses the inference.py file with specific parameters to execute the process. The torch.distributed.launch module is utilized for multi-GPU execution, specifying the number of processes per node and total nodes involved.",
        "type": "comment"
    },
    "566": {
        "file_id": 25,
        "content": "--prompt_dir $prompt_dir \\\n--text_input \\\n--video_length 16 \\\n--frame_stride 3",
        "type": "code",
        "location": "/scripts/run_mp.sh:39-42"
    },
    "567": {
        "file_id": 25,
        "content": "The code snippet specifies command-line options for the DynamiCrafter script. It sets the prompt directory, enables text input mode, sets video length to 16 frames and frame stride to 3.",
        "type": "comment"
    },
    "568": {
        "file_id": 26,
        "content": "/utils/utils.py",
        "type": "filepath"
    },
    "569": {
        "file_id": 26,
        "content": "The code provides functions for parameter counting, matching, and object instantiation from config strings. It imports libraries and defines utility functions like image resizing and process setup, using \"env://\" initialization and NCCL communication backend for distributed training.",
        "type": "summary"
    },
    "570": {
        "file_id": 26,
        "content": "import importlib\nimport numpy as np\nimport cv2\nimport torch\nimport torch.distributed as dist\ndef count_params(model, verbose=False):\n    total_params = sum(p.numel() for p in model.parameters())\n    if verbose:\n        print(f\"{model.__class__.__name__} has {total_params*1.e-6:.2f} M params.\")\n    return total_params\ndef check_istarget(name, para_list):\n    \"\"\" \n    name: full name of source para\n    para_list: partial name of target para \n    \"\"\"\n    istarget=False\n    for para in para_list:\n        if para in name:\n            return True\n    return istarget\ndef instantiate_from_config(config):\n    if not \"target\" in config:\n        if config == '__is_first_stage__':\n            return None\n        elif config == \"__is_unconditional__\":\n            return None\n        raise KeyError(\"Expected key `target` to instantiate.\")\n    return get_obj_from_str(config[\"target\"])(**config.get(\"params\", dict()))\ndef get_obj_from_str(string, reload=False):\n    module, cls = string.rsplit(\".\", 1)\n    if reload:\n        module_imp = importlib.import_module(module)",
        "type": "code",
        "location": "/utils/utils.py:1-40"
    },
    "571": {
        "file_id": 26,
        "content": "The code includes functions for counting the number of parameters in a model, checking if a parameter matches another partial name, and instantiating an object from a config string. It also imports several libraries such as numpy, cv2, torch, and torch.distributed, and defines a function to import a module and class specified by a string.",
        "type": "comment"
    },
    "572": {
        "file_id": 26,
        "content": "        importlib.reload(module_imp)\n    return getattr(importlib.import_module(module, package=None), cls)\ndef load_npz_from_dir(data_dir):\n    data = [np.load(os.path.join(data_dir, data_name))['arr_0'] for data_name in os.listdir(data_dir)]\n    data = np.concatenate(data, axis=0)\n    return data\ndef load_npz_from_paths(data_paths):\n    data = [np.load(data_path)['arr_0'] for data_path in data_paths]\n    data = np.concatenate(data, axis=0)\n    return data   \ndef resize_numpy_image(image, max_resolution=512 * 512, resize_short_edge=None):\n    h, w = image.shape[:2]\n    if resize_short_edge is not None:\n        k = resize_short_edge / min(h, w)\n    else:\n        k = max_resolution / (h * w)\n        k = k**0.5\n    h = int(np.round(h * k / 64)) * 64\n    w = int(np.round(w * k / 64)) * 64\n    image = cv2.resize(image, (w, h), interpolation=cv2.INTER_LANCZOS4)\n    return image\ndef setup_dist(args):\n    if dist.is_initialized():\n        return\n    torch.cuda.set_device(args.local_rank)\n    torch.distributed.init_process_group(",
        "type": "code",
        "location": "/utils/utils.py:41-74"
    },
    "573": {
        "file_id": 26,
        "content": "This code contains several utility functions. \"load_npz_from_dir\" and \"load_npz_from_paths\" both load numpy files from directories or paths, respectively, and concatenate the data along axis 0. \"resize_numpy_image\" resizes a numpy image based on either user-specified maximum resolution or by preserving aspect ratio while keeping short edge within specified size. Lastly, \"setup_dist\" initializes distributed process group if not already initialized.",
        "type": "comment"
    },
    "574": {
        "file_id": 26,
        "content": "        'nccl',\n        init_method='env://'\n    )",
        "type": "code",
        "location": "/utils/utils.py:75-77"
    },
    "575": {
        "file_id": 26,
        "content": "This code sets the initialization method to \"env://\" and uses NCCL as the communication backend for distributed training.",
        "type": "comment"
    }
}