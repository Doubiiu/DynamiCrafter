{
    "100": {
        "file_id": 8,
        "content": "        self.model = DiffusionWrapper(unet_config, conditioning_key)\n        #count_params(self.model, verbose=True)\n        self.use_ema = use_ema\n        if self.use_ema:\n            self.model_ema = LitEma(self.model)\n            mainlogger.info(f\"Keeping EMAs of {len(list(self.model_ema.buffers()))}.\")\n        self.use_scheduler = scheduler_config is not None\n        if self.use_scheduler:\n            self.scheduler_config = scheduler_config\n        self.v_posterior = v_posterior\n        self.original_elbo_weight = original_elbo_weight\n        self.l_simple_weight = l_simple_weight\n        if monitor is not None:\n            self.monitor = monitor\n        if ckpt_path is not None:\n            self.init_from_ckpt(ckpt_path, ignore_keys=ignore_keys, only_model=load_only_unet)\n        self.register_schedule(given_betas=given_betas, beta_schedule=beta_schedule, timesteps=timesteps,\n                               linear_start=linear_start, linear_end=linear_end, cosine_s=cosine_s)\n        self.loss_type = loss_type",
        "type": "code",
        "location": "/lvdm/models/ddpm3d.py:81-104"
    },
    "101": {
        "file_id": 8,
        "content": "The code initializes a DDPM3d model with DiffusionWrapper, optionally an EMA copy of the model, and handles scheduler and checkpoint initialization. It also registers a schedule for betas, timesteps, and loss type.",
        "type": "comment"
    },
    "102": {
        "file_id": 8,
        "content": "        self.learn_logvar = learn_logvar\n        self.logvar = torch.full(fill_value=logvar_init, size=(self.num_timesteps,))\n        if self.learn_logvar:\n            self.logvar = nn.Parameter(self.logvar, requires_grad=True)\n    def register_schedule(self, given_betas=None, beta_schedule=\"linear\", timesteps=1000,\n                          linear_start=1e-4, linear_end=2e-2, cosine_s=8e-3):\n        if exists(given_betas):\n            betas = given_betas\n        else:\n            betas = make_beta_schedule(beta_schedule, timesteps, linear_start=linear_start, linear_end=linear_end,\n                                       cosine_s=cosine_s)\n        alphas = 1. - betas\n        alphas_cumprod = np.cumprod(alphas, axis=0)\n        alphas_cumprod_prev = np.append(1., alphas_cumprod[:-1])\n        timesteps, = betas.shape\n        self.num_timesteps = int(timesteps)\n        self.linear_start = linear_start\n        self.linear_end = linear_end\n        assert alphas_cumprod.shape[0] == self.num_timesteps, 'alphas have to be defined for each timestep'",
        "type": "code",
        "location": "/lvdm/models/ddpm3d.py:106-126"
    },
    "103": {
        "file_id": 8,
        "content": "The code initializes a DDPM model and registers a schedule for the given or computed betas using linear or cosine scheduling. The model's logvar is set based on the learn_logvar parameter, and the timesteps, linear_start, linear_end, and cosine_s values are registered to be used in the model.",
        "type": "comment"
    },
    "104": {
        "file_id": 8,
        "content": "        to_torch = partial(torch.tensor, dtype=torch.float32)\n        self.register_buffer('betas', to_torch(betas))\n        self.register_buffer('alphas_cumprod', to_torch(alphas_cumprod))\n        self.register_buffer('alphas_cumprod_prev', to_torch(alphas_cumprod_prev))\n        # calculations for diffusion q(x_t | x_{t-1}) and others\n        self.register_buffer('sqrt_alphas_cumprod', to_torch(np.sqrt(alphas_cumprod)))\n        self.register_buffer('sqrt_one_minus_alphas_cumprod', to_torch(np.sqrt(1. - alphas_cumprod)))\n        self.register_buffer('log_one_minus_alphas_cumprod', to_torch(np.log(1. - alphas_cumprod)))\n        self.register_buffer('sqrt_recip_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod)))\n        self.register_buffer('sqrt_recipm1_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod - 1)))\n        # calculations for posterior q(x_{t-1} | x_t, x_0)\n        posterior_variance = (1 - self.v_posterior) * betas * (1. - alphas_cumprod_prev) / (\n                    1. - alphas_cumprod) + self.v_posterior * betas",
        "type": "code",
        "location": "/lvdm/models/ddpm3d.py:128-143"
    },
    "105": {
        "file_id": 8,
        "content": "This code initializes buffer attributes and performs calculations for diffusion models, including the variance of the posterior q(x_{t-1} | x_t, x_0). The operations include sqrt, log, and reciprocal functions. These steps are used to create a Diffusion model for data analysis.",
        "type": "comment"
    },
    "106": {
        "file_id": 8,
        "content": "        # above: equal to 1. / (1. / (1. - alpha_cumprod_tm1) + alpha_t / beta_t)\n        self.register_buffer('posterior_variance', to_torch(posterior_variance))\n        # below: log calculation clipped because the posterior variance is 0 at the beginning of the diffusion chain\n        self.register_buffer('posterior_log_variance_clipped', to_torch(np.log(np.maximum(posterior_variance, 1e-20))))\n        self.register_buffer('posterior_mean_coef1', to_torch(\n            betas * np.sqrt(alphas_cumprod_prev) / (1. - alphas_cumprod)))\n        self.register_buffer('posterior_mean_coef2', to_torch(\n            (1. - alphas_cumprod_prev) * np.sqrt(alphas) / (1. - alphas_cumprod)))\n        if self.parameterization == \"eps\":\n            lvlb_weights = self.betas ** 2 / (\n                        2 * self.posterior_variance * to_torch(alphas) * (1 - self.alphas_cumprod))\n        elif self.parameterization == \"x0\":\n            lvlb_weights = 0.5 * np.sqrt(torch.Tensor(alphas_cumprod)) / (2. * 1 - torch.Tensor(alphas_cumprod))",
        "type": "code",
        "location": "/lvdm/models/ddpm3d.py:144-157"
    },
    "107": {
        "file_id": 8,
        "content": "This code defines and registers buffers for posterior variance, log of clipped posterior variance, and two mean coefficients. It then assigns weights based on the parameterization setting (either \"eps\" or \"x0\"). This appears to be part of a larger model used in a diffusion process.",
        "type": "comment"
    },
    "108": {
        "file_id": 8,
        "content": "        elif self.parameterization == \"v\":\n            lvlb_weights = torch.ones_like(self.betas ** 2 / (\n                    2 * self.posterior_variance * to_torch(alphas) * (1 - self.alphas_cumprod)))\n        else:\n            raise NotImplementedError(\"mu not supported\")\n        # TODO how to choose this term\n        lvlb_weights[0] = lvlb_weights[1]\n        self.register_buffer('lvlb_weights', lvlb_weights, persistent=False)\n        assert not torch.isnan(self.lvlb_weights).all()\n    @contextmanager\n    def ema_scope(self, context=None):\n        if self.use_ema:\n            self.model_ema.store(self.model.parameters())\n            self.model_ema.copy_to(self.model)\n            if context is not None:\n                mainlogger.info(f\"{context}: Switched to EMA weights\")\n        try:\n            yield None\n        finally:\n            if self.use_ema:\n                self.model_ema.restore(self.model.parameters())\n                if context is not None:\n                    mainlogger.info(f\"{context}: Restored training weights\")",
        "type": "code",
        "location": "/lvdm/models/ddpm3d.py:158-181"
    },
    "109": {
        "file_id": 8,
        "content": "This code defines a function that applies exponential moving average (EMA) to model parameters. If the parameterization is \"v\", it calculates lvlb_weights based on betas and alphas. It then registers the buffer 'lvlb_weights' for later use. The function also provides a context manager, ema_scope(), to switch between training weights (EMA) and current model weights.",
        "type": "comment"
    },
    "110": {
        "file_id": 8,
        "content": "    def init_from_ckpt(self, path, ignore_keys=list(), only_model=False):\n        sd = torch.load(path, map_location=\"cpu\")\n        if \"state_dict\" in list(sd.keys()):\n            sd = sd[\"state_dict\"]\n        keys = list(sd.keys())\n        for k in keys:\n            for ik in ignore_keys:\n                if k.startswith(ik):\n                    mainlogger.info(\"Deleting key {} from state_dict.\".format(k))\n                    del sd[k]\n        missing, unexpected = self.load_state_dict(sd, strict=False) if not only_model else self.model.load_state_dict(\n            sd, strict=False)\n        mainlogger.info(f\"Restored from {path} with {len(missing)} missing and {len(unexpected)} unexpected keys\")\n        if len(missing) > 0:\n            mainlogger.info(f\"Missing Keys: {missing}\")\n        if len(unexpected) > 0:\n            mainlogger.info(f\"Unexpected Keys: {unexpected}\")\n    def q_mean_variance(self, x_start, t):\n        \"\"\"\n        Get the distribution q(x_t | x_0).\n        :param x_start: the [N x C x ...] tensor of noiseless inputs.",
        "type": "code",
        "location": "/lvdm/models/ddpm3d.py:183-204"
    },
    "111": {
        "file_id": 8,
        "content": "The function `init_from_ckpt` loads a model from a checkpoint file path, deleting keys that start with specific ignore keys. It also returns the number of missing and unexpected keys. The `q_mean_variance` function computes the distribution q(x_t | x_0) for a given input tensor.",
        "type": "comment"
    },
    "112": {
        "file_id": 8,
        "content": "        :param t: the number of diffusion steps (minus 1). Here, 0 means one step.\n        :return: A tuple (mean, variance, log_variance), all of x_start's shape.\n        \"\"\"\n        mean = (extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start)\n        variance = extract_into_tensor(1.0 - self.alphas_cumprod, t, x_start.shape)\n        log_variance = extract_into_tensor(self.log_one_minus_alphas_cumprod, t, x_start.shape)\n        return mean, variance, log_variance\n    def predict_start_from_noise(self, x_t, t, noise):\n        return (\n                extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t -\n                extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise\n        )\n    def predict_start_from_z_and_v(self, x_t, t, v):\n        # self.register_buffer('sqrt_alphas_cumprod', to_torch(np.sqrt(alphas_cumprod)))\n        # self.register_buffer('sqrt_one_minus_alphas_cumprod', to_torch(np.sqrt(1. - alphas_cumprod)))\n        return (",
        "type": "code",
        "location": "/lvdm/models/ddpm3d.py:205-222"
    },
    "113": {
        "file_id": 8,
        "content": "The code defines a class with three methods: \"predict_start_from_noise\", \"predict_start_from_z_and_v\", and \"predict_start\". These methods are used for predicting the start values (x_start) of a diffusion process given different inputs such as noise, v, or z. The \"predict_start\" method takes t parameter which represents the number of diffusion steps. It returns mean, variance, and log_variance of x_start's shape. The \"predict_start_from_noise\" method uses sqrt_recip_alphas_cumprod and sqrt_recipm1_alphas_cumprod to predict the start value from noise. The \"predict_start_from_z_and_v\" method is used for predicting the start values from z and v.",
        "type": "comment"
    },
    "114": {
        "file_id": 8,
        "content": "                extract_into_tensor(self.sqrt_alphas_cumprod, t, x_t.shape) * x_t -\n                extract_into_tensor(self.sqrt_one_minus_alphas_cumprod, t, x_t.shape) * v\n        )\n    def predict_eps_from_z_and_v(self, x_t, t, v):\n        return (\n                extract_into_tensor(self.sqrt_alphas_cumprod, t, x_t.shape) * v +\n                extract_into_tensor(self.sqrt_one_minus_alphas_cumprod, t, x_t.shape) * x_t\n        )\n    def q_posterior(self, x_start, x_t, t):\n        posterior_mean = (\n                extract_into_tensor(self.posterior_mean_coef1, t, x_t.shape) * x_start +\n                extract_into_tensor(self.posterior_mean_coef2, t, x_t.shape) * x_t\n        )\n        posterior_variance = extract_into_tensor(self.posterior_variance, t, x_t.shape)\n        posterior_log_variance_clipped = extract_into_tensor(self.posterior_log_variance_clipped, t, x_t.shape)\n        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n    def p_mean_variance(self, x, t, clip_denoised: bool):",
        "type": "code",
        "location": "/lvdm/models/ddpm3d.py:223-242"
    },
    "115": {
        "file_id": 8,
        "content": "This code defines several methods within a class. The `predict_eps_from_z_and_v` method predicts the epsilon from given z and v. The `q_posterior` method calculates the posterior mean, variance, and log variance clipped for a given x_start, x_t, and t. Finally, the `p_mean_variance` method calculates the mean and variance for a given x, t, and clip_denoised boolean.",
        "type": "comment"
    },
    "116": {
        "file_id": 8,
        "content": "        model_out = self.model(x, t)\n        if self.parameterization == \"eps\":\n            x_recon = self.predict_start_from_noise(x, t=t, noise=model_out)\n        elif self.parameterization == \"x0\":\n            x_recon = model_out\n        if clip_denoised:\n            x_recon.clamp_(-1., 1.)\n        model_mean, posterior_variance, posterior_log_variance = self.q_posterior(x_start=x_recon, x_t=x, t=t)\n        return model_mean, posterior_variance, posterior_log_variance\n    @torch.no_grad()\n    def p_sample(self, x, t, clip_denoised=True, repeat_noise=False):\n        b, *_, device = *x.shape, x.device\n        model_mean, _, model_log_variance = self.p_mean_variance(x=x, t=t, clip_denoised=clip_denoised)\n        noise = noise_like(x.shape, device, repeat_noise)\n        # no noise when t == 0\n        nonzero_mask = (1 - (t == 0).float()).reshape(b, *((1,) * (len(x.shape) - 1)))\n        return model_mean + nonzero_mask * (0.5 * model_log_variance).exp() * noise\n    @torch.no_grad()\n    def p_sample_loop(self, shape, return_intermediates=False):",
        "type": "code",
        "location": "/lvdm/models/ddpm3d.py:243-264"
    },
    "117": {
        "file_id": 8,
        "content": "The model calculates the mean, variance, and log variance for the prior distribution using `p_mean_variance` function. It then generates noise and applies a mask to calculate the final sample using `p_sample`. The `p_sample_loop` function is an optimized version of `p_sample` with the option to return intermediate results.",
        "type": "comment"
    },
    "118": {
        "file_id": 8,
        "content": "        device = self.betas.device\n        b = shape[0]\n        img = torch.randn(shape, device=device)\n        intermediates = [img]\n        for i in tqdm(reversed(range(0, self.num_timesteps)), desc='Sampling t', total=self.num_timesteps):\n            img = self.p_sample(img, torch.full((b,), i, device=device, dtype=torch.long),\n                                clip_denoised=self.clip_denoised)\n            if i % self.log_every_t == 0 or i == self.num_timesteps - 1:\n                intermediates.append(img)\n        if return_intermediates:\n            return img, intermediates\n        return img\n    @torch.no_grad()\n    def sample(self, batch_size=16, return_intermediates=False):\n        image_size = self.image_size\n        channels = self.channels\n        return self.p_sample_loop((batch_size, channels, image_size, image_size),\n                                  return_intermediates=return_intermediates)\n    def q_sample(self, x_start, t, noise=None):\n        noise = default(noise, lambda: torch.randn_like(x_start))",
        "type": "code",
        "location": "/lvdm/models/ddpm3d.py:265-286"
    },
    "119": {
        "file_id": 8,
        "content": "This code defines a model that samples images from a diffusion process, with the ability to return intermediate frames if desired. It uses PyTorch and has a `sample` method for generating images and a `q_sample` method for querying sample states at specific timesteps. The code also includes a `p_sample_loop` function which iterates over timesteps in reverse order to generate the image frames.",
        "type": "comment"
    },
    "120": {
        "file_id": 8,
        "content": "        return (extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start +\n                extract_into_tensor(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise)\n    def get_v(self, x, noise, t):\n        return (\n                extract_into_tensor(self.sqrt_alphas_cumprod, t, x.shape) * noise -\n                extract_into_tensor(self.sqrt_one_minus_alphas_cumprod, t, x.shape) * x\n        )\n    def get_input(self, batch, k):\n        x = batch[k]\n        x = x.to(memory_format=torch.contiguous_format).float()\n        return x\n    def _get_rows_from_list(self, samples):\n        n_imgs_per_row = len(samples)\n        denoise_grid = rearrange(samples, 'n b c h w -> b n c h w')\n        denoise_grid = rearrange(denoise_grid, 'b n c h w -> (b n) c h w')\n        denoise_grid = make_grid(denoise_grid, nrow=n_imgs_per_row)\n        return denoise_grid\n    @torch.no_grad()\n    def log_images(self, batch, N=8, n_row=2, sample=True, return_keys=None, **kwargs):\n        log = dict()\n        x = self.get_input(batch, self.first_stage_key)",
        "type": "code",
        "location": "/lvdm/models/ddpm3d.py:287-311"
    },
    "121": {
        "file_id": 8,
        "content": "The code defines various methods for handling and manipulating images. The `get_v` method calculates a value used in denoising, the `get_input` method prepares input data, `_get_rows_from_list` rearranges samples into grids with specified number of rows, and `log_images` logs images from batch data.",
        "type": "comment"
    },
    "122": {
        "file_id": 8,
        "content": "        N = min(x.shape[0], N)\n        n_row = min(x.shape[0], n_row)\n        x = x.to(self.device)[:N]\n        log[\"inputs\"] = x\n        # get diffusion row\n        diffusion_row = list()\n        x_start = x[:n_row]\n        for t in range(self.num_timesteps):\n            if t % self.log_every_t == 0 or t == self.num_timesteps - 1:\n                t = repeat(torch.tensor([t]), '1 -> b', b=n_row)\n                t = t.to(self.device).long()\n                noise = torch.randn_like(x_start)\n                x_noisy = self.q_sample(x_start=x_start, t=t, noise=noise)\n                diffusion_row.append(x_noisy)\n        log[\"diffusion_row\"] = self._get_rows_from_list(diffusion_row)\n        if sample:\n            # get denoise row\n            with self.ema_scope(\"Plotting\"):\n                samples, denoise_row = self.sample(batch_size=N, return_intermediates=True)\n            log[\"samples\"] = samples\n            log[\"denoise_row\"] = self._get_rows_from_list(denoise_row)\n        if return_keys:\n            if np.intersect1d(list(log.keys()), return_keys).shape[0] == 0:",
        "type": "code",
        "location": "/lvdm/models/ddpm3d.py:312-340"
    },
    "123": {
        "file_id": 8,
        "content": "This code performs diffusion modeling and denoising. It first checks the number of input samples and rows, then iterates through each timestep to generate noisy versions of the inputs. If sampling is enabled, it also generates denoised rows. The generated results are stored in a log dictionary for further use.",
        "type": "comment"
    },
    "124": {
        "file_id": 8,
        "content": "                return log\n            else:\n                return {key: log[key] for key in return_keys}\n        return log\nclass LatentDiffusion(DDPM):\n    \"\"\"main class\"\"\"\n    def __init__(self,\n                 first_stage_config,\n                 cond_stage_config,\n                 num_timesteps_cond=None,\n                 cond_stage_key=\"caption\",\n                 cond_stage_trainable=False,\n                 cond_stage_forward=None,\n                 conditioning_key=None,\n                 uncond_prob=0.2,\n                 uncond_type=\"empty_seq\",\n                 scale_factor=1.0,\n                 scale_by_std=False,\n                 encoder_type=\"2d\",\n                 only_model=False,\n                 noise_strength=0,\n                 use_dynamic_rescale=False,\n                 base_scale=0.7,\n                 turning_step=400,\n                 loop_video=False,\n                 *args, **kwargs):\n        self.num_timesteps_cond = default(num_timesteps_cond, 1)\n        self.scale_by_std = scale_by_std\n        assert self.num_timesteps_cond <= kwargs['timesteps']",
        "type": "code",
        "location": "/lvdm/models/ddpm3d.py:341-371"
    },
    "125": {
        "file_id": 8,
        "content": "This code defines the LatentDiffusion class that extends the DDPM class and takes in various configuration parameters for different stages of the model. The LatentDiffusion class has methods to train and save the model, as well as initialize the internal components based on the given configurations.",
        "type": "comment"
    },
    "126": {
        "file_id": 8,
        "content": "        # for backwards compatibility after implementation of DiffusionWrapper\n        ckpt_path = kwargs.pop(\"ckpt_path\", None)\n        ignore_keys = kwargs.pop(\"ignore_keys\", [])\n        conditioning_key = default(conditioning_key, 'crossattn')\n        super().__init__(conditioning_key=conditioning_key, *args, **kwargs)\n        self.cond_stage_trainable = cond_stage_trainable\n        self.cond_stage_key = cond_stage_key\n        self.noise_strength = noise_strength\n        self.use_dynamic_rescale = use_dynamic_rescale\n        self.loop_video = loop_video\n        try:\n            self.num_downs = len(first_stage_config.params.ddconfig.ch_mult) - 1\n        except:\n            self.num_downs = 0\n        if not scale_by_std:\n            self.scale_factor = scale_factor\n        else:\n            self.register_buffer('scale_factor', torch.tensor(scale_factor))\n        if use_dynamic_rescale:\n            scale_arr1 = np.linspace(1.0, base_scale, turning_step)\n            scale_arr2 = np.full(self.num_timesteps, base_scale)",
        "type": "code",
        "location": "/lvdm/models/ddpm3d.py:372-394"
    },
    "127": {
        "file_id": 8,
        "content": "Code initializes DDPM3D model with optional parameters such as ckpt_path, ignore_keys, conditioning_key, and more. It also determines the number of downsampling stages based on config and sets scale_factor if not using dynamic rescale.",
        "type": "comment"
    },
    "128": {
        "file_id": 8,
        "content": "            scale_arr = np.concatenate((scale_arr1, scale_arr2))\n            to_torch = partial(torch.tensor, dtype=torch.float32)\n            self.register_buffer('scale_arr', to_torch(scale_arr))\n        self.instantiate_first_stage(first_stage_config)\n        self.instantiate_cond_stage(cond_stage_config)\n        self.first_stage_config = first_stage_config\n        self.cond_stage_config = cond_stage_config        \n        self.clip_denoised = False\n        self.cond_stage_forward = cond_stage_forward\n        self.encoder_type = encoder_type\n        assert(encoder_type in [\"2d\", \"3d\"])\n        self.uncond_prob = uncond_prob\n        self.classifier_free_guidance = True if uncond_prob > 0 else False\n        assert(uncond_type in [\"zero_embed\", \"empty_seq\"])\n        self.uncond_type = uncond_type\n        self.restarted_from_ckpt = False\n        if ckpt_path is not None:\n            self.init_from_ckpt(ckpt_path, ignore_keys, only_model=only_model)\n            self.restarted_from_ckpt = True\n    def make_cond_schedule(self, ):",
        "type": "code",
        "location": "/lvdm/models/ddpm3d.py:395-419"
    },
    "129": {
        "file_id": 8,
        "content": "Instantiate first and conditional stages, set configuration parameters, and initialize buffers. Verify encoder type and unconditional type. Allow restarting from checkpoint if provided path is not None.",
        "type": "comment"
    },
    "130": {
        "file_id": 8,
        "content": "        self.cond_ids = torch.full(size=(self.num_timesteps,), fill_value=self.num_timesteps - 1, dtype=torch.long)\n        ids = torch.round(torch.linspace(0, self.num_timesteps - 1, self.num_timesteps_cond)).long()\n        self.cond_ids[:self.num_timesteps_cond] = ids\n    def instantiate_first_stage(self, config):\n        model = instantiate_from_config(config)\n        self.first_stage_model = model.eval()\n        self.first_stage_model.train = disabled_train\n        for param in self.first_stage_model.parameters():\n            param.requires_grad = False\n    def instantiate_cond_stage(self, config):\n        if not self.cond_stage_trainable:\n            model = instantiate_from_config(config)\n            self.cond_stage_model = model.eval()\n            self.cond_stage_model.train = disabled_train\n            for param in self.cond_stage_model.parameters():\n                param.requires_grad = False\n        else:\n            model = instantiate_from_config(config)\n            self.cond_stage_model = model",
        "type": "code",
        "location": "/lvdm/models/ddpm3d.py:420-440"
    },
    "131": {
        "file_id": 8,
        "content": "This code initializes the conditional stages of a diffusion model. It assigns the conditioning IDs for timesteps, instantiates the first stage model with disabled gradients and fixed parameters, and then conditionally instantiates the second (conditional) stage model either with a fixed or trainable configuration depending on the `cond_stage_trainable` flag. The fixed models are used for evaluation, while the trainable one is for training.",
        "type": "comment"
    },
    "132": {
        "file_id": 8,
        "content": "    def get_learned_conditioning(self, c):\n        if self.cond_stage_forward is None:\n            if hasattr(self.cond_stage_model, 'encode') and callable(self.cond_stage_model.encode):\n                c = self.cond_stage_model.encode(c)\n                if isinstance(c, DiagonalGaussianDistribution):\n                    c = c.mode()\n            else:\n                c = self.cond_stage_model(c)\n        else:\n            assert hasattr(self.cond_stage_model, self.cond_stage_forward)\n            c = getattr(self.cond_stage_model, self.cond_stage_forward)(c)\n        return c\n    def get_first_stage_encoding(self, encoder_posterior, noise=None):\n        if isinstance(encoder_posterior, DiagonalGaussianDistribution):\n            z = encoder_posterior.sample(noise=noise)\n        elif isinstance(encoder_posterior, torch.Tensor):\n            z = encoder_posterior\n        else:\n            raise NotImplementedError(f\"encoder_posterior of type '{type(encoder_posterior)}' not yet implemented\")\n        return self.scale_factor * z",
        "type": "code",
        "location": "/lvdm/models/ddpm3d.py:442-462"
    },
    "133": {
        "file_id": 8,
        "content": "The code defines two methods: 'get_learned_conditioning' and 'get_first_stage_encoding'. The first method retrieves the learned conditioning from a model, possibly encoding it if the model has an 'encode' function. It also handles the case where the model does not have the necessary functions and returns the input unchanged or applies the model directly to the input. The second method takes an encoder_posterior (encoder posterior) as input and returns the first stage encoding, handling DiagonalGaussianDistribution, torch.Tensor, and other types of encoder_posterior inputs appropriately.",
        "type": "comment"
    },
    "134": {
        "file_id": 8,
        "content": "    @torch.no_grad()\n    def encode_first_stage(self, x):\n        if self.encoder_type == \"2d\" and x.dim() == 5:\n            b, _, t, _, _ = x.shape\n            x = rearrange(x, 'b c t h w -> (b t) c h w')\n            reshape_back = True\n        else:\n            reshape_back = False\n        encoder_posterior = self.first_stage_model.encode(x)\n        results = self.get_first_stage_encoding(encoder_posterior).detach()\n        if reshape_back:\n            results = rearrange(results, '(b t) c h w -> b c t h w', b=b,t=t)\n        return results\n    def decode_core(self, z, **kwargs):\n        if self.encoder_type == \"2d\" and z.dim() == 5:\n            b, _, t, _, _ = z.shape\n            z = rearrange(z, 'b c t h w -> (b t) c h w')\n            reshape_back = True\n        else:\n            reshape_back = False\n        z = 1. / self.scale_factor * z\n        results = self.first_stage_model.decode(z, **kwargs)\n        if reshape_back:\n            results = rearrange(results, '(b t) c h w -> b c t h w', b=b,t=t)\n        return results",
        "type": "code",
        "location": "/lvdm/models/ddpm3d.py:464-495"
    },
    "135": {
        "file_id": 8,
        "content": "This code defines two functions: `encode_first_stage` and `decode_core`. The first function encodes an input tensor using a first-stage model and may perform reshaping operations based on the tensor's shape. The second function decodes a tensor using the same first-stage model, also potentially reshaping it before or after the decoding process. Both functions consider the tensor's dimensionality and apply reshaping if needed.",
        "type": "comment"
    },
    "136": {
        "file_id": 8,
        "content": "    @torch.no_grad()\n    def decode_first_stage(self, z, **kwargs):\n        return self.decode_core(z, **kwargs)\n    # same as above but without decorator\n    def differentiable_decode_first_stage(self, z, **kwargs):\n        return self.decode_core(z, **kwargs)\n    def forward(self, x, c, **kwargs):\n        t = torch.randint(0, self.num_timesteps, (x.shape[0],), device=self.device).long()\n        if self.use_dynamic_rescale:\n            x = x * extract_into_tensor(self.scale_arr, t, x.shape)\n        return self.p_losses(x, c, t, **kwargs)\n    def apply_model(self, x_noisy, t, cond, **kwargs):\n        if isinstance(cond, dict):\n            # hybrid case, cond is exptected to be a dict\n            pass\n        else:\n            if not isinstance(cond, list):\n                cond = [cond]\n            key = 'c_concat' if self.model.conditioning_key == 'concat' else 'c_crossattn'\n            cond = {key: cond}\n        x_recon = self.model(x_noisy, t, **cond, **kwargs)\n        if isinstance(x_recon, tuple):\n            return x_recon[0]",
        "type": "code",
        "location": "/lvdm/models/ddpm3d.py:497-524"
    },
    "137": {
        "file_id": 8,
        "content": "The code defines a function that takes in noisy input (x_noisy) and a timestep (t), as well as some conditioning information (cond). It applies the model to generate reconstructed output (x_recon). If cond is a dictionary, it does something specific. The model returns either a single output or a tuple of outputs.",
        "type": "comment"
    },
    "138": {
        "file_id": 8,
        "content": "        else:\n            return x_recon\n    def _get_denoise_row_from_list(self, samples, desc=''):\n        denoise_row = []\n        for zd in tqdm(samples, desc=desc):\n            denoise_row.append(self.decode_first_stage(zd.to(self.device)))\n        n_log_timesteps = len(denoise_row)\n        denoise_row = torch.stack(denoise_row)  # n_log_timesteps, b, C, H, W\n        if denoise_row.dim() == 5:\n            denoise_grid = rearrange(denoise_row, 'n b c h w -> b n c h w')\n            denoise_grid = rearrange(denoise_grid, 'b n c h w -> (b n) c h w')\n            denoise_grid = make_grid(denoise_grid, nrow=n_log_timesteps)\n        elif denoise_row.dim() == 6:\n            # video, grid_size=[n_log_timesteps*bs, t]\n            video_length = denoise_row.shape[3]\n            denoise_grid = rearrange(denoise_row, 'n b c t h w -> b n c t h w')\n            denoise_grid = rearrange(denoise_grid, 'b n c t h w -> (b n) c t h w')\n            denoise_grid = rearrange(denoise_grid, 'n c t h w -> (n t) c h w')\n            denoise_grid = make_grid(denoise_grid, nrow=video_length)",
        "type": "code",
        "location": "/lvdm/models/ddpm3d.py:525-546"
    },
    "139": {
        "file_id": 8,
        "content": "This function, `_get_denoise_row_from_list`, takes a list of samples and denoises them using the first-stage decoder. It then rearranges the data into a grid format based on the number of timesteps and batch size. If there are 5 dimensions (timesteps, batch, channels, height, width), it is reshaped into a 2D grid. If there are 6 dimensions (timesteps, batch, channels, time, height, width), it is reshaped into a 3D grid suitable for video visualization. The rearranged data is then displayed as a grid using `make_grid` function.",
        "type": "comment"
    },
    "140": {
        "file_id": 8,
        "content": "        else:\n            raise ValueError\n        return denoise_grid\n    def p_mean_variance(self, x, c, t, clip_denoised: bool, return_x0=False, score_corrector=None, corrector_kwargs=None, **kwargs):\n        t_in = t\n        model_out = self.apply_model(x, t_in, c, **kwargs)\n        if score_corrector is not None:\n            assert self.parameterization == \"eps\"\n            model_out = score_corrector.modify_score(self, model_out, x, t, c, **corrector_kwargs)\n        if self.parameterization == \"eps\":\n            x_recon = self.predict_start_from_noise(x, t=t, noise=model_out)\n        elif self.parameterization == \"x0\":\n            x_recon = model_out\n        else:\n            raise NotImplementedError()\n        if clip_denoised:\n            x_recon.clamp_(-1., 1.)\n        model_mean, posterior_variance, posterior_log_variance = self.q_posterior(x_start=x_recon, x_t=x, t=t)\n        if return_x0:\n            return model_mean, posterior_variance, posterior_log_variance, x_recon\n        else:\n            return model_mean, posterior_variance, posterior_log_variance",
        "type": "code",
        "location": "/lvdm/models/ddpm3d.py:547-576"
    },
    "141": {
        "file_id": 8,
        "content": "This code defines a function that calculates the mean and variance of the data for a denoising diffusion probability model (DDPM). The function takes in an input image 'x', conditioning information 'c', time step 't', and other parameters. If the parameterization is set to \"eps\" or \"x0\", it reconstructs the image from noise and calculates the mean and variance. If the image needs to be denoised, it clamps the values between -1 and 1. If return_x0 is True, it returns the mean, variance, and log variance along with the original input image.",
        "type": "comment"
    },
    "142": {
        "file_id": 8,
        "content": "    @torch.no_grad()\n    def p_sample(self, x, c, t, clip_denoised=False, repeat_noise=False, return_x0=False, \\\n                 temperature=1., noise_dropout=0., score_corrector=None, corrector_kwargs=None, **kwargs):\n        b, *_, device = *x.shape, x.device\n        outputs = self.p_mean_variance(x=x, c=c, t=t, clip_denoised=clip_denoised, return_x0=return_x0, \\\n                                       score_corrector=score_corrector, corrector_kwargs=corrector_kwargs, **kwargs)\n        if return_x0:\n            model_mean, _, model_log_variance, x0 = outputs\n        else:\n            model_mean, _, model_log_variance = outputs\n        noise = noise_like(x.shape, device, repeat_noise) * temperature\n        if noise_dropout > 0.:\n            noise = torch.nn.functional.dropout(noise, p=noise_dropout)\n        # no noise when t == 0\n        nonzero_mask = (1 - (t == 0).float()).reshape(b, *((1,) * (len(x.shape) - 1)))\n        if return_x0:\n            return model_mean + nonzero_mask * (0.5 * model_log_variance).exp() * noise, x0",
        "type": "code",
        "location": "/lvdm/models/ddpm3d.py:578-596"
    },
    "143": {
        "file_id": 8,
        "content": "This code defines a `p_sample` function that takes input `x`, condition `c`, and time step `t`. It computes mean and variance from the model, applies noise based on temperature and dropout rate, and combines them to generate samples. The output is either the denoised sample and original input (`return_x0=True`) or just the denoised sample (`return_x0=False`).",
        "type": "comment"
    },
    "144": {
        "file_id": 8,
        "content": "        else:\n            return model_mean + nonzero_mask * (0.5 * model_log_variance).exp() * noise\n    @torch.no_grad()\n    def p_sample_loop(self, cond, shape, return_intermediates=False, x_T=None, verbose=True, callback=None, \\\n                      timesteps=None, mask=None, x0=None, img_callback=None, start_T=None, log_every_t=None, **kwargs):\n        if not log_every_t:\n            log_every_t = self.log_every_t\n        device = self.betas.device\n        b = shape[0]        \n        # sample an initial noise\n        if x_T is None:\n            img = torch.randn(shape, device=device)\n        else:\n            img = x_T\n        intermediates = [img]\n        if timesteps is None:\n            timesteps = self.num_timesteps\n        if start_T is not None:\n            timesteps = min(timesteps, start_T)\n        iterator = tqdm(reversed(range(0, timesteps)), desc='Sampling t', total=timesteps) if verbose else reversed(range(0, timesteps))\n        if mask is not None:\n            assert x0 is not None\n            assert x0.shape[2:3] == mask.shape[2:3]  # spatial size has to match",
        "type": "code",
        "location": "/lvdm/models/ddpm3d.py:597-624"
    },
    "145": {
        "file_id": 8,
        "content": "This function samples an initial noise and iteratively updates it to generate images at different timesteps. It returns intermediate outputs if requested. The code also checks for the presence of timesteps, start_T, and log_every_t parameters, and handles them accordingly. The function uses a tqdm progress bar for verbose output during sampling.",
        "type": "comment"
    },
    "146": {
        "file_id": 8,
        "content": "        for i in iterator:\n            ts = torch.full((b,), i, device=device, dtype=torch.long)\n            if self.shorten_cond_schedule:\n                assert self.model.conditioning_key != 'hybrid'\n                tc = self.cond_ids[ts].to(cond.device)\n                cond = self.q_sample(x_start=cond, t=tc, noise=torch.randn_like(cond))\n            img = self.p_sample(img, cond, ts, clip_denoised=self.clip_denoised, **kwargs)\n            if mask is not None:\n                img_orig = self.q_sample(x0, ts)\n                img = img_orig * mask + (1. - mask) * img\n            if i % log_every_t == 0 or i == timesteps - 1:\n                intermediates.append(img)\n            if callback: callback(i)\n            if img_callback: img_callback(img, i)\n        if return_intermediates:\n            return img, intermediates\n        return img\nclass LatentVisualDiffusion(LatentDiffusion):\n    def __init__(self, img_cond_stage_config, image_proj_stage_config, freeze_embedder=True, *args, **kwargs):\n        super().__init__(*args, **kwargs)",
        "type": "code",
        "location": "/lvdm/models/ddpm3d.py:626-650"
    },
    "147": {
        "file_id": 8,
        "content": "This code iterates over a range of timesteps, for each timestep it generates an image, optionally applies masking to the generated image and appends it to intermediates list. The method also updates the conditional input for the model at each timestep, clips the denoised image if necessary, and calls callback functions after each iteration. If return_intermediates is True, it returns the final image and a list of intermediate images; otherwise, it only returns the final image. The LatentVisualDiffusion class initializes the LatentDiffusion base class with specified configuration for image conditioning and projection stages.",
        "type": "comment"
    },
    "148": {
        "file_id": 8,
        "content": "        self._init_embedder(img_cond_stage_config, freeze_embedder)\n        self.image_proj_model = instantiate_from_config(image_proj_stage_config)\n    def _init_embedder(self, config, freeze=True):\n        embedder = instantiate_from_config(config)\n        if freeze:\n            self.embedder = embedder.eval()\n            self.embedder.train = disabled_train\n            for param in self.embedder.parameters():\n                param.requires_grad = False\nclass DiffusionWrapper(pl.LightningModule):\n    def __init__(self, diff_model_config, conditioning_key):\n        super().__init__()\n        self.diffusion_model = instantiate_from_config(diff_model_config)\n        self.conditioning_key = conditioning_key\n    def forward(self, x, t, c_concat: list = None, c_crossattn: list = None,\n                c_adm=None, s=None, mask=None, **kwargs):\n        # temporal_context = fps is foNone\n        if self.conditioning_key is None:\n            out = self.diffusion_model(x, t)\n        elif self.conditioning_key == 'concat':",
        "type": "code",
        "location": "/lvdm/models/ddpm3d.py:651-674"
    },
    "149": {
        "file_id": 8,
        "content": "This code initializes an embedder and sets up the DiffusionWrapper class. The _init_embedder function initializes an embedder model, freezing its parameters if specified. The DiffusionWrapper class is a LightningModule that takes a diffusion model configuration and a conditioning key (concat or other). If no conditioning key is provided, it directly passes data through the diffusion model; otherwise, it performs conditioning operations before passing the data.",
        "type": "comment"
    },
    "150": {
        "file_id": 8,
        "content": "            xc = torch.cat([x] + c_concat, dim=1)\n            out = self.diffusion_model(xc, t, **kwargs)\n        elif self.conditioning_key == 'crossattn':\n            cc = torch.cat(c_crossattn, 1)\n            out = self.diffusion_model(x, t, context=cc, **kwargs)\n        elif self.conditioning_key == 'hybrid':\n            ## it is just right [b,c,t,h,w]: concatenate in channel dim\n            xc = torch.cat([x] + c_concat, dim=1)\n            cc = torch.cat(c_crossattn, 1)\n            out = self.diffusion_model(xc, t, context=cc, **kwargs)\n        elif self.conditioning_key == 'resblockcond':\n            cc = c_crossattn[0]\n            out = self.diffusion_model(x, t, context=cc)\n        elif self.conditioning_key == 'adm':\n            cc = c_crossattn[0]\n            out = self.diffusion_model(x, t, y=cc)\n        elif self.conditioning_key == 'hybrid-adm':\n            assert c_adm is not None\n            xc = torch.cat([x] + c_concat, dim=1)\n            cc = torch.cat(c_crossattn, 1)\n            out = self.diffusion_model(xc, t, context=cc, y=c_adm, **kwargs)",
        "type": "code",
        "location": "/lvdm/models/ddpm3d.py:675-695"
    },
    "151": {
        "file_id": 8,
        "content": "The code above contains conditional branches for different conditioning methods, such as hybrid, crossattn, resblockcond, adm, and hybrid-adm. It concatenates the input (x) with other tensors based on the conditioning key, and then passes the resulting tensor to the diffusion model for processing. The output of the model is stored in 'out'.",
        "type": "comment"
    },
    "152": {
        "file_id": 8,
        "content": "        elif self.conditioning_key == 'hybrid-time':\n            assert s is not None\n            xc = torch.cat([x] + c_concat, dim=1)\n            cc = torch.cat(c_crossattn, 1)\n            out = self.diffusion_model(xc, t, context=cc, s=s)\n        elif self.conditioning_key == 'concat-time-mask':\n            # assert s is not None\n            xc = torch.cat([x] + c_concat, dim=1)\n            out = self.diffusion_model(xc, t, context=None, s=s, mask=mask)\n        elif self.conditioning_key == 'concat-adm-mask':\n            # assert s is not None\n            if c_concat is not None:\n                xc = torch.cat([x] + c_concat, dim=1)\n            else:\n                xc = x\n            out = self.diffusion_model(xc, t, context=None, y=s, mask=mask)\n        elif self.conditioning_key == 'hybrid-adm-mask':\n            cc = torch.cat(c_crossattn, 1)\n            if c_concat is not None:\n                xc = torch.cat([x] + c_concat, dim=1)\n            else:\n                xc = x\n            out = self.diffusion_model(xc, t, context=cc, y=s, mask=mask)",
        "type": "code",
        "location": "/lvdm/models/ddpm3d.py:696-718"
    },
    "153": {
        "file_id": 8,
        "content": "This code block handles various conditioning methods for the diffusion model. It uses different combinations of inputs such as x, c_concat, t, s, and mask based on the specified conditioning key. The keys include 'hybrid-time', 'concat-time-mask', 'concat-adm-mask', and 'hybrid-adm-mask'. Each block performs specific operations and asserts before passing the data to self.diffusion_model for further processing.",
        "type": "comment"
    },
    "154": {
        "file_id": 8,
        "content": "        elif self.conditioning_key == 'hybrid-time-adm': # adm means y, e.g., class index\n            # assert s is not None\n            assert c_adm is not None\n            xc = torch.cat([x] + c_concat, dim=1)\n            cc = torch.cat(c_crossattn, 1)\n            out = self.diffusion_model(xc, t, context=cc, s=s, y=c_adm)\n        elif self.conditioning_key == 'crossattn-adm':\n            assert c_adm is not None\n            cc = torch.cat(c_crossattn, 1)\n            out = self.diffusion_model(x, t, context=cc, y=c_adm)\n        else:\n            raise NotImplementedError()\n        return out",
        "type": "code",
        "location": "/lvdm/models/ddpm3d.py:719-732"
    },
    "155": {
        "file_id": 8,
        "content": "Code handles different conditioning keys for model inputs. If 'hybrid-time-adm' key, concatenates x and c_concat along dimension 1, then passes to diffusion model with context cc, s, and c_adm. If 'crossattn-adm' key, only concatenates c_crossattn along dimension 1 before passing to diffusion model with y as c_adm. For other keys, raises NotImplementedError.",
        "type": "comment"
    },
    "156": {
        "file_id": 9,
        "content": "/lvdm/models/samplers/ddim.py",
        "type": "filepath"
    },
    "157": {
        "file_id": 9,
        "content": "The DDIMSampler class in PyTorch implements both DDPM and DDIM sampling, handles timesteps, and is used for image generation with input conditioning and optional parameters. It applies the model to generate e_t, combines values, transforms based on parameterization type, and performs DDIM sampling using denoising and fast reconstruction functions.",
        "type": "summary"
    },
    "158": {
        "file_id": 9,
        "content": "import numpy as np\nfrom tqdm import tqdm\nimport torch\nfrom lvdm.models.utils_diffusion import make_ddim_sampling_parameters, make_ddim_timesteps\nfrom lvdm.common import noise_like\nfrom lvdm.common import extract_into_tensor\nclass DDIMSampler(object):\n    def __init__(self, model, schedule=\"linear\", **kwargs):\n        super().__init__()\n        self.model = model\n        self.ddpm_num_timesteps = model.num_timesteps\n        self.schedule = schedule\n        self.counter = 0\n    def register_buffer(self, name, attr):\n        if type(attr) == torch.Tensor:\n            if attr.device != torch.device(\"cuda\"):\n                attr = attr.to(torch.device(\"cuda\"))\n        setattr(self, name, attr)\n    def make_schedule(self, ddim_num_steps, ddim_discretize=\"uniform\", ddim_eta=0., verbose=True):\n        self.ddim_timesteps = make_ddim_timesteps(ddim_discr_method=ddim_discretize, num_ddim_timesteps=ddim_num_steps,\n                                                  num_ddpm_timesteps=self.ddpm_num_timesteps,verbose=verbose)",
        "type": "code",
        "location": "/lvdm/models/samplers/ddim.py:1-25"
    },
    "159": {
        "file_id": 9,
        "content": "DDIMSampler class is being initialized with a model, schedule and other parameters. It has methods to make scheduling based on given steps, register buffers for tensor attributes, and uses make_ddim_timesteps function from lvdm.utils_diffusion module. This code seems to be part of a larger diffusion model that utilizes DDIM sampling.",
        "type": "comment"
    },
    "160": {
        "file_id": 9,
        "content": "        alphas_cumprod = self.model.alphas_cumprod\n        assert alphas_cumprod.shape[0] == self.ddpm_num_timesteps, 'alphas have to be defined for each timestep'\n        to_torch = lambda x: x.clone().detach().to(torch.float32).to(self.model.device)\n        if self.model.use_dynamic_rescale:\n            self.ddim_scale_arr = self.model.scale_arr[self.ddim_timesteps]\n            self.ddim_scale_arr_prev = torch.cat([self.ddim_scale_arr[0:1], self.ddim_scale_arr[:-1]])\n        self.register_buffer('betas', to_torch(self.model.betas))\n        self.register_buffer('alphas_cumprod', to_torch(alphas_cumprod))\n        self.register_buffer('alphas_cumprod_prev', to_torch(self.model.alphas_cumprod_prev))\n        # calculations for diffusion q(x_t | x_{t-1}) and others\n        self.register_buffer('sqrt_alphas_cumprod', to_torch(np.sqrt(alphas_cumprod.cpu())))\n        self.register_buffer('sqrt_one_minus_alphas_cumprod', to_torch(np.sqrt(1. - alphas_cumprod.cpu())))\n        self.register_buffer('log_one_minus_alphas_cumprod', to_torch(np.log(1. - alphas_cumprod.cpu())))",
        "type": "code",
        "location": "/lvdm/models/samplers/ddim.py:26-41"
    },
    "161": {
        "file_id": 9,
        "content": "This code is initializing various buffers for a diffusion model. It assigns the alphas_cumprod to a buffer, asserts that its shape matches the number of timesteps, converts tensors to float32 type and moves them to the device, registers buffers for betas, alphas_cumprod, alphas_cumprod_prev, sqrt_alphas_cumprod, sqrt_one_minus_alphas_cumprod, and log_one_minus_alphas_cumprod. This model is used in a diffusion process to calculate q(x_t | x_{t-1}) and other related calculations.",
        "type": "comment"
    },
    "162": {
        "file_id": 9,
        "content": "        self.register_buffer('sqrt_recip_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod.cpu())))\n        self.register_buffer('sqrt_recipm1_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod.cpu() - 1)))\n        # ddim sampling parameters\n        ddim_sigmas, ddim_alphas, ddim_alphas_prev = make_ddim_sampling_parameters(alphacums=alphas_cumprod.cpu(),\n                                                                                   ddim_timesteps=self.ddim_timesteps,\n                                                                                   eta=ddim_eta,verbose=verbose)\n        self.register_buffer('ddim_sigmas', ddim_sigmas)\n        self.register_buffer('ddim_alphas', ddim_alphas)\n        self.register_buffer('ddim_alphas_prev', ddim_alphas_prev)\n        self.register_buffer('ddim_sqrt_one_minus_alphas', np.sqrt(1. - ddim_alphas))\n        sigmas_for_original_sampling_steps = ddim_eta * torch.sqrt(\n            (1 - self.alphas_cumprod_prev) / (1 - self.alphas_cumprod) * (\n                        1 - self.alphas_cumprod / self.alphas_cumprod_prev))",
        "type": "code",
        "location": "/lvdm/models/samplers/ddim.py:42-55"
    },
    "163": {
        "file_id": 9,
        "content": "The code registers buffers for DDIM sampling parameters and calculates sigmas_for_original_sampling_steps. It initializes ddim_sigmas, ddim_alphas, ddim_alphas_prev, and ddim_sqrt_one_minus_alphas. The alphas_cumprod is computed from the input alphas_cumprod and alphas_cumprod_prev is also calculated. Sigmas for original sampling steps are determined based on these parameters. This code implements DDIM (Denoising Diffusion Probabilistic Models) sampling technique in PyTorch.",
        "type": "comment"
    },
    "164": {
        "file_id": 9,
        "content": "        self.register_buffer('ddim_sigmas_for_original_num_steps', sigmas_for_original_sampling_steps)\n    @torch.no_grad()\n    def sample(self,\n               S,\n               batch_size,\n               shape,\n               conditioning=None,\n               callback=None,\n               normals_sequence=None,\n               img_callback=None,\n               quantize_x0=False,\n               eta=0.,\n               mask=None,\n               x0=None,\n               temperature=1.,\n               noise_dropout=0.,\n               score_corrector=None,\n               corrector_kwargs=None,\n               verbose=True,\n               schedule_verbose=False,\n               x_T=None,\n               log_every_t=100,\n               unconditional_guidance_scale=1.,\n               unconditional_conditioning=None,\n               precision=None,\n               fs=None,\n               # this has to come in the same format as the conditioning, # e.g. as encoded tokens, ...\n               **kwargs\n               ):\n        # check condition bs",
        "type": "code",
        "location": "/lvdm/models/samplers/ddim.py:56-87"
    },
    "165": {
        "file_id": 9,
        "content": "This code defines a DDPM (Denoising Diffusion Probabilistic Models) sampler class with a sample method. The sample method takes input S, batch_size, shape, and other optional parameters to generate samples from the model. It also has methods for registering buffer and various options for controlling the sampling process.",
        "type": "comment"
    },
    "166": {
        "file_id": 9,
        "content": "        if conditioning is not None:\n            if isinstance(conditioning, dict):\n                try:\n                    cbs = conditioning[list(conditioning.keys())[0]].shape[0]\n                except:\n                    cbs = conditioning[list(conditioning.keys())[0]][0].shape[0]\n                if cbs != batch_size:\n                    print(f\"Warning: Got {cbs} conditionings but batch-size is {batch_size}\")\n            else:\n                if conditioning.shape[0] != batch_size:\n                    print(f\"Warning: Got {conditioning.shape[0]} conditionings but batch-size is {batch_size}\")\n        self.make_schedule(ddim_num_steps=S, ddim_eta=eta, verbose=schedule_verbose)\n        # make shape\n        if len(shape) == 3:\n            C, H, W = shape\n            size = (batch_size, C, H, W)\n        elif len(shape) == 4:\n            C, T, H, W = shape\n            size = (batch_size, C, T, H, W)\n        # print(f'Data shape for DDIM sampling is {size}, eta {eta}')\n        samples, intermediates = self.ddim_sampling(conditioning, size,",
        "type": "code",
        "location": "/lvdm/models/samplers/ddim.py:88-112"
    },
    "167": {
        "file_id": 9,
        "content": "Checks if conditioning is provided, ensures its shape matches the batch size. Sets up the sampling schedule, determines data shape based on input shape, and then performs DDIM sampling.",
        "type": "comment"
    },
    "168": {
        "file_id": 9,
        "content": "                                                    callback=callback,\n                                                    img_callback=img_callback,\n                                                    quantize_denoised=quantize_x0,\n                                                    mask=mask, x0=x0,\n                                                    ddim_use_original_steps=False,\n                                                    noise_dropout=noise_dropout,\n                                                    temperature=temperature,\n                                                    score_corrector=score_corrector,\n                                                    corrector_kwargs=corrector_kwargs,\n                                                    x_T=x_T,\n                                                    log_every_t=log_every_t,\n                                                    unconditional_guidance_scale=unconditional_guidance_scale,\n                                                    unconditional_conditioning=unconditional_conditioning,",
        "type": "code",
        "location": "/lvdm/models/samplers/ddim.py:113-125"
    },
    "169": {
        "file_id": 9,
        "content": "This code block defines a function that takes in parameters such as callback, img_callback, quantize_denoised, mask, x0, ddim_use_original_steps, noise_dropout, temperature, score_corrector, corrector_kwargs, x_T, log_every_t, unconditional_guidance_scale, and unconditional_conditioning. It uses these parameters to perform denoising diffusion probabilistic sampling for image generation or processing.",
        "type": "comment"
    },
    "170": {
        "file_id": 9,
        "content": "                                                    verbose=verbose,\n                                                    precision=precision,\n                                                    fs=fs,\n                                                    **kwargs)\n        return samples, intermediates\n    @torch.no_grad()\n    def ddim_sampling(self, cond, shape,\n                      x_T=None, ddim_use_original_steps=False,\n                      callback=None, timesteps=None, quantize_denoised=False,\n                      mask=None, x0=None, img_callback=None, log_every_t=100,\n                      temperature=1., noise_dropout=0., score_corrector=None, corrector_kwargs=None,\n                      unconditional_guidance_scale=1., unconditional_conditioning=None, verbose=True,precision=None,fs=None,\n                      **kwargs):\n        device = self.model.betas.device        \n        b = shape[0]\n        if x_T is None:\n            img = torch.randn(shape, device=device)\n        else:\n            img = x_T",
        "type": "code",
        "location": "/lvdm/models/samplers/ddim.py:126-145"
    },
    "171": {
        "file_id": 9,
        "content": "This function performs DDIM sampling using the specified `cond`, `shape` and other optional parameters. It returns the generated samples and intermediates if required. The function is wrapped in a `torch.no_grad()` context to avoid unnecessary gradients computation.",
        "type": "comment"
    },
    "172": {
        "file_id": 9,
        "content": "        if precision is not None:\n            if precision == 16:\n                img = img.to(dtype=torch.float16)\n        if timesteps is None:\n            timesteps = self.ddpm_num_timesteps if ddim_use_original_steps else self.ddim_timesteps\n        elif timesteps is not None and not ddim_use_original_steps:\n            subset_end = int(min(timesteps / self.ddim_timesteps.shape[0], 1) * self.ddim_timesteps.shape[0]) - 1\n            timesteps = self.ddim_timesteps[:subset_end]\n        intermediates = {'x_inter': [img], 'pred_x0': [img]}\n        time_range = reversed(range(0,timesteps)) if ddim_use_original_steps else np.flip(timesteps)\n        total_steps = timesteps if ddim_use_original_steps else timesteps.shape[0]\n        if verbose:\n            iterator = tqdm(time_range, desc='DDIM Sampler', total=total_steps)\n        else:\n            iterator = time_range\n        clean_cond = kwargs.pop(\"clean_cond\", False)\n        for i, step in enumerate(iterator):\n            index = total_steps - i - 1\n            ts = torch.full((b,), step, device=device, dtype=torch.long)",
        "type": "code",
        "location": "/lvdm/models/samplers/ddim.py:146-167"
    },
    "173": {
        "file_id": 9,
        "content": "This code block is responsible for handling the timesteps and preparing the inputs for DDIM sampling in a diffusion model. It first checks if precision or timesteps are specified, then determines the appropriate timesteps to use based on ddim_use_original_steps flag. It creates intermediates dictionary, defines time range for iterating through timesteps, and optionally sets up progress bar. Lastly, it processes clean_cond parameter and prepares tensor of current timestep (ts).",
        "type": "comment"
    },
    "174": {
        "file_id": 9,
        "content": "            ## use mask to blend noised original latent (img_orig) & new sampled latent (img)\n            if mask is not None:\n                assert x0 is not None\n                if clean_cond:\n                    img_orig = x0\n                else:\n                    img_orig = self.model.q_sample(x0, ts)  # TODO: deterministic forward pass? <ddim inversion>\n                img = img_orig * mask + (1. - mask) * img # keep original & modify use img\n            outs = self.p_sample_ddim(img, cond, ts, index=index, use_original_steps=ddim_use_original_steps,\n                                      quantize_denoised=quantize_denoised, temperature=temperature,\n                                      noise_dropout=noise_dropout, score_corrector=score_corrector,\n                                      corrector_kwargs=corrector_kwargs,\n                                      unconditional_guidance_scale=unconditional_guidance_scale,\n                                      unconditional_conditioning=unconditional_conditioning,",
        "type": "code",
        "location": "/lvdm/models/samplers/ddim.py:169-183"
    },
    "175": {
        "file_id": 9,
        "content": "The code checks if a mask is provided and, if so, blends the noised original latent (img_orig) with the new sampled latent (img) based on the mask value. It also performs a deterministic forward pass using ddim inversion. The function then calls p_sample_ddim to generate the output image.",
        "type": "comment"
    },
    "176": {
        "file_id": 9,
        "content": "                                      mask=mask,x0=x0,fs=fs,\n                                      **kwargs)\n            img, pred_x0 = outs\n            if callback: callback(i)\n            if img_callback: img_callback(pred_x0, i)\n            if index % log_every_t == 0 or index == total_steps - 1:\n                intermediates['x_inter'].append(img)\n                intermediates['pred_x0'].append(pred_x0)\n        return img, intermediates\n    @torch.no_grad()\n    def p_sample_ddim(self, x, c, t, index, repeat_noise=False, use_original_steps=False, quantize_denoised=False,\n                      temperature=1., noise_dropout=0., score_corrector=None, corrector_kwargs=None,\n                      unconditional_guidance_scale=1., unconditional_conditioning=None,\n                      uc_type=None, conditional_guidance_scale_temporal=None,mask=None,x0=None, **kwargs):\n        b, *_, device = *x.shape, x.device\n        if x.dim() == 5:\n            is_video = True\n        else:\n            is_video = False\n        if unconditional_conditioning is None or unconditional_guidance_scale == 1.:",
        "type": "code",
        "location": "/lvdm/models/samplers/ddim.py:184-210"
    },
    "177": {
        "file_id": 9,
        "content": "This code is part of the DDIM (Denoising Diffusion Probabilistic Models) sampler in a machine learning model. It takes in inputs, runs a denoising process, and outputs denoised images or video frames. The code allows for various optional parameters to control the sampling process, such as temperature, noise dropout rate, and guidance scale. It also supports conditional guidance and can handle video input if needed.",
        "type": "comment"
    },
    "178": {
        "file_id": 9,
        "content": "            e_t = self.model.apply_model(x, t, c, **kwargs) # unet denoiser\n        else:\n            ### with unconditional condition\n            if isinstance(c, torch.Tensor) or isinstance(c, dict):\n                e_t_cond = self.model.apply_model(x, t, c, **kwargs)\n                e_t_uncond = self.model.apply_model(x, t, unconditional_conditioning, **kwargs)\n            else:\n                raise NotImplementedError\n            e_t = e_t_uncond + unconditional_guidance_scale * (e_t_cond - e_t_uncond)\n        if self.model.parameterization == \"v\":\n            e_t = self.model.predict_eps_from_z_and_v(x, t, e_t)\n        if score_corrector is not None:\n            assert self.model.parameterization == \"eps\"\n            e_t = score_corrector.modify_score(self.model, e_t, x, t, c, **corrector_kwargs)\n        alphas = self.model.alphas_cumprod if use_original_steps else self.ddim_alphas\n        alphas_prev = self.model.alphas_cumprod_prev if use_original_steps else self.ddim_alphas_prev\n        sqrt_",
        "type": "code",
        "location": "/lvdm/models/samplers/ddim.py:211-232"
    },
    "179": {
        "file_id": 9,
        "content": "This code applies the model to generate e_t, considering both conditional and unconditional conditioning. If the conditioning is a tensor or dict, it calculates e_t_cond and e_t_uncond separately, then combines them based on guidance scale. The parameterization type determines if e_t needs further transformation. Alphas values are used for cumulative product calculations depending on the use_original_steps flag.",
        "type": "comment"
    },
    "180": {
        "file_id": 9,
        "content": "one_minus_alphas = self.model.sqrt_one_minus_alphas_cumprod if use_original_steps else self.ddim_sqrt_one_minus_alphas\n        sigmas = self.ddim_sigmas_for_original_num_steps if use_original_steps else self.ddim_sigmas\n        # select parameters corresponding to the currently considered timestep\n        if is_video:\n            size = (b, 1, 1, 1, 1)\n        else:\n            size = (b, 1, 1, 1)\n        a_t = torch.full(size, alphas[index], device=device)\n        a_prev = torch.full(size, alphas_prev[index], device=device)\n        sigma_t = torch.full(size, sigmas[index], device=device)\n        sqrt_one_minus_at = torch.full(size, sqrt_one_minus_alphas[index],device=device)\n        # current prediction for x_0\n        if self.model.parameterization != \"v\":\n            pred_x0 = (x - sqrt_one_minus_at * e_t) / a_t.sqrt()\n        else:\n            pred_x0 = self.model.predict_start_from_z_and_v(x, t, e_t)\n        if self.model.use_dynamic_rescale:\n            scale_t = torch.full(size, self.ddim_scale_arr[index], device=device)",
        "type": "code",
        "location": "/lvdm/models/samplers/ddim.py:232-252"
    },
    "181": {
        "file_id": 9,
        "content": "This code snippet selects parameters based on the current timestep, calculates the prediction for x0, and applies dynamic rescaling if needed. It handles both video and non-video inputs, and uses different parameterization methods depending on the model setting.",
        "type": "comment"
    },
    "182": {
        "file_id": 9,
        "content": "            prev_scale_t = torch.full(size, self.ddim_scale_arr_prev[index], device=device)\n            rescale = (prev_scale_t / scale_t)\n            pred_x0 *= rescale\n        if quantize_denoised:\n            pred_x0, _, *_ = self.model.first_stage_model.quantize(pred_x0)\n        # direction pointing to x_t\n        dir_xt = (1. - a_prev - sigma_t**2).sqrt() * e_t\n        noise = sigma_t * noise_like(x.shape, device, repeat_noise) * temperature\n        if noise_dropout > 0.:\n            noise = torch.nn.functional.dropout(noise, p=noise_dropout)\n        x_prev = a_prev.sqrt() * pred_x0 + dir_xt + noise\n        return x_prev, pred_x0\n    @torch.no_grad()\n    def decode(self, x_latent, cond, t_start, unconditional_guidance_scale=1.0, unconditional_conditioning=None,\n               use_original_steps=False, callback=None):\n        timesteps = np.arange(self.ddpm_num_timesteps) if use_original_steps else self.ddim_timesteps\n        timesteps = timesteps[:t_start]\n        time_range = np.flip(timesteps)\n        total_steps = timesteps.shape[0]",
        "type": "code",
        "location": "/lvdm/models/samplers/ddim.py:253-278"
    },
    "183": {
        "file_id": 9,
        "content": "This code defines a DDIM decoding function for a diffusion model. It calculates intermediate variables for each time step, denoises the input using a first-stage model, and applies noise dropout if applicable. The decode method takes latent input, conditional information, start timestep, optional guidance scale, unconditional conditioning, use_original_steps flag, and a callback function as arguments to generate intermediate time steps and output the final result.",
        "type": "comment"
    },
    "184": {
        "file_id": 9,
        "content": "        print(f\"Running DDIM Sampling with {total_steps} timesteps\")\n        iterator = tqdm(time_range, desc='Decoding image', total=total_steps)\n        x_dec = x_latent\n        for i, step in enumerate(iterator):\n            index = total_steps - i - 1\n            ts = torch.full((x_latent.shape[0],), step, device=x_latent.device, dtype=torch.long)\n            x_dec, _ = self.p_sample_ddim(x_dec, cond, ts, index=index, use_original_steps=use_original_steps,\n                                          unconditional_guidance_scale=unconditional_guidance_scale,\n                                          unconditional_conditioning=unconditional_conditioning)\n            if callback: callback(i)\n        return x_dec\n    @torch.no_grad()\n    def stochastic_encode(self, x0, t, use_original_steps=False, noise=None):\n        # fast, but does not allow for exact reconstruction\n        # t serves as an index to gather the correct alphas\n        if use_original_steps:\n            sqrt_alphas_cumprod = self.sqrt_alphas_cumprod",
        "type": "code",
        "location": "/lvdm/models/samplers/ddim.py:279-297"
    },
    "185": {
        "file_id": 9,
        "content": "This code snippet is performing DDIM (Denoising Diffusion Probabilistic Models) sampling with a specified number of timesteps. It uses the `p_sample_ddim` function to iterate over the steps, applying noisy transitions and denoising them step-by-step. The `stochastic_encode` function provides a fast but non-exact reconstruction method for the latent space.",
        "type": "comment"
    },
    "186": {
        "file_id": 9,
        "content": "            sqrt_one_minus_alphas_cumprod = self.sqrt_one_minus_alphas_cumprod\n        else:\n            sqrt_alphas_cumprod = torch.sqrt(self.ddim_alphas)\n            sqrt_one_minus_alphas_cumprod = self.ddim_sqrt_one_minus_alphas\n        if noise is None:\n            noise = torch.randn_like(x0)\n        return (extract_into_tensor(sqrt_alphas_cumprod, t, x0.shape) * x0 +\n                extract_into_tensor(sqrt_one_minus_alphas_cumprod, t, x0.shape) * noise)",
        "type": "code",
        "location": "/lvdm/models/samplers/ddim.py:298-306"
    },
    "187": {
        "file_id": 9,
        "content": "Computes and returns the denoising diffusion model's (DDPM) sample using provided inputs. If noise is None, generates random noise. Multiplies alpha_cumprod and one_minus_alpha_cumprod with x0 and noise respectively before summing them.",
        "type": "comment"
    },
    "188": {
        "file_id": 10,
        "content": "/lvdm/models/samplers/ddim_multiplecond.py",
        "type": "filepath"
    },
    "189": {
        "file_id": 10,
        "content": "The DDIMSampler class enables efficient diffusion model sampling using Numpy and GPU buffers, implementing denoising diffusion probabilistic models for image generation with adjustable outputs and a DDIM sampler. The code defines a DDIM multiple condition sampler for stochastic encoding, iterating over timesteps and providing encoded output based on alpha values and optional callbacks.",
        "type": "summary"
    },
    "190": {
        "file_id": 10,
        "content": "import numpy as np\nfrom tqdm import tqdm\nimport torch\nfrom lvdm.models.utils_diffusion import make_ddim_sampling_parameters, make_ddim_timesteps\nfrom lvdm.common import noise_like\nfrom lvdm.common import extract_into_tensor\nclass DDIMSampler(object):\n    def __init__(self, model, schedule=\"linear\", **kwargs):\n        super().__init__()\n        self.model = model\n        self.ddpm_num_timesteps = model.num_timesteps\n        self.schedule = schedule\n        self.counter = 0\n    def register_buffer(self, name, attr):\n        if type(attr) == torch.Tensor:\n            if attr.device != torch.device(\"cuda\"):\n                attr = attr.to(torch.device(\"cuda\"))\n        setattr(self, name, attr)\n    def make_schedule(self, ddim_num_steps, ddim_discretize=\"uniform\", ddim_eta=0., verbose=True):\n        self.ddim_timesteps = make_ddim_timesteps(ddim_discr_method=ddim_discretize, num_ddim_timesteps=ddim_num_steps,\n                                                  num_ddpm_timesteps=self.ddpm_num_timesteps,verbose=verbose)",
        "type": "code",
        "location": "/lvdm/models/samplers/ddim_multiplecond.py:1-25"
    },
    "191": {
        "file_id": 10,
        "content": "This code defines a DDIMSampler class for diffusion model sampling. It takes a model, schedule, and optional keyword arguments as inputs. The class has methods to make a schedule based on the number of desired DDIM steps, discretization method, and eta value. It also includes helper methods like register_buffer for managing buffers in the sampler.",
        "type": "comment"
    },
    "192": {
        "file_id": 10,
        "content": "        alphas_cumprod = self.model.alphas_cumprod\n        assert alphas_cumprod.shape[0] == self.ddpm_num_timesteps, 'alphas have to be defined for each timestep'\n        to_torch = lambda x: x.clone().detach().to(torch.float32).to(self.model.device)\n        self.register_buffer('betas', to_torch(self.model.betas))\n        self.register_buffer('alphas_cumprod', to_torch(alphas_cumprod))\n        self.register_buffer('alphas_cumprod_prev', to_torch(self.model.alphas_cumprod_prev))\n        # calculations for diffusion q(x_t | x_{t-1}) and others\n        self.register_buffer('sqrt_alphas_cumprod', to_torch(np.sqrt(alphas_cumprod.cpu())))\n        self.register_buffer('sqrt_one_minus_alphas_cumprod', to_torch(np.sqrt(1. - alphas_cumprod.cpu())))\n        self.register_buffer('log_one_minus_alphas_cumprod', to_torch(np.log(1. - alphas_cumprod.cpu())))\n        self.register_buffer('sqrt_recip_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod.cpu())))\n        self.register_buffer('sqrt_recipm1_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod.cpu() - 1)))",
        "type": "code",
        "location": "/lvdm/models/samplers/ddim_multiplecond.py:26-39"
    },
    "193": {
        "file_id": 10,
        "content": "This code initializes variables for the DDPM sampler. It checks the shape of alphas_cumprod and registers buffers for betas, alphas_cumprod, alphas_cumprod_prev, sqrt_alphas_cumprod, sqrt_one_minus_alphas_cumprod, log_one_minus_alphas_cumprod, sqrt_recip_alphas_cumprod, and sqrt_recipm1_alphas_cumprod. All buffers are created with the same device as the model for efficient GPU usage.",
        "type": "comment"
    },
    "194": {
        "file_id": 10,
        "content": "        # ddim sampling parameters\n        ddim_sigmas, ddim_alphas, ddim_alphas_prev = make_ddim_sampling_parameters(alphacums=alphas_cumprod.cpu(),\n                                                                                   ddim_timesteps=self.ddim_timesteps,\n                                                                                   eta=ddim_eta,verbose=verbose)\n        self.register_buffer('ddim_sigmas', ddim_sigmas)\n        self.register_buffer('ddim_alphas', ddim_alphas)\n        self.register_buffer('ddim_alphas_prev', ddim_alphas_prev)\n        self.register_buffer('ddim_sqrt_one_minus_alphas', np.sqrt(1. - ddim_alphas))\n        sigmas_for_original_sampling_steps = ddim_eta * torch.sqrt(\n            (1 - self.alphas_cumprod_prev) / (1 - self.alphas_cumprod) * (\n                        1 - self.alphas_cumprod / self.alphas_cumprod_prev))\n        self.register_buffer('ddim_sigmas_for_original_num_steps', sigmas_for_original_sampling_steps)\n    @torch.no_grad()\n    def sample(self,\n               S,",
        "type": "code",
        "location": "/lvdm/models/samplers/ddim_multiplecond.py:41-56"
    },
    "195": {
        "file_id": 10,
        "content": "This code segment is initializing DDIM sampling parameters and buffers for the model. It calculates ddim_sigmas, ddim_alphas, ddim_alphas_prev, and other related values to be used in the sample function later. The code also uses numpy for some calculations and registers these buffer parameters to the model for future use.",
        "type": "comment"
    },
    "196": {
        "file_id": 10,
        "content": "               batch_size,\n               shape,\n               conditioning=None,\n               callback=None,\n               normals_sequence=None,\n               img_callback=None,\n               quantize_x0=False,\n               eta=0.,\n               mask=None,\n               x0=None,\n               temperature=1.,\n               noise_dropout=0.,\n               score_corrector=None,\n               corrector_kwargs=None,\n               verbose=True,\n               schedule_verbose=False,\n               x_T=None,\n               log_every_t=100,\n               unconditional_guidance_scale=1.,\n               unconditional_conditioning=None,\n               precision=None,\n               fs=None,\n               # this has to come in the same format as the conditioning, # e.g. as encoded tokens, ...\n               **kwargs\n               ):\n        # check condition bs\n        if conditioning is not None:\n            if isinstance(conditioning, dict):\n                try:\n                    cbs = conditioning[list(conditioning.keys())[0]].shape[0]",
        "type": "code",
        "location": "/lvdm/models/samplers/ddim_multiplecond.py:57-87"
    },
    "197": {
        "file_id": 10,
        "content": "This function takes multiple arguments for DDIM sampler including batch_size, shape, conditioning (if any), callbacks, and more. It checks the shape of the conditioning input.",
        "type": "comment"
    },
    "198": {
        "file_id": 10,
        "content": "                except:\n                    cbs = conditioning[list(conditioning.keys())[0]][0].shape[0]\n                if cbs != batch_size:\n                    print(f\"Warning: Got {cbs} conditionings but batch-size is {batch_size}\")\n            else:\n                if conditioning.shape[0] != batch_size:\n                    print(f\"Warning: Got {conditioning.shape[0]} conditionings but batch-size is {batch_size}\")\n        self.make_schedule(ddim_num_steps=S, ddim_eta=eta, verbose=schedule_verbose)\n        # make shape\n        if len(shape) == 3:\n            C, H, W = shape\n            size = (batch_size, C, H, W)\n        elif len(shape) == 4:\n            C, T, H, W = shape\n            size = (batch_size, C, T, H, W)\n        # print(f'Data shape for DDIM sampling is {size}, eta {eta}')\n        samples, intermediates = self.ddim_sampling(conditioning, size,\n                                                    callback=callback,\n                                                    img_callback=img_callback,",
        "type": "code",
        "location": "/lvdm/models/samplers/ddim_multiplecond.py:88-110"
    },
    "199": {
        "file_id": 10,
        "content": "This code block checks if the number of conditionings matches the batch size, and if not, it prints a warning. It then determines the shape of the samples based on the input shape and creates a data structure for DDIM sampling with the specified eta value.",
        "type": "comment"
    }
}